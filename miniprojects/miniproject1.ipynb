{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    }
   },
   "outputs": [],
   "source": [
    "student1 = \"Pau Argelaguet Franquelo\"\n",
    "student2 = \"Natalie Bolon Brun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "np.random.seed(hash(student1 + student2) % 2**32)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('data/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('data/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We furter reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30246 samples, validate on 7562 samples\n",
      "Epoch 1/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 1.6074 - acc: 0.2228 - val_loss: 1.5612 - val_acc: 0.1263\n",
      "Epoch 2/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.5913 - acc: 0.2636 - val_loss: 1.5823 - val_acc: 0.0664\n",
      "Epoch 3/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5848 - acc: 0.2718 - val_loss: 1.5252 - val_acc: 0.2017\n",
      "Epoch 4/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 1.5793 - acc: 0.2827 - val_loss: 1.5594 - val_acc: 0.1128\n",
      "Epoch 5/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 1.5737 - acc: 0.2911 - val_loss: 1.5526 - val_acc: 0.1448\n",
      "Epoch 6/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5678 - acc: 0.3021 - val_loss: 1.5403 - val_acc: 0.1357\n",
      "Epoch 7/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.5613 - acc: 0.3053 - val_loss: 1.5481 - val_acc: 0.1246\n",
      "Epoch 8/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5540 - acc: 0.3094 - val_loss: 1.5069 - val_acc: 0.2374\n",
      "Epoch 9/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 1.5460 - acc: 0.3217 - val_loss: 1.5542 - val_acc: 0.1201\n",
      "Epoch 10/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.5373 - acc: 0.3239 - val_loss: 1.4995 - val_acc: 0.2152\n",
      "Epoch 11/1500\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 1.5280 - acc: 0.3341 - val_loss: 1.5138 - val_acc: 0.1928\n",
      "Epoch 12/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5177 - acc: 0.3450 - val_loss: 1.5009 - val_acc: 0.2227\n",
      "Epoch 13/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 1.5068 - acc: 0.3565 - val_loss: 1.4756 - val_acc: 0.2620\n",
      "Epoch 14/1500\n",
      "30246/30246 [==============================] - 1s 35us/step - loss: 1.4953 - acc: 0.3692 - val_loss: 1.4469 - val_acc: 0.3223\n",
      "Epoch 15/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.4834 - acc: 0.3826 - val_loss: 1.4640 - val_acc: 0.2975\n",
      "Epoch 16/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 1.4712 - acc: 0.3962 - val_loss: 1.4556 - val_acc: 0.3032\n",
      "Epoch 17/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.4585 - acc: 0.4097 - val_loss: 1.4212 - val_acc: 0.3840\n",
      "Epoch 18/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.4454 - acc: 0.4230 - val_loss: 1.4387 - val_acc: 0.3280\n",
      "Epoch 19/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.4319 - acc: 0.4373 - val_loss: 1.3993 - val_acc: 0.3921\n",
      "Epoch 20/1500\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 1.4180 - acc: 0.4520 - val_loss: 1.4180 - val_acc: 0.3662\n",
      "Epoch 21/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 1.4038 - acc: 0.4675 - val_loss: 1.3711 - val_acc: 0.4213\n",
      "Epoch 22/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 1.3892 - acc: 0.4805 - val_loss: 1.3633 - val_acc: 0.4330\n",
      "Epoch 23/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3744 - acc: 0.4933 - val_loss: 1.3193 - val_acc: 0.5233\n",
      "Epoch 24/1500\n",
      "30246/30246 [==============================] - 1s 42us/step - loss: 1.3590 - acc: 0.5067 - val_loss: 1.3800 - val_acc: 0.4033\n",
      "Epoch 25/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3434 - acc: 0.5189 - val_loss: 1.3431 - val_acc: 0.4435\n",
      "Epoch 26/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 1.3278 - acc: 0.5281 - val_loss: 1.3022 - val_acc: 0.5045\n",
      "Epoch 27/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 1.3119 - acc: 0.5405 - val_loss: 1.2764 - val_acc: 0.5442\n",
      "Epoch 28/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 1.2960 - acc: 0.5506 - val_loss: 1.2584 - val_acc: 0.5570\n",
      "Epoch 29/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 1.2801 - acc: 0.5591 - val_loss: 1.2416 - val_acc: 0.5783\n",
      "Epoch 30/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.2640 - acc: 0.5700 - val_loss: 1.2050 - val_acc: 0.6222\n",
      "Epoch 31/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.2484 - acc: 0.5816 - val_loss: 1.2062 - val_acc: 0.5858\n",
      "Epoch 32/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 1.2328 - acc: 0.5879 - val_loss: 1.1561 - val_acc: 0.6645\n",
      "Epoch 33/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 1.2176 - acc: 0.5992 - val_loss: 1.1818 - val_acc: 0.6355\n",
      "Epoch 34/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 1.2024 - acc: 0.6070 - val_loss: 1.1651 - val_acc: 0.6383\n",
      "Epoch 35/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 1.1874 - acc: 0.6128 - val_loss: 1.1407 - val_acc: 0.6533\n",
      "Epoch 36/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.1731 - acc: 0.6221 - val_loss: 1.1291 - val_acc: 0.6624\n",
      "Epoch 37/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.1588 - acc: 0.6259 - val_loss: 1.1311 - val_acc: 0.6590\n",
      "Epoch 38/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 1.1448 - acc: 0.6323 - val_loss: 1.0881 - val_acc: 0.6936\n",
      "Epoch 39/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 1.1312 - acc: 0.6383 - val_loss: 1.0974 - val_acc: 0.6711\n",
      "Epoch 40/1500\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 1.1180 - acc: 0.6422 - val_loss: 1.0450 - val_acc: 0.7215\n",
      "Epoch 41/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 1.1052 - acc: 0.6467 - val_loss: 1.0002 - val_acc: 0.7552\n",
      "Epoch 42/1500\n",
      "30246/30246 [==============================] - 1s 33us/step - loss: 1.0928 - acc: 0.6516 - val_loss: 1.0778 - val_acc: 0.7001\n",
      "Epoch 43/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 1.0806 - acc: 0.6563 - val_loss: 0.9917 - val_acc: 0.7572\n",
      "Epoch 44/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 1.0689 - acc: 0.6602 - val_loss: 1.0110 - val_acc: 0.7338\n",
      "Epoch 45/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 1.0576 - acc: 0.6638 - val_loss: 1.0068 - val_acc: 0.7413\n",
      "Epoch 46/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 1.0467 - acc: 0.6697 - val_loss: 0.9926 - val_acc: 0.7444\n",
      "Epoch 47/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 1.0361 - acc: 0.6728 - val_loss: 0.9752 - val_acc: 0.7514\n",
      "Epoch 48/1500\n",
      "30246/30246 [==============================] - 1s 47us/step - loss: 1.0258 - acc: 0.6765 - val_loss: 0.9377 - val_acc: 0.7677\n",
      "Epoch 49/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 1.0159 - acc: 0.6802 - val_loss: 0.9714 - val_acc: 0.7505\n",
      "Epoch 50/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.0063 - acc: 0.6837 - val_loss: 0.9252 - val_acc: 0.7720\n",
      "Epoch 51/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9968 - acc: 0.6882 - val_loss: 0.9015 - val_acc: 0.7856\n",
      "Epoch 52/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9877 - acc: 0.6930 - val_loss: 0.9124 - val_acc: 0.7760\n",
      "Epoch 53/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9790 - acc: 0.6950 - val_loss: 0.9291 - val_acc: 0.7647\n",
      "Epoch 54/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9707 - acc: 0.6982 - val_loss: 0.9092 - val_acc: 0.7708\n",
      "Epoch 55/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9625 - acc: 0.7006 - val_loss: 0.9459 - val_acc: 0.7502\n",
      "Epoch 56/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.9546 - acc: 0.7034 - val_loss: 0.8873 - val_acc: 0.7829\n",
      "Epoch 57/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.9468 - acc: 0.7064 - val_loss: 0.8688 - val_acc: 0.7924\n",
      "Epoch 58/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9392 - acc: 0.7088 - val_loss: 0.9513 - val_acc: 0.7482\n",
      "Epoch 59/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.9323 - acc: 0.7124 - val_loss: 0.9092 - val_acc: 0.7665\n",
      "Epoch 60/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9252 - acc: 0.7136 - val_loss: 0.8981 - val_acc: 0.7678\n",
      "Epoch 61/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9186 - acc: 0.7164 - val_loss: 0.8155 - val_acc: 0.8133\n",
      "Epoch 62/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9121 - acc: 0.7181 - val_loss: 0.8115 - val_acc: 0.8129\n",
      "Epoch 63/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9060 - acc: 0.7215 - val_loss: 0.8802 - val_acc: 0.7715\n",
      "Epoch 64/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8998 - acc: 0.7228 - val_loss: 0.7939 - val_acc: 0.8151\n",
      "Epoch 65/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8940 - acc: 0.7246 - val_loss: 0.8156 - val_acc: 0.8006\n",
      "Epoch 66/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8882 - acc: 0.7259 - val_loss: 0.8215 - val_acc: 0.7967\n",
      "Epoch 67/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8827 - acc: 0.7276 - val_loss: 0.7832 - val_acc: 0.8168\n",
      "Epoch 68/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8775 - acc: 0.7294 - val_loss: 0.8694 - val_acc: 0.7727\n",
      "Epoch 69/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8722 - acc: 0.7315 - val_loss: 0.7965 - val_acc: 0.8072\n",
      "Epoch 70/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8670 - acc: 0.7330 - val_loss: 0.8424 - val_acc: 0.7782\n",
      "Epoch 71/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8622 - acc: 0.7351 - val_loss: 0.8221 - val_acc: 0.7899\n",
      "Epoch 72/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8578 - acc: 0.7365 - val_loss: 0.7878 - val_acc: 0.8076\n",
      "Epoch 73/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8531 - acc: 0.7378 - val_loss: 0.8081 - val_acc: 0.7928\n",
      "Epoch 74/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8484 - acc: 0.7397 - val_loss: 0.7811 - val_acc: 0.8081\n",
      "Epoch 75/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8441 - acc: 0.7419 - val_loss: 0.7546 - val_acc: 0.8203\n",
      "Epoch 76/1500\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.8402 - acc: 0.7432 - val_loss: 0.7213 - val_acc: 0.8342\n",
      "Epoch 77/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8358 - acc: 0.7445 - val_loss: 0.7884 - val_acc: 0.8018\n",
      "Epoch 78/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8320 - acc: 0.7463 - val_loss: 0.7378 - val_acc: 0.8250\n",
      "Epoch 79/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.7478 - val_loss: 0.7499 - val_acc: 0.8171\n",
      "Epoch 80/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8244 - acc: 0.7482 - val_loss: 0.7423 - val_acc: 0.8190\n",
      "Epoch 81/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.8205 - acc: 0.7488 - val_loss: 0.7602 - val_acc: 0.8120\n",
      "Epoch 82/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8172 - acc: 0.7514 - val_loss: 0.7831 - val_acc: 0.7986\n",
      "Epoch 83/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8138 - acc: 0.7526 - val_loss: 0.7404 - val_acc: 0.8172\n",
      "Epoch 84/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8104 - acc: 0.7524 - val_loss: 0.7837 - val_acc: 0.7981\n",
      "Epoch 85/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8070 - acc: 0.7536 - val_loss: 0.8005 - val_acc: 0.7870\n",
      "Epoch 86/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8037 - acc: 0.7549 - val_loss: 0.7605 - val_acc: 0.8073\n",
      "Epoch 87/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8006 - acc: 0.7565 - val_loss: 0.7165 - val_acc: 0.8250\n",
      "Epoch 88/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7974 - acc: 0.7559 - val_loss: 0.6985 - val_acc: 0.8336\n",
      "Epoch 89/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7943 - acc: 0.7592 - val_loss: 0.7253 - val_acc: 0.8186\n",
      "Epoch 90/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7915 - acc: 0.7594 - val_loss: 0.7366 - val_acc: 0.8146\n",
      "Epoch 91/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7886 - acc: 0.7604 - val_loss: 0.7405 - val_acc: 0.8147\n",
      "Epoch 92/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7858 - acc: 0.7601 - val_loss: 0.7499 - val_acc: 0.8088\n",
      "Epoch 93/1500\n",
      "30246/30246 [==============================] - 2s 58us/step - loss: 0.7833 - acc: 0.7616 - val_loss: 0.7472 - val_acc: 0.8080\n",
      "Epoch 94/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7806 - acc: 0.7622 - val_loss: 0.7018 - val_acc: 0.8290\n",
      "Epoch 95/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7779 - acc: 0.7626 - val_loss: 0.7343 - val_acc: 0.8141\n",
      "Epoch 96/1500\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 0.7752 - acc: 0.7643 - val_loss: 0.6706 - val_acc: 0.8397\n",
      "Epoch 97/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.7726 - acc: 0.7654 - val_loss: 0.7191 - val_acc: 0.8171\n",
      "Epoch 98/1500\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 0.7702 - acc: 0.7659 - val_loss: 0.7172 - val_acc: 0.8192\n",
      "Epoch 99/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7677 - acc: 0.7662 - val_loss: 0.7079 - val_acc: 0.8223\n",
      "Epoch 100/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7651 - acc: 0.7678 - val_loss: 0.6764 - val_acc: 0.8339\n",
      "Epoch 101/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7630 - acc: 0.7685 - val_loss: 0.6833 - val_acc: 0.8307\n",
      "Epoch 102/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7609 - acc: 0.7691 - val_loss: 0.7360 - val_acc: 0.8061\n",
      "Epoch 103/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7585 - acc: 0.7700 - val_loss: 0.6823 - val_acc: 0.8301\n",
      "Epoch 104/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7560 - acc: 0.7709 - val_loss: 0.6994 - val_acc: 0.8223\n",
      "Epoch 105/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7543 - acc: 0.7718 - val_loss: 0.7051 - val_acc: 0.8213\n",
      "Epoch 106/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7519 - acc: 0.7723 - val_loss: 0.6758 - val_acc: 0.8342\n",
      "Epoch 107/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7498 - acc: 0.7729 - val_loss: 0.7255 - val_acc: 0.8086\n",
      "Epoch 108/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7481 - acc: 0.7747 - val_loss: 0.6959 - val_acc: 0.8246\n",
      "Epoch 109/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.7455 - acc: 0.7743 - val_loss: 0.6506 - val_acc: 0.8421\n",
      "Epoch 110/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7441 - acc: 0.7755 - val_loss: 0.7135 - val_acc: 0.8159\n",
      "Epoch 111/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.7421 - acc: 0.7758 - val_loss: 0.6872 - val_acc: 0.8291\n",
      "Epoch 112/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7401 - acc: 0.7769 - val_loss: 0.6830 - val_acc: 0.8256\n",
      "Epoch 113/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7384 - acc: 0.7768 - val_loss: 0.6958 - val_acc: 0.8233\n",
      "Epoch 114/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7364 - acc: 0.7779 - val_loss: 0.7078 - val_acc: 0.8180\n",
      "Epoch 115/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7347 - acc: 0.7784 - val_loss: 0.6856 - val_acc: 0.8254\n",
      "Epoch 116/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7327 - acc: 0.7790 - val_loss: 0.7063 - val_acc: 0.8163\n",
      "Epoch 117/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7312 - acc: 0.7795 - val_loss: 0.7292 - val_acc: 0.8027\n",
      "Epoch 118/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7290 - acc: 0.7799 - val_loss: 0.6972 - val_acc: 0.8130\n",
      "Epoch 119/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.7278 - acc: 0.7802 - val_loss: 0.6737 - val_acc: 0.8286\n",
      "Epoch 120/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7258 - acc: 0.7817 - val_loss: 0.7055 - val_acc: 0.8174\n",
      "Epoch 121/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7242 - acc: 0.7823 - val_loss: 0.6066 - val_acc: 0.8600\n",
      "Epoch 122/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7227 - acc: 0.7830 - val_loss: 0.6791 - val_acc: 0.8260\n",
      "Epoch 123/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7211 - acc: 0.7834 - val_loss: 0.6758 - val_acc: 0.8273\n",
      "Epoch 124/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7192 - acc: 0.7840 - val_loss: 0.7090 - val_acc: 0.8112\n",
      "Epoch 125/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7177 - acc: 0.7842 - val_loss: 0.7246 - val_acc: 0.8073\n",
      "Epoch 126/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7159 - acc: 0.7861 - val_loss: 0.6521 - val_acc: 0.8393\n",
      "Epoch 127/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7147 - acc: 0.7853 - val_loss: 0.6974 - val_acc: 0.8190\n",
      "Epoch 128/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7132 - acc: 0.7856 - val_loss: 0.6213 - val_acc: 0.8483\n",
      "Epoch 129/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.7114 - acc: 0.7856 - val_loss: 0.6903 - val_acc: 0.8220\n",
      "Epoch 130/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7100 - acc: 0.7869 - val_loss: 0.6286 - val_acc: 0.8466\n",
      "Epoch 131/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7085 - acc: 0.7867 - val_loss: 0.7342 - val_acc: 0.7991\n",
      "Epoch 132/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7070 - acc: 0.7875 - val_loss: 0.6126 - val_acc: 0.8516\n",
      "Epoch 133/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7059 - acc: 0.7874 - val_loss: 0.6933 - val_acc: 0.8187\n",
      "Epoch 134/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7041 - acc: 0.7878 - val_loss: 0.6438 - val_acc: 0.8384\n",
      "Epoch 135/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7030 - acc: 0.7904 - val_loss: 0.7486 - val_acc: 0.7932\n",
      "Epoch 136/1500\n",
      "30246/30246 [==============================] - 1s 33us/step - loss: 0.7017 - acc: 0.7890 - val_loss: 0.5950 - val_acc: 0.8604\n",
      "Epoch 137/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7001 - acc: 0.7891 - val_loss: 0.6993 - val_acc: 0.8129\n",
      "Epoch 138/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6987 - acc: 0.7912 - val_loss: 0.6276 - val_acc: 0.8450\n",
      "Epoch 139/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6975 - acc: 0.7909 - val_loss: 0.6846 - val_acc: 0.8208\n",
      "Epoch 140/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6962 - acc: 0.7908 - val_loss: 0.6951 - val_acc: 0.8186\n",
      "Epoch 141/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6948 - acc: 0.7922 - val_loss: 0.6309 - val_acc: 0.8428\n",
      "Epoch 142/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6933 - acc: 0.7928 - val_loss: 0.6599 - val_acc: 0.8302\n",
      "Epoch 143/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6922 - acc: 0.7929 - val_loss: 0.6300 - val_acc: 0.8442\n",
      "Epoch 144/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6908 - acc: 0.7935 - val_loss: 0.6290 - val_acc: 0.8421\n",
      "Epoch 145/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6896 - acc: 0.7926 - val_loss: 0.6315 - val_acc: 0.8421\n",
      "Epoch 146/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6882 - acc: 0.7946 - val_loss: 0.6581 - val_acc: 0.8309\n",
      "Epoch 147/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6871 - acc: 0.7939 - val_loss: 0.6575 - val_acc: 0.8319\n",
      "Epoch 148/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6859 - acc: 0.7947 - val_loss: 0.6758 - val_acc: 0.8209\n",
      "Epoch 149/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6848 - acc: 0.7947 - val_loss: 0.6777 - val_acc: 0.8203\n",
      "Epoch 150/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6836 - acc: 0.7948 - val_loss: 0.6913 - val_acc: 0.8180\n",
      "Epoch 151/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6823 - acc: 0.7964 - val_loss: 0.6612 - val_acc: 0.8299\n",
      "Epoch 152/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6809 - acc: 0.7969 - val_loss: 0.6484 - val_acc: 0.8355\n",
      "Epoch 153/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6798 - acc: 0.7974 - val_loss: 0.6257 - val_acc: 0.8441\n",
      "Epoch 154/1500\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 0.6786 - acc: 0.7957 - val_loss: 0.5817 - val_acc: 0.8629\n",
      "Epoch 155/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6775 - acc: 0.7979 - val_loss: 0.6258 - val_acc: 0.8436\n",
      "Epoch 156/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6763 - acc: 0.7972 - val_loss: 0.6853 - val_acc: 0.8208\n",
      "Epoch 157/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.6754 - acc: 0.7977 - val_loss: 0.6585 - val_acc: 0.8293\n",
      "Epoch 158/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6740 - acc: 0.7981 - val_loss: 0.6356 - val_acc: 0.8412\n",
      "Epoch 159/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6729 - acc: 0.7987 - val_loss: 0.6641 - val_acc: 0.8272\n",
      "Epoch 160/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6718 - acc: 0.7987 - val_loss: 0.6338 - val_acc: 0.8395\n",
      "Epoch 161/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6706 - acc: 0.7982 - val_loss: 0.6481 - val_acc: 0.8358\n",
      "Epoch 162/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6694 - acc: 0.7996 - val_loss: 0.6101 - val_acc: 0.8506\n",
      "Epoch 163/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6684 - acc: 0.7998 - val_loss: 0.6042 - val_acc: 0.8532\n",
      "Epoch 164/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6672 - acc: 0.8003 - val_loss: 0.6157 - val_acc: 0.8444\n",
      "Epoch 165/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6662 - acc: 0.8007 - val_loss: 0.6289 - val_acc: 0.8422\n",
      "Epoch 166/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6651 - acc: 0.8005 - val_loss: 0.6286 - val_acc: 0.8428\n",
      "Epoch 167/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6641 - acc: 0.8013 - val_loss: 0.6446 - val_acc: 0.8356\n",
      "Epoch 168/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6631 - acc: 0.8009 - val_loss: 0.5842 - val_acc: 0.8605\n",
      "Epoch 169/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6618 - acc: 0.8021 - val_loss: 0.6581 - val_acc: 0.8306\n",
      "Epoch 170/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6610 - acc: 0.8022 - val_loss: 0.6117 - val_acc: 0.8488\n",
      "Epoch 171/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6599 - acc: 0.8022 - val_loss: 0.6187 - val_acc: 0.8454\n",
      "Epoch 172/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6590 - acc: 0.8027 - val_loss: 0.6674 - val_acc: 0.8256\n",
      "Epoch 173/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6578 - acc: 0.8031 - val_loss: 0.6579 - val_acc: 0.8311\n",
      "Epoch 174/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6565 - acc: 0.8023 - val_loss: 0.5832 - val_acc: 0.8601\n",
      "Epoch 175/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6557 - acc: 0.8037 - val_loss: 0.6464 - val_acc: 0.8350\n",
      "Epoch 176/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6549 - acc: 0.8041 - val_loss: 0.5778 - val_acc: 0.8635\n",
      "Epoch 177/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6537 - acc: 0.8052 - val_loss: 0.5921 - val_acc: 0.8555\n",
      "Epoch 178/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6526 - acc: 0.8041 - val_loss: 0.5949 - val_acc: 0.8577\n",
      "Epoch 179/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6520 - acc: 0.8054 - val_loss: 0.6110 - val_acc: 0.8483\n",
      "Epoch 180/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6509 - acc: 0.8060 - val_loss: 0.6535 - val_acc: 0.8294\n",
      "Epoch 181/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6495 - acc: 0.8051 - val_loss: 0.6373 - val_acc: 0.8376\n",
      "Epoch 182/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6487 - acc: 0.8064 - val_loss: 0.5611 - val_acc: 0.8691\n",
      "Epoch 183/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6476 - acc: 0.8073 - val_loss: 0.6155 - val_acc: 0.8451\n",
      "Epoch 184/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6469 - acc: 0.8065 - val_loss: 0.6025 - val_acc: 0.8490\n",
      "Epoch 185/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6459 - acc: 0.8078 - val_loss: 0.5925 - val_acc: 0.8564\n",
      "Epoch 186/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6447 - acc: 0.8075 - val_loss: 0.5433 - val_acc: 0.8765\n",
      "Epoch 187/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6441 - acc: 0.8072 - val_loss: 0.5924 - val_acc: 0.8557\n",
      "Epoch 188/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6431 - acc: 0.8082 - val_loss: 0.5754 - val_acc: 0.8635\n",
      "Epoch 189/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6420 - acc: 0.8082 - val_loss: 0.5779 - val_acc: 0.8622\n",
      "Epoch 190/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6409 - acc: 0.8091 - val_loss: 0.5875 - val_acc: 0.8573\n",
      "Epoch 191/1500\n",
      "30246/30246 [==============================] - 2s 55us/step - loss: 0.6400 - acc: 0.8090 - val_loss: 0.5776 - val_acc: 0.8630\n",
      "Epoch 192/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6393 - acc: 0.8090 - val_loss: 0.6276 - val_acc: 0.8432\n",
      "Epoch 193/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6383 - acc: 0.8092 - val_loss: 0.6228 - val_acc: 0.8440\n",
      "Epoch 194/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6372 - acc: 0.8095 - val_loss: 0.6082 - val_acc: 0.8486\n",
      "Epoch 195/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.6366 - acc: 0.8098 - val_loss: 0.6143 - val_acc: 0.8449\n",
      "Epoch 196/1500\n",
      "30246/30246 [==============================] - 1s 44us/step - loss: 0.6353 - acc: 0.8090 - val_loss: 0.5670 - val_acc: 0.8654\n",
      "Epoch 197/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6345 - acc: 0.8111 - val_loss: 0.6396 - val_acc: 0.8351\n",
      "Epoch 198/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6336 - acc: 0.8105 - val_loss: 0.5832 - val_acc: 0.8602\n",
      "Epoch 199/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.6327 - acc: 0.8110 - val_loss: 0.5925 - val_acc: 0.8533\n",
      "Epoch 200/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6319 - acc: 0.8106 - val_loss: 0.5924 - val_acc: 0.8553\n",
      "Epoch 201/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6311 - acc: 0.8119 - val_loss: 0.5756 - val_acc: 0.8615\n",
      "Epoch 202/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6302 - acc: 0.8121 - val_loss: 0.6468 - val_acc: 0.8325\n",
      "Epoch 203/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6295 - acc: 0.8123 - val_loss: 0.5411 - val_acc: 0.8760\n",
      "Epoch 204/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6285 - acc: 0.8134 - val_loss: 0.6329 - val_acc: 0.8367\n",
      "Epoch 205/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6272 - acc: 0.8135 - val_loss: 0.6480 - val_acc: 0.8298\n",
      "Epoch 206/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6267 - acc: 0.8124 - val_loss: 0.5825 - val_acc: 0.8596\n",
      "Epoch 207/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6260 - acc: 0.8128 - val_loss: 0.6163 - val_acc: 0.8428\n",
      "Epoch 208/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6247 - acc: 0.8136 - val_loss: 0.6484 - val_acc: 0.8314\n",
      "Epoch 209/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6240 - acc: 0.8138 - val_loss: 0.5869 - val_acc: 0.8578\n",
      "Epoch 210/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6231 - acc: 0.8131 - val_loss: 0.6714 - val_acc: 0.8244\n",
      "Epoch 211/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6224 - acc: 0.8143 - val_loss: 0.5811 - val_acc: 0.8597\n",
      "Epoch 212/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6219 - acc: 0.8145 - val_loss: 0.5896 - val_acc: 0.8561\n",
      "Epoch 213/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6205 - acc: 0.8153 - val_loss: 0.6347 - val_acc: 0.8363\n",
      "Epoch 214/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6198 - acc: 0.8158 - val_loss: 0.6101 - val_acc: 0.8481\n",
      "Epoch 215/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6194 - acc: 0.8155 - val_loss: 0.6042 - val_acc: 0.8492\n",
      "Epoch 216/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6183 - acc: 0.8154 - val_loss: 0.6220 - val_acc: 0.8436\n",
      "Epoch 217/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6174 - acc: 0.8160 - val_loss: 0.5511 - val_acc: 0.8719\n",
      "Epoch 218/1500\n",
      "30246/30246 [==============================] - 1s 42us/step - loss: 0.6169 - acc: 0.8159 - val_loss: 0.5575 - val_acc: 0.8650\n",
      "Epoch 219/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6157 - acc: 0.8156 - val_loss: 0.5749 - val_acc: 0.8621\n",
      "Epoch 220/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6151 - acc: 0.8170 - val_loss: 0.6134 - val_acc: 0.8434\n",
      "Epoch 221/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6142 - acc: 0.8178 - val_loss: 0.5776 - val_acc: 0.8594\n",
      "Epoch 222/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6132 - acc: 0.8177 - val_loss: 0.5345 - val_acc: 0.8768\n",
      "Epoch 223/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.6125 - acc: 0.8173 - val_loss: 0.5548 - val_acc: 0.8691\n",
      "Epoch 224/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6117 - acc: 0.8184 - val_loss: 0.6172 - val_acc: 0.8424\n",
      "Epoch 225/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6105 - acc: 0.8188 - val_loss: 0.5814 - val_acc: 0.8568\n",
      "Epoch 226/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.6100 - acc: 0.8189 - val_loss: 0.6145 - val_acc: 0.8441\n",
      "Epoch 227/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6089 - acc: 0.8188 - val_loss: 0.5963 - val_acc: 0.8508\n",
      "Epoch 228/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6085 - acc: 0.8188 - val_loss: 0.6294 - val_acc: 0.8407\n",
      "Epoch 229/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6078 - acc: 0.8195 - val_loss: 0.5622 - val_acc: 0.8664\n",
      "Epoch 230/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.6067 - acc: 0.8193 - val_loss: 0.6021 - val_acc: 0.8500\n",
      "Epoch 231/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6061 - acc: 0.8197 - val_loss: 0.6228 - val_acc: 0.8418\n",
      "Epoch 232/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6051 - acc: 0.8206 - val_loss: 0.6211 - val_acc: 0.8438\n",
      "Epoch 233/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.6046 - acc: 0.8211 - val_loss: 0.5735 - val_acc: 0.8604\n",
      "Epoch 234/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6035 - acc: 0.8200 - val_loss: 0.5932 - val_acc: 0.8518\n",
      "Epoch 235/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6025 - acc: 0.8218 - val_loss: 0.5166 - val_acc: 0.8823\n",
      "Epoch 236/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6022 - acc: 0.8218 - val_loss: 0.4937 - val_acc: 0.8924\n",
      "Epoch 237/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6014 - acc: 0.8221 - val_loss: 0.5657 - val_acc: 0.8631\n",
      "Epoch 238/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6002 - acc: 0.8212 - val_loss: 0.5848 - val_acc: 0.8568\n",
      "Epoch 239/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5997 - acc: 0.8228 - val_loss: 0.5651 - val_acc: 0.8652\n",
      "Epoch 240/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5991 - acc: 0.8225 - val_loss: 0.5413 - val_acc: 0.8737\n",
      "Epoch 241/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5983 - acc: 0.8231 - val_loss: 0.5509 - val_acc: 0.8701\n",
      "Epoch 242/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5975 - acc: 0.8236 - val_loss: 0.5872 - val_acc: 0.8560\n",
      "Epoch 243/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5967 - acc: 0.8233 - val_loss: 0.5839 - val_acc: 0.8578\n",
      "Epoch 244/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5962 - acc: 0.8232 - val_loss: 0.5686 - val_acc: 0.8619\n",
      "Epoch 245/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5953 - acc: 0.8232 - val_loss: 0.5710 - val_acc: 0.8627\n",
      "Epoch 246/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5945 - acc: 0.8247 - val_loss: 0.5429 - val_acc: 0.8734\n",
      "Epoch 247/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5938 - acc: 0.8239 - val_loss: 0.5718 - val_acc: 0.8615\n",
      "Epoch 248/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5930 - acc: 0.8256 - val_loss: 0.5999 - val_acc: 0.8494\n",
      "Epoch 249/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5922 - acc: 0.8248 - val_loss: 0.6190 - val_acc: 0.8426\n",
      "Epoch 250/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5916 - acc: 0.8251 - val_loss: 0.5791 - val_acc: 0.8582\n",
      "Epoch 251/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5909 - acc: 0.8251 - val_loss: 0.5514 - val_acc: 0.8697\n",
      "Epoch 252/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5895 - acc: 0.8257 - val_loss: 0.5870 - val_acc: 0.8559\n",
      "Epoch 253/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5888 - acc: 0.8266 - val_loss: 0.6071 - val_acc: 0.8491\n",
      "Epoch 254/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5886 - acc: 0.8264 - val_loss: 0.5009 - val_acc: 0.8893\n",
      "Epoch 255/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5877 - acc: 0.8266 - val_loss: 0.5758 - val_acc: 0.8605\n",
      "Epoch 256/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5873 - acc: 0.8269 - val_loss: 0.5581 - val_acc: 0.8663\n",
      "Epoch 257/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5864 - acc: 0.8272 - val_loss: 0.5494 - val_acc: 0.8734\n",
      "Epoch 258/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5860 - acc: 0.8271 - val_loss: 0.5514 - val_acc: 0.8695\n",
      "Epoch 259/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5851 - acc: 0.8267 - val_loss: 0.6014 - val_acc: 0.8503\n",
      "Epoch 260/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5843 - acc: 0.8285 - val_loss: 0.4969 - val_acc: 0.8889\n",
      "Epoch 261/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5835 - acc: 0.8282 - val_loss: 0.5511 - val_acc: 0.8690\n",
      "Epoch 262/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5828 - acc: 0.8282 - val_loss: 0.5617 - val_acc: 0.8645\n",
      "Epoch 263/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5821 - acc: 0.8296 - val_loss: 0.5176 - val_acc: 0.8824\n",
      "Epoch 264/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5817 - acc: 0.8286 - val_loss: 0.5952 - val_acc: 0.8526\n",
      "Epoch 265/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5807 - acc: 0.8294 - val_loss: 0.5847 - val_acc: 0.8561\n",
      "Epoch 266/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5798 - acc: 0.8306 - val_loss: 0.4980 - val_acc: 0.8887\n",
      "Epoch 267/1500\n",
      "30246/30246 [==============================] - 2s 53us/step - loss: 0.5794 - acc: 0.8297 - val_loss: 0.5828 - val_acc: 0.8580\n",
      "Epoch 268/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.5785 - acc: 0.8300 - val_loss: 0.4940 - val_acc: 0.8884\n",
      "Epoch 269/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5780 - acc: 0.8301 - val_loss: 0.5777 - val_acc: 0.8594\n",
      "Epoch 270/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.5771 - acc: 0.8299 - val_loss: 0.5613 - val_acc: 0.8659\n",
      "Epoch 271/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.5765 - acc: 0.8306 - val_loss: 0.5460 - val_acc: 0.8707\n",
      "Epoch 272/1500\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 0.5757 - acc: 0.8302 - val_loss: 0.5786 - val_acc: 0.8596\n",
      "Epoch 273/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.5750 - acc: 0.8314 - val_loss: 0.5014 - val_acc: 0.8875\n",
      "Epoch 274/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.5744 - acc: 0.8321 - val_loss: 0.5667 - val_acc: 0.8619\n",
      "Epoch 275/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5737 - acc: 0.8313 - val_loss: 0.5460 - val_acc: 0.8712\n",
      "Epoch 276/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5729 - acc: 0.8323 - val_loss: 0.5416 - val_acc: 0.8738\n",
      "Epoch 277/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5718 - acc: 0.8322 - val_loss: 0.5392 - val_acc: 0.8736\n",
      "Epoch 278/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5718 - acc: 0.8325 - val_loss: 0.5399 - val_acc: 0.8721\n",
      "Epoch 279/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5708 - acc: 0.8330 - val_loss: 0.4882 - val_acc: 0.8894\n",
      "Epoch 280/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.5700 - acc: 0.8334 - val_loss: 0.5465 - val_acc: 0.8704\n",
      "Epoch 281/1500\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.5691 - acc: 0.8327 - val_loss: 0.5494 - val_acc: 0.8717\n",
      "Epoch 282/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5687 - acc: 0.8329 - val_loss: 0.6028 - val_acc: 0.8473\n",
      "Epoch 283/1500\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.5680 - acc: 0.8333 - val_loss: 0.5473 - val_acc: 0.8684\n",
      "Epoch 284/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.5670 - acc: 0.8343 - val_loss: 0.5219 - val_acc: 0.8801\n",
      "Epoch 285/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5667 - acc: 0.8343 - val_loss: 0.5577 - val_acc: 0.8671\n",
      "Epoch 286/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5658 - acc: 0.8332 - val_loss: 0.5055 - val_acc: 0.8839\n",
      "Epoch 287/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5650 - acc: 0.8353 - val_loss: 0.5814 - val_acc: 0.8563\n",
      "Epoch 288/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5643 - acc: 0.8340 - val_loss: 0.5407 - val_acc: 0.8721\n",
      "Epoch 289/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5636 - acc: 0.8345 - val_loss: 0.5847 - val_acc: 0.8567\n",
      "Epoch 290/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.5629 - acc: 0.8362 - val_loss: 0.5279 - val_acc: 0.8766\n",
      "Epoch 291/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.5624 - acc: 0.8350 - val_loss: 0.5160 - val_acc: 0.8812\n",
      "Epoch 292/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5617 - acc: 0.8357 - val_loss: 0.6136 - val_acc: 0.8440\n",
      "Epoch 293/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5612 - acc: 0.8354 - val_loss: 0.5862 - val_acc: 0.8545\n",
      "Epoch 294/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5603 - acc: 0.8369 - val_loss: 0.5299 - val_acc: 0.8753\n",
      "Epoch 295/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5595 - acc: 0.8364 - val_loss: 0.5261 - val_acc: 0.8764\n",
      "Epoch 296/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5585 - acc: 0.8365 - val_loss: 0.5372 - val_acc: 0.8720\n",
      "Epoch 297/1500\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.5586 - acc: 0.8369 - val_loss: 0.5277 - val_acc: 0.8779\n",
      "Epoch 298/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5579 - acc: 0.8385 - val_loss: 0.5455 - val_acc: 0.8667\n",
      "Epoch 299/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.5572 - acc: 0.8376 - val_loss: 0.5838 - val_acc: 0.8532\n",
      "Epoch 300/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5566 - acc: 0.8378 - val_loss: 0.5890 - val_acc: 0.8516\n",
      "Epoch 301/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5555 - acc: 0.8389 - val_loss: 0.5155 - val_acc: 0.8790\n",
      "Epoch 302/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5554 - acc: 0.8394 - val_loss: 0.5432 - val_acc: 0.8696\n",
      "Epoch 303/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5545 - acc: 0.8398 - val_loss: 0.5042 - val_acc: 0.8819\n",
      "Epoch 304/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5534 - acc: 0.8394 - val_loss: 0.5792 - val_acc: 0.8574\n",
      "Epoch 305/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5531 - acc: 0.8393 - val_loss: 0.5608 - val_acc: 0.8642\n",
      "Epoch 306/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5524 - acc: 0.8383 - val_loss: 0.4961 - val_acc: 0.8857\n",
      "Epoch 307/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5518 - acc: 0.8396 - val_loss: 0.5047 - val_acc: 0.8822\n",
      "Epoch 308/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5510 - acc: 0.8402 - val_loss: 0.5633 - val_acc: 0.8622\n",
      "Epoch 309/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5506 - acc: 0.8404 - val_loss: 0.5484 - val_acc: 0.8664\n",
      "Epoch 310/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5495 - acc: 0.8416 - val_loss: 0.4998 - val_acc: 0.8838\n",
      "Epoch 311/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5491 - acc: 0.8411 - val_loss: 0.5010 - val_acc: 0.8847\n",
      "Epoch 312/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5488 - acc: 0.8412 - val_loss: 0.5404 - val_acc: 0.8664\n",
      "Epoch 313/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5480 - acc: 0.8409 - val_loss: 0.5153 - val_acc: 0.8789\n",
      "Epoch 314/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5470 - acc: 0.8417 - val_loss: 0.5900 - val_acc: 0.8524\n",
      "Epoch 315/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5463 - acc: 0.8418 - val_loss: 0.5524 - val_acc: 0.8688\n",
      "Epoch 316/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5463 - acc: 0.8416 - val_loss: 0.4817 - val_acc: 0.8905\n",
      "Epoch 317/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5453 - acc: 0.8430 - val_loss: 0.5406 - val_acc: 0.8695\n",
      "Epoch 318/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5449 - acc: 0.8419 - val_loss: 0.5514 - val_acc: 0.8672\n",
      "Epoch 319/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5439 - acc: 0.8412 - val_loss: 0.5414 - val_acc: 0.8692\n",
      "Epoch 320/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5438 - acc: 0.8432 - val_loss: 0.5508 - val_acc: 0.8658\n",
      "Epoch 321/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5427 - acc: 0.8432 - val_loss: 0.4996 - val_acc: 0.8832\n",
      "Epoch 322/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5426 - acc: 0.8432 - val_loss: 0.5612 - val_acc: 0.8615\n",
      "Epoch 323/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5417 - acc: 0.8422 - val_loss: 0.5495 - val_acc: 0.8671\n",
      "Epoch 324/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5409 - acc: 0.8439 - val_loss: 0.5565 - val_acc: 0.8656\n",
      "Epoch 325/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5406 - acc: 0.8435 - val_loss: 0.5550 - val_acc: 0.8630\n",
      "Epoch 326/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5397 - acc: 0.8439 - val_loss: 0.4831 - val_acc: 0.8910\n",
      "Epoch 327/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5392 - acc: 0.8432 - val_loss: 0.5119 - val_acc: 0.8785\n",
      "Epoch 328/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5386 - acc: 0.8442 - val_loss: 0.5708 - val_acc: 0.8611\n",
      "Epoch 329/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5376 - acc: 0.8442 - val_loss: 0.5379 - val_acc: 0.8678\n",
      "Epoch 330/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5375 - acc: 0.8451 - val_loss: 0.5121 - val_acc: 0.8795\n",
      "Epoch 331/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5367 - acc: 0.8457 - val_loss: 0.5736 - val_acc: 0.8533\n",
      "Epoch 332/1500\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.5357 - acc: 0.8455 - val_loss: 0.5085 - val_acc: 0.8828\n",
      "Epoch 333/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5352 - acc: 0.8452 - val_loss: 0.5180 - val_acc: 0.8761\n",
      "Epoch 334/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5347 - acc: 0.8452 - val_loss: 0.5436 - val_acc: 0.8667\n",
      "Epoch 335/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5344 - acc: 0.8453 - val_loss: 0.5175 - val_acc: 0.8761\n",
      "Epoch 336/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5339 - acc: 0.8455 - val_loss: 0.5366 - val_acc: 0.8696\n",
      "Epoch 337/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.5331 - acc: 0.8458 - val_loss: 0.5361 - val_acc: 0.8683\n",
      "Epoch 338/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5325 - acc: 0.8465 - val_loss: 0.5717 - val_acc: 0.8605\n",
      "Epoch 339/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.5318 - acc: 0.8450 - val_loss: 0.5032 - val_acc: 0.8818\n",
      "Epoch 340/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5311 - acc: 0.8453 - val_loss: 0.5409 - val_acc: 0.8690\n",
      "Epoch 341/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5306 - acc: 0.8474 - val_loss: 0.5220 - val_acc: 0.8766\n",
      "Epoch 342/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5304 - acc: 0.8464 - val_loss: 0.5308 - val_acc: 0.8719\n",
      "Epoch 343/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5296 - acc: 0.8471 - val_loss: 0.4659 - val_acc: 0.8938\n",
      "Epoch 344/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5290 - acc: 0.8470 - val_loss: 0.5225 - val_acc: 0.8748\n",
      "Epoch 345/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5284 - acc: 0.8479 - val_loss: 0.5847 - val_acc: 0.8519\n",
      "Epoch 346/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5276 - acc: 0.8468 - val_loss: 0.5481 - val_acc: 0.8647\n",
      "Epoch 347/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5273 - acc: 0.8469 - val_loss: 0.4823 - val_acc: 0.8889\n",
      "Epoch 348/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5266 - acc: 0.8466 - val_loss: 0.5134 - val_acc: 0.8771\n",
      "Epoch 349/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5260 - acc: 0.8483 - val_loss: 0.5897 - val_acc: 0.8520\n",
      "Epoch 350/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5255 - acc: 0.8483 - val_loss: 0.5272 - val_acc: 0.8703\n",
      "Epoch 351/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5248 - acc: 0.8484 - val_loss: 0.5298 - val_acc: 0.8696\n",
      "Epoch 352/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5242 - acc: 0.8488 - val_loss: 0.4707 - val_acc: 0.8913\n",
      "Epoch 353/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5236 - acc: 0.8485 - val_loss: 0.5216 - val_acc: 0.8740\n",
      "Epoch 354/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5230 - acc: 0.8483 - val_loss: 0.5594 - val_acc: 0.8638\n",
      "Epoch 355/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5230 - acc: 0.8492 - val_loss: 0.5268 - val_acc: 0.8748\n",
      "Epoch 356/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5223 - acc: 0.8485 - val_loss: 0.5067 - val_acc: 0.8803\n",
      "Epoch 357/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5214 - acc: 0.8499 - val_loss: 0.4961 - val_acc: 0.8835\n",
      "Epoch 358/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5209 - acc: 0.8495 - val_loss: 0.5182 - val_acc: 0.8758\n",
      "Epoch 359/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5206 - acc: 0.8501 - val_loss: 0.5005 - val_acc: 0.8820\n",
      "Epoch 360/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5199 - acc: 0.8495 - val_loss: 0.5360 - val_acc: 0.8703\n",
      "Epoch 361/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5192 - acc: 0.8500 - val_loss: 0.5528 - val_acc: 0.8629\n",
      "Epoch 362/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5188 - acc: 0.8500 - val_loss: 0.5233 - val_acc: 0.8716\n",
      "Epoch 363/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5181 - acc: 0.8506 - val_loss: 0.5682 - val_acc: 0.8567\n",
      "Epoch 364/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.5177 - acc: 0.8500 - val_loss: 0.5115 - val_acc: 0.8793\n",
      "Epoch 365/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5172 - acc: 0.8508 - val_loss: 0.4582 - val_acc: 0.8951\n",
      "Epoch 366/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5163 - acc: 0.8500 - val_loss: 0.4813 - val_acc: 0.8883\n",
      "Epoch 367/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5157 - acc: 0.8516 - val_loss: 0.4914 - val_acc: 0.8823\n",
      "Epoch 368/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5155 - acc: 0.8509 - val_loss: 0.5421 - val_acc: 0.8650\n",
      "Epoch 369/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5148 - acc: 0.8518 - val_loss: 0.5074 - val_acc: 0.8777\n",
      "Epoch 370/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5143 - acc: 0.8522 - val_loss: 0.5528 - val_acc: 0.8631\n",
      "Epoch 371/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5136 - acc: 0.8513 - val_loss: 0.5209 - val_acc: 0.8736\n",
      "Epoch 372/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5129 - acc: 0.8525 - val_loss: 0.4895 - val_acc: 0.8857\n",
      "Epoch 373/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5125 - acc: 0.8524 - val_loss: 0.5106 - val_acc: 0.8782\n",
      "Epoch 374/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5122 - acc: 0.8522 - val_loss: 0.5224 - val_acc: 0.8734\n",
      "Epoch 375/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5116 - acc: 0.8517 - val_loss: 0.4978 - val_acc: 0.8824\n",
      "Epoch 376/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5112 - acc: 0.8524 - val_loss: 0.5294 - val_acc: 0.8697\n",
      "Epoch 377/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5102 - acc: 0.8524 - val_loss: 0.5033 - val_acc: 0.8794\n",
      "Epoch 378/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.5098 - acc: 0.8535 - val_loss: 0.4880 - val_acc: 0.8861\n",
      "Epoch 379/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5096 - acc: 0.8535 - val_loss: 0.5155 - val_acc: 0.8757\n",
      "Epoch 380/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5091 - acc: 0.8535 - val_loss: 0.5360 - val_acc: 0.8691\n",
      "Epoch 381/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5085 - acc: 0.8539 - val_loss: 0.5638 - val_acc: 0.8609\n",
      "Epoch 382/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5075 - acc: 0.8531 - val_loss: 0.4544 - val_acc: 0.8976\n",
      "Epoch 383/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5072 - acc: 0.8540 - val_loss: 0.5009 - val_acc: 0.8810\n",
      "Epoch 384/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5068 - acc: 0.8537 - val_loss: 0.4588 - val_acc: 0.8965\n",
      "Epoch 385/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5061 - acc: 0.8541 - val_loss: 0.5177 - val_acc: 0.8744\n",
      "Epoch 386/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5057 - acc: 0.8545 - val_loss: 0.5050 - val_acc: 0.8775\n",
      "Epoch 387/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5051 - acc: 0.8553 - val_loss: 0.4923 - val_acc: 0.8819\n",
      "Epoch 388/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5046 - acc: 0.8542 - val_loss: 0.4842 - val_acc: 0.8880\n",
      "Epoch 389/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5042 - acc: 0.8545 - val_loss: 0.4957 - val_acc: 0.8823\n",
      "Epoch 390/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5035 - acc: 0.8555 - val_loss: 0.5029 - val_acc: 0.8830\n",
      "Epoch 391/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5028 - acc: 0.8545 - val_loss: 0.5408 - val_acc: 0.8668\n",
      "Epoch 392/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5022 - acc: 0.8554 - val_loss: 0.4711 - val_acc: 0.8906\n",
      "Epoch 393/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5022 - acc: 0.8555 - val_loss: 0.4856 - val_acc: 0.8843\n",
      "Epoch 394/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5016 - acc: 0.8552 - val_loss: 0.5377 - val_acc: 0.8676\n",
      "Epoch 395/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5010 - acc: 0.8565 - val_loss: 0.5369 - val_acc: 0.8651\n",
      "Epoch 396/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5001 - acc: 0.8558 - val_loss: 0.5232 - val_acc: 0.8721\n",
      "Epoch 397/1500\n",
      "30246/30246 [==============================] - 1s 44us/step - loss: 0.4998 - acc: 0.8564 - val_loss: 0.4845 - val_acc: 0.8847\n",
      "Epoch 398/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4992 - acc: 0.8567 - val_loss: 0.4756 - val_acc: 0.8896\n",
      "Epoch 399/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4990 - acc: 0.8570 - val_loss: 0.5281 - val_acc: 0.8711\n",
      "Epoch 400/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4985 - acc: 0.8563 - val_loss: 0.5064 - val_acc: 0.8778\n",
      "Epoch 401/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4976 - acc: 0.8574 - val_loss: 0.4733 - val_acc: 0.8889\n",
      "Epoch 402/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4972 - acc: 0.8573 - val_loss: 0.5579 - val_acc: 0.8609\n",
      "Epoch 403/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4970 - acc: 0.8574 - val_loss: 0.4848 - val_acc: 0.8843\n",
      "Epoch 404/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4965 - acc: 0.8567 - val_loss: 0.4901 - val_acc: 0.8838\n",
      "Epoch 405/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4954 - acc: 0.8572 - val_loss: 0.4771 - val_acc: 0.8856\n",
      "Epoch 406/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4955 - acc: 0.8573 - val_loss: 0.4873 - val_acc: 0.8846\n",
      "Epoch 407/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4949 - acc: 0.8584 - val_loss: 0.4786 - val_acc: 0.8879\n",
      "Epoch 408/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.4940 - acc: 0.8585 - val_loss: 0.5151 - val_acc: 0.8748\n",
      "Epoch 409/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4936 - acc: 0.8578 - val_loss: 0.4511 - val_acc: 0.8967\n",
      "Epoch 410/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4933 - acc: 0.8579 - val_loss: 0.4979 - val_acc: 0.8795\n",
      "Epoch 411/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4928 - acc: 0.8579 - val_loss: 0.5271 - val_acc: 0.8715\n",
      "Epoch 412/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4923 - acc: 0.8588 - val_loss: 0.5286 - val_acc: 0.8699\n",
      "Epoch 413/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4918 - acc: 0.8586 - val_loss: 0.5159 - val_acc: 0.8749\n",
      "Epoch 414/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4915 - acc: 0.8596 - val_loss: 0.4830 - val_acc: 0.8853\n",
      "Epoch 415/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4905 - acc: 0.8605 - val_loss: 0.4532 - val_acc: 0.8955\n",
      "Epoch 416/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4902 - acc: 0.8588 - val_loss: 0.4753 - val_acc: 0.8901\n",
      "Epoch 417/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4898 - acc: 0.8588 - val_loss: 0.4884 - val_acc: 0.8836\n",
      "Epoch 418/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4894 - acc: 0.8590 - val_loss: 0.4937 - val_acc: 0.8807\n",
      "Epoch 419/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4887 - acc: 0.8593 - val_loss: 0.4557 - val_acc: 0.8943\n",
      "Epoch 420/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4881 - acc: 0.8596 - val_loss: 0.4754 - val_acc: 0.8884\n",
      "Epoch 421/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4880 - acc: 0.8595 - val_loss: 0.5580 - val_acc: 0.8601\n",
      "Epoch 422/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4873 - acc: 0.8592 - val_loss: 0.4998 - val_acc: 0.8795\n",
      "Epoch 423/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4868 - acc: 0.8609 - val_loss: 0.5120 - val_acc: 0.8738\n",
      "Epoch 424/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4863 - acc: 0.8601 - val_loss: 0.4422 - val_acc: 0.8996\n",
      "Epoch 425/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4860 - acc: 0.8601 - val_loss: 0.5059 - val_acc: 0.8768\n",
      "Epoch 426/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4852 - acc: 0.8606 - val_loss: 0.4860 - val_acc: 0.8844\n",
      "Epoch 427/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4849 - acc: 0.8603 - val_loss: 0.4888 - val_acc: 0.8842\n",
      "Epoch 428/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4845 - acc: 0.8617 - val_loss: 0.5458 - val_acc: 0.8658\n",
      "Epoch 429/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4842 - acc: 0.8617 - val_loss: 0.4974 - val_acc: 0.8811\n",
      "Epoch 430/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4836 - acc: 0.8615 - val_loss: 0.5573 - val_acc: 0.8601\n",
      "Epoch 431/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4829 - acc: 0.8625 - val_loss: 0.4808 - val_acc: 0.8859\n",
      "Epoch 432/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4824 - acc: 0.8617 - val_loss: 0.4870 - val_acc: 0.8850\n",
      "Epoch 433/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4820 - acc: 0.8615 - val_loss: 0.5005 - val_acc: 0.8787\n",
      "Epoch 434/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4815 - acc: 0.8615 - val_loss: 0.4338 - val_acc: 0.9020\n",
      "Epoch 435/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4811 - acc: 0.8617 - val_loss: 0.5047 - val_acc: 0.8771\n",
      "Epoch 436/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4803 - acc: 0.8628 - val_loss: 0.5055 - val_acc: 0.8774\n",
      "Epoch 437/1500\n",
      "30246/30246 [==============================] - 2s 59us/step - loss: 0.4800 - acc: 0.8630 - val_loss: 0.5056 - val_acc: 0.8762\n",
      "Epoch 438/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4795 - acc: 0.8628 - val_loss: 0.5134 - val_acc: 0.8736\n",
      "Epoch 439/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4792 - acc: 0.8627 - val_loss: 0.4759 - val_acc: 0.8861\n",
      "Epoch 440/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4785 - acc: 0.8621 - val_loss: 0.5246 - val_acc: 0.8697\n",
      "Epoch 441/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4776 - acc: 0.8628 - val_loss: 0.4422 - val_acc: 0.8988\n",
      "Epoch 442/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4781 - acc: 0.8620 - val_loss: 0.5087 - val_acc: 0.8758\n",
      "Epoch 443/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4774 - acc: 0.8638 - val_loss: 0.4883 - val_acc: 0.8826\n",
      "Epoch 444/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4770 - acc: 0.8626 - val_loss: 0.4997 - val_acc: 0.8795\n",
      "Epoch 445/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4761 - acc: 0.8630 - val_loss: 0.5184 - val_acc: 0.8715\n",
      "Epoch 446/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4760 - acc: 0.8637 - val_loss: 0.4869 - val_acc: 0.8824\n",
      "Epoch 447/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4753 - acc: 0.8634 - val_loss: 0.4950 - val_acc: 0.8801\n",
      "Epoch 448/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4745 - acc: 0.8637 - val_loss: 0.5222 - val_acc: 0.8697\n",
      "Epoch 449/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4744 - acc: 0.8635 - val_loss: 0.5342 - val_acc: 0.8674\n",
      "Epoch 450/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4740 - acc: 0.8635 - val_loss: 0.4914 - val_acc: 0.8789\n",
      "Epoch 451/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4735 - acc: 0.8640 - val_loss: 0.5282 - val_acc: 0.8696\n",
      "Epoch 452/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4726 - acc: 0.8642 - val_loss: 0.5701 - val_acc: 0.8570\n",
      "Epoch 453/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4726 - acc: 0.8647 - val_loss: 0.5143 - val_acc: 0.8721\n",
      "Epoch 454/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4723 - acc: 0.8644 - val_loss: 0.5198 - val_acc: 0.8695\n",
      "Epoch 455/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4716 - acc: 0.8641 - val_loss: 0.5139 - val_acc: 0.8728\n",
      "Epoch 456/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4712 - acc: 0.8651 - val_loss: 0.5059 - val_acc: 0.8749\n",
      "Epoch 457/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4703 - acc: 0.8651 - val_loss: 0.5090 - val_acc: 0.8749\n",
      "Epoch 458/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4702 - acc: 0.8649 - val_loss: 0.5235 - val_acc: 0.8705\n",
      "Epoch 459/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4697 - acc: 0.8656 - val_loss: 0.5314 - val_acc: 0.8672\n",
      "Epoch 460/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4690 - acc: 0.8653 - val_loss: 0.4345 - val_acc: 0.8995\n",
      "Epoch 461/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4685 - acc: 0.8653 - val_loss: 0.5235 - val_acc: 0.8697\n",
      "Epoch 462/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4682 - acc: 0.8656 - val_loss: 0.5022 - val_acc: 0.8765\n",
      "Epoch 463/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4679 - acc: 0.8666 - val_loss: 0.5064 - val_acc: 0.8770\n",
      "Epoch 464/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4674 - acc: 0.8657 - val_loss: 0.3946 - val_acc: 0.9125\n",
      "Epoch 465/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4669 - acc: 0.8660 - val_loss: 0.5003 - val_acc: 0.8807\n",
      "Epoch 466/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4665 - acc: 0.8654 - val_loss: 0.4738 - val_acc: 0.8861\n",
      "Epoch 467/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4660 - acc: 0.8663 - val_loss: 0.4824 - val_acc: 0.8848\n",
      "Epoch 468/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4650 - acc: 0.8662 - val_loss: 0.5016 - val_acc: 0.8769\n",
      "Epoch 469/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4649 - acc: 0.8666 - val_loss: 0.4888 - val_acc: 0.8794\n",
      "Epoch 470/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.4647 - acc: 0.8670 - val_loss: 0.4792 - val_acc: 0.8853\n",
      "Epoch 471/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4642 - acc: 0.8668 - val_loss: 0.4532 - val_acc: 0.8931\n",
      "Epoch 472/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4640 - acc: 0.8673 - val_loss: 0.5182 - val_acc: 0.8711\n",
      "Epoch 473/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4636 - acc: 0.8675 - val_loss: 0.4322 - val_acc: 0.8996\n",
      "Epoch 474/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4631 - acc: 0.8672 - val_loss: 0.5468 - val_acc: 0.8609\n",
      "Epoch 475/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4623 - acc: 0.8676 - val_loss: 0.3807 - val_acc: 0.9163\n",
      "Epoch 476/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4623 - acc: 0.8677 - val_loss: 0.4822 - val_acc: 0.8835\n",
      "Epoch 477/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4617 - acc: 0.8683 - val_loss: 0.4960 - val_acc: 0.8782\n",
      "Epoch 478/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4615 - acc: 0.8678 - val_loss: 0.5529 - val_acc: 0.8586\n",
      "Epoch 479/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4609 - acc: 0.8689 - val_loss: 0.4435 - val_acc: 0.8974\n",
      "Epoch 480/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4601 - acc: 0.8679 - val_loss: 0.4972 - val_acc: 0.8778\n",
      "Epoch 481/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4598 - acc: 0.8688 - val_loss: 0.5031 - val_acc: 0.8745\n",
      "Epoch 482/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4592 - acc: 0.8695 - val_loss: 0.5047 - val_acc: 0.8754\n",
      "Epoch 483/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4586 - acc: 0.8692 - val_loss: 0.5018 - val_acc: 0.8761\n",
      "Epoch 484/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4584 - acc: 0.8700 - val_loss: 0.5026 - val_acc: 0.8765\n",
      "Epoch 485/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4583 - acc: 0.8692 - val_loss: 0.5366 - val_acc: 0.8652\n",
      "Epoch 486/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4577 - acc: 0.8700 - val_loss: 0.4978 - val_acc: 0.8775\n",
      "Epoch 487/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4573 - acc: 0.8699 - val_loss: 0.4709 - val_acc: 0.8877\n",
      "Epoch 488/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4571 - acc: 0.8698 - val_loss: 0.5302 - val_acc: 0.8662\n",
      "Epoch 489/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4565 - acc: 0.8697 - val_loss: 0.4720 - val_acc: 0.8847\n",
      "Epoch 490/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.4562 - acc: 0.8686 - val_loss: 0.4365 - val_acc: 0.8980\n",
      "Epoch 491/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4559 - acc: 0.8699 - val_loss: 0.5209 - val_acc: 0.8703\n",
      "Epoch 492/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4549 - acc: 0.8700 - val_loss: 0.5191 - val_acc: 0.8705\n",
      "Epoch 493/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4544 - acc: 0.8709 - val_loss: 0.5076 - val_acc: 0.8756\n",
      "Epoch 494/1500\n",
      "30246/30246 [==============================] - ETA: 0s - loss: 0.4581 - acc: 0.869 - 0s 15us/step - loss: 0.4545 - acc: 0.8709 - val_loss: 0.5127 - val_acc: 0.8737\n",
      "Epoch 495/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4538 - acc: 0.8705 - val_loss: 0.5052 - val_acc: 0.8742\n",
      "Epoch 496/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4534 - acc: 0.8700 - val_loss: 0.4650 - val_acc: 0.8879\n",
      "Epoch 497/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.4531 - acc: 0.8706 - val_loss: 0.5656 - val_acc: 0.8553\n",
      "Epoch 498/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4527 - acc: 0.8711 - val_loss: 0.4577 - val_acc: 0.8898\n",
      "Epoch 499/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4525 - acc: 0.8704 - val_loss: 0.4426 - val_acc: 0.8938\n",
      "Epoch 500/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4519 - acc: 0.8700 - val_loss: 0.4592 - val_acc: 0.8893\n",
      "Epoch 501/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4512 - acc: 0.8708 - val_loss: 0.4324 - val_acc: 0.8999\n",
      "Epoch 502/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.4513 - acc: 0.8717 - val_loss: 0.5037 - val_acc: 0.8741\n",
      "Epoch 503/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4506 - acc: 0.8711 - val_loss: 0.5638 - val_acc: 0.8556\n",
      "Epoch 504/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4503 - acc: 0.8715 - val_loss: 0.4786 - val_acc: 0.8831\n",
      "Epoch 505/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4502 - acc: 0.8721 - val_loss: 0.4375 - val_acc: 0.8971\n",
      "Epoch 506/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4494 - acc: 0.8726 - val_loss: 0.4513 - val_acc: 0.8912\n",
      "Epoch 507/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4491 - acc: 0.8724 - val_loss: 0.4960 - val_acc: 0.8781\n",
      "Epoch 508/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4489 - acc: 0.8719 - val_loss: 0.4557 - val_acc: 0.8937\n",
      "Epoch 509/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4481 - acc: 0.8725 - val_loss: 0.5187 - val_acc: 0.8688\n",
      "Epoch 510/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4477 - acc: 0.8721 - val_loss: 0.4962 - val_acc: 0.8774\n",
      "Epoch 511/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4475 - acc: 0.8724 - val_loss: 0.5153 - val_acc: 0.8713\n",
      "Epoch 512/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4470 - acc: 0.8724 - val_loss: 0.5038 - val_acc: 0.8737\n",
      "Epoch 513/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4464 - acc: 0.8732 - val_loss: 0.4373 - val_acc: 0.8969\n",
      "Epoch 514/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4461 - acc: 0.8720 - val_loss: 0.5423 - val_acc: 0.8623\n",
      "Epoch 515/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4458 - acc: 0.8727 - val_loss: 0.4966 - val_acc: 0.8783\n",
      "Epoch 516/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4455 - acc: 0.8729 - val_loss: 0.4667 - val_acc: 0.8876\n",
      "Epoch 517/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.4444 - acc: 0.8726 - val_loss: 0.4452 - val_acc: 0.8971\n",
      "Epoch 518/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4447 - acc: 0.8727 - val_loss: 0.5069 - val_acc: 0.8725\n",
      "Epoch 519/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4441 - acc: 0.8737 - val_loss: 0.4341 - val_acc: 0.8978\n",
      "Epoch 520/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4438 - acc: 0.8729 - val_loss: 0.4655 - val_acc: 0.8871\n",
      "Epoch 521/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4429 - acc: 0.8754 - val_loss: 0.4777 - val_acc: 0.8823\n",
      "Epoch 522/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4428 - acc: 0.8737 - val_loss: 0.4865 - val_acc: 0.8809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 523/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4424 - acc: 0.8740 - val_loss: 0.4979 - val_acc: 0.8765\n",
      "Epoch 524/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.4419 - acc: 0.8741 - val_loss: 0.4543 - val_acc: 0.8906\n",
      "Epoch 525/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4414 - acc: 0.8744 - val_loss: 0.4811 - val_acc: 0.8818\n",
      "Epoch 526/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4417 - acc: 0.8751 - val_loss: 0.4804 - val_acc: 0.8824\n",
      "Epoch 527/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4409 - acc: 0.8740 - val_loss: 0.4930 - val_acc: 0.8786\n",
      "Epoch 528/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4406 - acc: 0.8745 - val_loss: 0.4996 - val_acc: 0.8757\n",
      "Epoch 529/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4404 - acc: 0.8747 - val_loss: 0.4868 - val_acc: 0.8802\n",
      "Epoch 530/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4393 - acc: 0.8757 - val_loss: 0.4735 - val_acc: 0.8847\n",
      "Epoch 531/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4395 - acc: 0.8755 - val_loss: 0.4926 - val_acc: 0.8766\n",
      "Epoch 532/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4390 - acc: 0.8753 - val_loss: 0.5056 - val_acc: 0.8740\n",
      "Epoch 533/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4388 - acc: 0.8749 - val_loss: 0.4893 - val_acc: 0.8781\n",
      "Epoch 534/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4383 - acc: 0.8754 - val_loss: 0.5141 - val_acc: 0.8704\n",
      "Epoch 535/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4378 - acc: 0.8749 - val_loss: 0.4660 - val_acc: 0.8868\n",
      "Epoch 536/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4377 - acc: 0.8759 - val_loss: 0.5082 - val_acc: 0.8719\n",
      "Epoch 537/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4364 - acc: 0.8760 - val_loss: 0.4920 - val_acc: 0.8774\n",
      "Epoch 538/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4368 - acc: 0.8769 - val_loss: 0.4935 - val_acc: 0.8777\n",
      "Epoch 539/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4363 - acc: 0.8760 - val_loss: 0.4337 - val_acc: 0.8974\n",
      "Epoch 540/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4358 - acc: 0.8762 - val_loss: 0.5045 - val_acc: 0.8741\n",
      "Epoch 541/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4354 - acc: 0.8764 - val_loss: 0.4097 - val_acc: 0.9033\n",
      "Epoch 542/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4347 - acc: 0.8766 - val_loss: 0.4830 - val_acc: 0.8794\n",
      "Epoch 543/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4350 - acc: 0.8761 - val_loss: 0.4553 - val_acc: 0.8892\n",
      "Epoch 544/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4345 - acc: 0.8756 - val_loss: 0.4521 - val_acc: 0.8905\n",
      "Epoch 545/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4339 - acc: 0.8772 - val_loss: 0.4594 - val_acc: 0.8872\n",
      "Epoch 546/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4332 - acc: 0.8770 - val_loss: 0.4889 - val_acc: 0.8799\n",
      "Epoch 547/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4332 - acc: 0.8770 - val_loss: 0.5020 - val_acc: 0.8749\n",
      "Epoch 548/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4333 - acc: 0.8767 - val_loss: 0.5042 - val_acc: 0.8724\n",
      "Epoch 549/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4326 - acc: 0.8766 - val_loss: 0.4632 - val_acc: 0.8879\n",
      "Epoch 550/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4325 - acc: 0.8774 - val_loss: 0.4526 - val_acc: 0.8891\n",
      "Epoch 551/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4322 - acc: 0.8769 - val_loss: 0.4791 - val_acc: 0.8834\n",
      "Epoch 552/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4317 - acc: 0.8769 - val_loss: 0.4465 - val_acc: 0.8929\n",
      "Epoch 553/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4313 - acc: 0.8777 - val_loss: 0.4603 - val_acc: 0.8884\n",
      "Epoch 554/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4309 - acc: 0.8775 - val_loss: 0.5082 - val_acc: 0.8741\n",
      "Epoch 555/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4303 - acc: 0.8771 - val_loss: 0.4910 - val_acc: 0.8794\n",
      "Epoch 556/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4300 - acc: 0.8769 - val_loss: 0.4673 - val_acc: 0.8867\n",
      "Epoch 557/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4299 - acc: 0.8779 - val_loss: 0.4499 - val_acc: 0.8906\n",
      "Epoch 558/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4292 - acc: 0.8794 - val_loss: 0.5112 - val_acc: 0.8736\n",
      "Epoch 559/1500\n",
      "30246/30246 [==============================] - ETA: 0s - loss: 0.4294 - acc: 0.878 - 0s 15us/step - loss: 0.4288 - acc: 0.8782 - val_loss: 0.4699 - val_acc: 0.8867\n",
      "Epoch 560/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4287 - acc: 0.8773 - val_loss: 0.4486 - val_acc: 0.8910\n",
      "Epoch 561/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4280 - acc: 0.8787 - val_loss: 0.5060 - val_acc: 0.8746\n",
      "Epoch 562/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4278 - acc: 0.8779 - val_loss: 0.4747 - val_acc: 0.8830\n",
      "Epoch 563/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.4277 - acc: 0.8776 - val_loss: 0.4503 - val_acc: 0.8908\n",
      "Epoch 564/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4273 - acc: 0.8784 - val_loss: 0.4645 - val_acc: 0.8873\n",
      "Epoch 565/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4271 - acc: 0.8783 - val_loss: 0.4396 - val_acc: 0.8971\n",
      "Epoch 566/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.4266 - acc: 0.8788 - val_loss: 0.4823 - val_acc: 0.8819\n",
      "Epoch 567/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4266 - acc: 0.8793 - val_loss: 0.4624 - val_acc: 0.8893\n",
      "Epoch 568/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4260 - acc: 0.8789 - val_loss: 0.4830 - val_acc: 0.8810\n",
      "Epoch 569/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4259 - acc: 0.8792 - val_loss: 0.5124 - val_acc: 0.8723\n",
      "Epoch 570/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4251 - acc: 0.8792 - val_loss: 0.5206 - val_acc: 0.8708\n",
      "Epoch 571/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4253 - acc: 0.8790 - val_loss: 0.4573 - val_acc: 0.8871\n",
      "Epoch 572/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4245 - acc: 0.8798 - val_loss: 0.4352 - val_acc: 0.8959\n",
      "Epoch 573/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4243 - acc: 0.8799 - val_loss: 0.4196 - val_acc: 0.9023\n",
      "Epoch 574/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4238 - acc: 0.8810 - val_loss: 0.3964 - val_acc: 0.9114\n",
      "Epoch 575/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4237 - acc: 0.8798 - val_loss: 0.4842 - val_acc: 0.8812\n",
      "Epoch 576/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4234 - acc: 0.8793 - val_loss: 0.4462 - val_acc: 0.8922\n",
      "Epoch 577/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4233 - acc: 0.8797 - val_loss: 0.4230 - val_acc: 0.8999\n",
      "Epoch 578/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4224 - acc: 0.8808 - val_loss: 0.5023 - val_acc: 0.8745\n",
      "Epoch 579/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4223 - acc: 0.8809 - val_loss: 0.4955 - val_acc: 0.8769\n",
      "Epoch 580/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4220 - acc: 0.8805 - val_loss: 0.4602 - val_acc: 0.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 581/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4217 - acc: 0.8794 - val_loss: 0.4791 - val_acc: 0.8810\n",
      "Epoch 582/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4213 - acc: 0.8801 - val_loss: 0.4577 - val_acc: 0.8901\n",
      "Epoch 583/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4213 - acc: 0.8812 - val_loss: 0.4927 - val_acc: 0.8761\n",
      "Epoch 584/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4206 - acc: 0.8806 - val_loss: 0.4554 - val_acc: 0.8897\n",
      "Epoch 585/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4208 - acc: 0.8806 - val_loss: 0.5263 - val_acc: 0.8671\n",
      "Epoch 586/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4201 - acc: 0.8807 - val_loss: 0.4390 - val_acc: 0.8949\n",
      "Epoch 587/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4197 - acc: 0.8820 - val_loss: 0.4322 - val_acc: 0.8991\n",
      "Epoch 588/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4194 - acc: 0.8814 - val_loss: 0.4721 - val_acc: 0.8869\n",
      "Epoch 589/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4190 - acc: 0.8807 - val_loss: 0.4631 - val_acc: 0.8856\n",
      "Epoch 590/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4186 - acc: 0.8821 - val_loss: 0.4865 - val_acc: 0.8790\n",
      "Epoch 591/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4184 - acc: 0.8814 - val_loss: 0.5125 - val_acc: 0.8720\n",
      "Epoch 592/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4184 - acc: 0.8817 - val_loss: 0.4686 - val_acc: 0.8851\n",
      "Epoch 593/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4181 - acc: 0.8812 - val_loss: 0.3695 - val_acc: 0.9173\n",
      "Epoch 594/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4179 - acc: 0.8817 - val_loss: 0.4675 - val_acc: 0.8868\n",
      "Epoch 595/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4174 - acc: 0.8811 - val_loss: 0.4793 - val_acc: 0.8807\n",
      "Epoch 596/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4170 - acc: 0.8815 - val_loss: 0.4578 - val_acc: 0.8898\n",
      "Epoch 597/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4166 - acc: 0.8818 - val_loss: 0.4524 - val_acc: 0.8929\n",
      "Epoch 598/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4163 - acc: 0.8812 - val_loss: 0.4759 - val_acc: 0.8830\n",
      "Epoch 599/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4161 - acc: 0.8821 - val_loss: 0.5169 - val_acc: 0.8712\n",
      "Epoch 600/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4160 - acc: 0.8822 - val_loss: 0.4791 - val_acc: 0.8831\n",
      "Epoch 601/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4150 - acc: 0.8825 - val_loss: 0.4077 - val_acc: 0.9089\n",
      "Epoch 602/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4147 - acc: 0.8821 - val_loss: 0.4486 - val_acc: 0.8917\n",
      "Epoch 603/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4150 - acc: 0.8831 - val_loss: 0.4711 - val_acc: 0.8847\n",
      "Epoch 604/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4145 - acc: 0.8830 - val_loss: 0.4642 - val_acc: 0.8861\n",
      "Epoch 605/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4140 - acc: 0.8827 - val_loss: 0.3917 - val_acc: 0.9111\n",
      "Epoch 606/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4142 - acc: 0.8832 - val_loss: 0.4271 - val_acc: 0.8974\n",
      "Epoch 607/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4132 - acc: 0.8827 - val_loss: 0.4825 - val_acc: 0.8822\n",
      "Epoch 608/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4137 - acc: 0.8827 - val_loss: 0.4998 - val_acc: 0.8725\n",
      "Epoch 609/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4133 - acc: 0.8830 - val_loss: 0.4891 - val_acc: 0.8787\n",
      "Epoch 610/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4130 - acc: 0.8842 - val_loss: 0.4391 - val_acc: 0.8945\n",
      "Epoch 611/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4117 - acc: 0.8832 - val_loss: 0.4155 - val_acc: 0.9041\n",
      "Epoch 612/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8832 - val_loss: 0.5030 - val_acc: 0.8740\n",
      "Epoch 613/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4122 - acc: 0.8840 - val_loss: 0.4735 - val_acc: 0.8852\n",
      "Epoch 614/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4116 - acc: 0.8836 - val_loss: 0.4115 - val_acc: 0.9039\n",
      "Epoch 615/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4116 - acc: 0.8835 - val_loss: 0.4623 - val_acc: 0.8873\n",
      "Epoch 616/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4109 - acc: 0.8842 - val_loss: 0.4056 - val_acc: 0.9077\n",
      "Epoch 617/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4108 - acc: 0.8839 - val_loss: 0.4541 - val_acc: 0.8896\n",
      "Epoch 618/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4103 - acc: 0.8837 - val_loss: 0.4916 - val_acc: 0.8781\n",
      "Epoch 619/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8831 - val_loss: 0.4138 - val_acc: 0.9025\n",
      "Epoch 620/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4100 - acc: 0.8839 - val_loss: 0.4415 - val_acc: 0.8949\n",
      "Epoch 621/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4095 - acc: 0.8845 - val_loss: 0.4681 - val_acc: 0.8847\n",
      "Epoch 622/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4093 - acc: 0.8842 - val_loss: 0.5078 - val_acc: 0.8713\n",
      "Epoch 623/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4090 - acc: 0.8842 - val_loss: 0.4906 - val_acc: 0.8765\n",
      "Epoch 624/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4087 - acc: 0.8842 - val_loss: 0.4668 - val_acc: 0.8868\n",
      "Epoch 625/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4082 - acc: 0.8850 - val_loss: 0.4330 - val_acc: 0.8953\n",
      "Epoch 626/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4083 - acc: 0.8841 - val_loss: 0.4536 - val_acc: 0.8909\n",
      "Epoch 627/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4081 - acc: 0.8838 - val_loss: 0.5407 - val_acc: 0.8623\n",
      "Epoch 628/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4078 - acc: 0.8850 - val_loss: 0.4496 - val_acc: 0.8918\n",
      "Epoch 629/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4074 - acc: 0.8853 - val_loss: 0.4445 - val_acc: 0.8937\n",
      "Epoch 630/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4072 - acc: 0.8855 - val_loss: 0.4520 - val_acc: 0.8905\n",
      "Epoch 631/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.4066 - acc: 0.8853 - val_loss: 0.3704 - val_acc: 0.9173\n",
      "Epoch 632/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4067 - acc: 0.8854 - val_loss: 0.4486 - val_acc: 0.8924\n",
      "Epoch 633/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.4067 - acc: 0.8855 - val_loss: 0.4306 - val_acc: 0.8962\n",
      "Epoch 634/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.4060 - acc: 0.8848 - val_loss: 0.4674 - val_acc: 0.8860\n",
      "Epoch 635/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4058 - acc: 0.8854 - val_loss: 0.5043 - val_acc: 0.8757\n",
      "Epoch 636/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4056 - acc: 0.8859 - val_loss: 0.4416 - val_acc: 0.8957\n",
      "Epoch 637/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4049 - acc: 0.8857 - val_loss: 0.4205 - val_acc: 0.9017\n",
      "Epoch 638/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4048 - acc: 0.8856 - val_loss: 0.4584 - val_acc: 0.8883\n",
      "Epoch 639/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.4050 - acc: 0.8862 - val_loss: 0.4525 - val_acc: 0.8914\n",
      "Epoch 640/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4042 - acc: 0.8852 - val_loss: 0.4694 - val_acc: 0.8836\n",
      "Epoch 641/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4043 - acc: 0.8851 - val_loss: 0.4470 - val_acc: 0.8931\n",
      "Epoch 642/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4041 - acc: 0.8855 - val_loss: 0.4549 - val_acc: 0.8881\n",
      "Epoch 643/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4034 - acc: 0.8850 - val_loss: 0.4731 - val_acc: 0.8848\n",
      "Epoch 644/1500\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.4035 - acc: 0.8849 - val_loss: 0.4134 - val_acc: 0.9025\n",
      "Epoch 645/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.4034 - acc: 0.8857 - val_loss: 0.4437 - val_acc: 0.8931\n",
      "Epoch 646/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.4026 - acc: 0.8857 - val_loss: 0.4332 - val_acc: 0.8975\n",
      "Epoch 647/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4023 - acc: 0.8861 - val_loss: 0.4346 - val_acc: 0.8971\n",
      "Epoch 648/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.4023 - acc: 0.8863 - val_loss: 0.4797 - val_acc: 0.8831\n",
      "Epoch 649/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4017 - acc: 0.8856 - val_loss: 0.4465 - val_acc: 0.8950\n",
      "Epoch 650/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4019 - acc: 0.8854 - val_loss: 0.5260 - val_acc: 0.8664\n",
      "Epoch 651/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4014 - acc: 0.8867 - val_loss: 0.4545 - val_acc: 0.8909\n",
      "Epoch 652/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.4012 - acc: 0.8860 - val_loss: 0.4712 - val_acc: 0.8843\n",
      "Epoch 653/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.4011 - acc: 0.8866 - val_loss: 0.4111 - val_acc: 0.9051\n",
      "Epoch 654/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4008 - acc: 0.8859 - val_loss: 0.4509 - val_acc: 0.8906\n",
      "Epoch 655/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4003 - acc: 0.8868 - val_loss: 0.3972 - val_acc: 0.9084\n",
      "Epoch 656/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3998 - acc: 0.8859 - val_loss: 0.4631 - val_acc: 0.8877\n",
      "Epoch 657/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4005 - acc: 0.8868 - val_loss: 0.4667 - val_acc: 0.8876\n",
      "Epoch 658/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3996 - acc: 0.8869 - val_loss: 0.4481 - val_acc: 0.8930\n",
      "Epoch 659/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3995 - acc: 0.8866 - val_loss: 0.4741 - val_acc: 0.8860\n",
      "Epoch 660/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3991 - acc: 0.8869 - val_loss: 0.4520 - val_acc: 0.8904\n",
      "Epoch 661/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3986 - acc: 0.8874 - val_loss: 0.4112 - val_acc: 0.9028\n",
      "Epoch 662/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3988 - acc: 0.8862 - val_loss: 0.4154 - val_acc: 0.9032\n",
      "Epoch 663/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3986 - acc: 0.8867 - val_loss: 0.5448 - val_acc: 0.8619\n",
      "Epoch 664/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3982 - acc: 0.8868 - val_loss: 0.4753 - val_acc: 0.8843\n",
      "Epoch 665/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3982 - acc: 0.8870 - val_loss: 0.4680 - val_acc: 0.8868\n",
      "Epoch 666/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3975 - acc: 0.8872 - val_loss: 0.4399 - val_acc: 0.8958\n",
      "Epoch 667/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3975 - acc: 0.8873 - val_loss: 0.4455 - val_acc: 0.8937\n",
      "Epoch 668/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3971 - acc: 0.8889 - val_loss: 0.4166 - val_acc: 0.9027\n",
      "Epoch 669/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3971 - acc: 0.8875 - val_loss: 0.4511 - val_acc: 0.8920\n",
      "Epoch 670/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3964 - acc: 0.8872 - val_loss: 0.5160 - val_acc: 0.8717\n",
      "Epoch 671/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3966 - acc: 0.8870 - val_loss: 0.4497 - val_acc: 0.8925\n",
      "Epoch 672/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3959 - acc: 0.8874 - val_loss: 0.4494 - val_acc: 0.8918\n",
      "Epoch 673/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3959 - acc: 0.8877 - val_loss: 0.4729 - val_acc: 0.8846\n",
      "Epoch 674/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3955 - acc: 0.8877 - val_loss: 0.4556 - val_acc: 0.8897\n",
      "Epoch 675/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3957 - acc: 0.8882 - val_loss: 0.4584 - val_acc: 0.8905\n",
      "Epoch 676/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.3952 - acc: 0.8882 - val_loss: 0.4542 - val_acc: 0.8912\n",
      "Epoch 677/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3952 - acc: 0.8873 - val_loss: 0.4760 - val_acc: 0.8846\n",
      "Epoch 678/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3948 - acc: 0.8880 - val_loss: 0.4705 - val_acc: 0.8869\n",
      "Epoch 679/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.3941 - acc: 0.8879 - val_loss: 0.5053 - val_acc: 0.8758\n",
      "Epoch 680/1500\n",
      "30246/30246 [==============================] - 2s 77us/step - loss: 0.3944 - acc: 0.8875 - val_loss: 0.5192 - val_acc: 0.8712\n",
      "Epoch 681/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3937 - acc: 0.8890 - val_loss: 0.4099 - val_acc: 0.9048\n",
      "Epoch 682/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3938 - acc: 0.8880 - val_loss: 0.4466 - val_acc: 0.8924\n",
      "Epoch 683/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3933 - acc: 0.8876 - val_loss: 0.4487 - val_acc: 0.8929\n",
      "Epoch 684/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3935 - acc: 0.8878 - val_loss: 0.4832 - val_acc: 0.8819\n",
      "Epoch 685/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3927 - acc: 0.8880 - val_loss: 0.4988 - val_acc: 0.8753\n",
      "Epoch 686/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3931 - acc: 0.8884 - val_loss: 0.4394 - val_acc: 0.8949\n",
      "Epoch 687/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3923 - acc: 0.8889 - val_loss: 0.4569 - val_acc: 0.8909\n",
      "Epoch 688/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3925 - acc: 0.8892 - val_loss: 0.4728 - val_acc: 0.8842\n",
      "Epoch 689/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3918 - acc: 0.8893 - val_loss: 0.4080 - val_acc: 0.9036\n",
      "Epoch 690/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3918 - acc: 0.8878 - val_loss: 0.4752 - val_acc: 0.8840\n",
      "Epoch 691/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3915 - acc: 0.8888 - val_loss: 0.4176 - val_acc: 0.9002\n",
      "Epoch 692/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3914 - acc: 0.8884 - val_loss: 0.4642 - val_acc: 0.8885\n",
      "Epoch 693/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3909 - acc: 0.8896 - val_loss: 0.4149 - val_acc: 0.9032\n",
      "Epoch 694/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3913 - acc: 0.8890 - val_loss: 0.4226 - val_acc: 0.9003\n",
      "Epoch 695/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3904 - acc: 0.8900 - val_loss: 0.4631 - val_acc: 0.8865\n",
      "Epoch 696/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3906 - acc: 0.8896 - val_loss: 0.4413 - val_acc: 0.8955\n",
      "Epoch 697/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3898 - acc: 0.8892 - val_loss: 0.4649 - val_acc: 0.8865\n",
      "Epoch 698/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3899 - acc: 0.8890 - val_loss: 0.4282 - val_acc: 0.8999\n",
      "Epoch 699/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3893 - acc: 0.8892 - val_loss: 0.4159 - val_acc: 0.9008\n",
      "Epoch 700/1500\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 0.3893 - acc: 0.8895 - val_loss: 0.4179 - val_acc: 0.9024\n",
      "Epoch 701/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3892 - acc: 0.8896 - val_loss: 0.4519 - val_acc: 0.8910\n",
      "Epoch 702/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3891 - acc: 0.8892 - val_loss: 0.4883 - val_acc: 0.8803\n",
      "Epoch 703/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3886 - acc: 0.8892 - val_loss: 0.4491 - val_acc: 0.8900\n",
      "Epoch 704/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3884 - acc: 0.8893 - val_loss: 0.4463 - val_acc: 0.8921\n",
      "Epoch 705/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3884 - acc: 0.8898 - val_loss: 0.4628 - val_acc: 0.8884\n",
      "Epoch 706/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3878 - acc: 0.8895 - val_loss: 0.4497 - val_acc: 0.8941\n",
      "Epoch 707/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3882 - acc: 0.8894 - val_loss: 0.4536 - val_acc: 0.8921\n",
      "Epoch 708/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3876 - acc: 0.8909 - val_loss: 0.4392 - val_acc: 0.8974\n",
      "Epoch 709/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3873 - acc: 0.8894 - val_loss: 0.4396 - val_acc: 0.8954\n",
      "Epoch 710/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3873 - acc: 0.8896 - val_loss: 0.4223 - val_acc: 0.8994\n",
      "Epoch 711/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3872 - acc: 0.8902 - val_loss: 0.4629 - val_acc: 0.8873\n",
      "Epoch 712/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3866 - acc: 0.8897 - val_loss: 0.4393 - val_acc: 0.8963\n",
      "Epoch 713/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3864 - acc: 0.8909 - val_loss: 0.4563 - val_acc: 0.8887\n",
      "Epoch 714/1500\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 0.3862 - acc: 0.8894 - val_loss: 0.4990 - val_acc: 0.8782\n",
      "Epoch 715/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3861 - acc: 0.8902 - val_loss: 0.5026 - val_acc: 0.8737\n",
      "Epoch 716/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3856 - acc: 0.8906 - val_loss: 0.4514 - val_acc: 0.8924\n",
      "Epoch 717/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3855 - acc: 0.8898 - val_loss: 0.4328 - val_acc: 0.8979\n",
      "Epoch 718/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3854 - acc: 0.8899 - val_loss: 0.4487 - val_acc: 0.8935\n",
      "Epoch 719/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3853 - acc: 0.8908 - val_loss: 0.4963 - val_acc: 0.8753\n",
      "Epoch 720/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3848 - acc: 0.8910 - val_loss: 0.4433 - val_acc: 0.8954\n",
      "Epoch 721/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3847 - acc: 0.8908 - val_loss: 0.4556 - val_acc: 0.8889\n",
      "Epoch 722/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3845 - acc: 0.8902 - val_loss: 0.4758 - val_acc: 0.8860\n",
      "Epoch 723/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3846 - acc: 0.8905 - val_loss: 0.4116 - val_acc: 0.9051\n",
      "Epoch 724/1500\n",
      "30246/30246 [==============================] - 2s 51us/step - loss: 0.3835 - acc: 0.8916 - val_loss: 0.4707 - val_acc: 0.8853\n",
      "Epoch 725/1500\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.3836 - acc: 0.8907 - val_loss: 0.4492 - val_acc: 0.8930\n",
      "Epoch 726/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3836 - acc: 0.8905 - val_loss: 0.4482 - val_acc: 0.8929\n",
      "Epoch 727/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3832 - acc: 0.8903 - val_loss: 0.4527 - val_acc: 0.8913\n",
      "Epoch 728/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3830 - acc: 0.8900 - val_loss: 0.4501 - val_acc: 0.8930\n",
      "Epoch 729/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3826 - acc: 0.8916 - val_loss: 0.4170 - val_acc: 0.9019\n",
      "Epoch 730/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3831 - acc: 0.8909 - val_loss: 0.4400 - val_acc: 0.8970\n",
      "Epoch 731/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3827 - acc: 0.8910 - val_loss: 0.4269 - val_acc: 0.8995\n",
      "Epoch 732/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3823 - acc: 0.8922 - val_loss: 0.4331 - val_acc: 0.8991\n",
      "Epoch 733/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3822 - acc: 0.8917 - val_loss: 0.4530 - val_acc: 0.8896\n",
      "Epoch 734/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3823 - acc: 0.8918 - val_loss: 0.4691 - val_acc: 0.8867\n",
      "Epoch 735/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3813 - acc: 0.8914 - val_loss: 0.4571 - val_acc: 0.8887\n",
      "Epoch 736/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3812 - acc: 0.8932 - val_loss: 0.4914 - val_acc: 0.8771\n",
      "Epoch 737/1500\n",
      "30246/30246 [==============================] - 1s 45us/step - loss: 0.3810 - acc: 0.8923 - val_loss: 0.4390 - val_acc: 0.8975\n",
      "Epoch 738/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3810 - acc: 0.8924 - val_loss: 0.4502 - val_acc: 0.8914\n",
      "Epoch 739/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3811 - acc: 0.8926 - val_loss: 0.4427 - val_acc: 0.8971\n",
      "Epoch 740/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3806 - acc: 0.8923 - val_loss: 0.5095 - val_acc: 0.8754\n",
      "Epoch 741/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3799 - acc: 0.8922 - val_loss: 0.4452 - val_acc: 0.8954\n",
      "Epoch 742/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3800 - acc: 0.8923 - val_loss: 0.4612 - val_acc: 0.8893\n",
      "Epoch 743/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3800 - acc: 0.8924 - val_loss: 0.4065 - val_acc: 0.9048\n",
      "Epoch 744/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3798 - acc: 0.8917 - val_loss: 0.4697 - val_acc: 0.8864\n",
      "Epoch 745/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3794 - acc: 0.8929 - val_loss: 0.4507 - val_acc: 0.8918\n",
      "Epoch 746/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3794 - acc: 0.8927 - val_loss: 0.4678 - val_acc: 0.8884\n",
      "Epoch 747/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3788 - acc: 0.8922 - val_loss: 0.4098 - val_acc: 0.9043\n",
      "Epoch 748/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3786 - acc: 0.8919 - val_loss: 0.4186 - val_acc: 0.9024\n",
      "Epoch 749/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3786 - acc: 0.8929 - val_loss: 0.4589 - val_acc: 0.8898\n",
      "Epoch 750/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3785 - acc: 0.8919 - val_loss: 0.4274 - val_acc: 0.8996\n",
      "Epoch 751/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3782 - acc: 0.8922 - val_loss: 0.4856 - val_acc: 0.8828\n",
      "Epoch 752/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3778 - acc: 0.8929 - val_loss: 0.4443 - val_acc: 0.8938\n",
      "Epoch 753/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3779 - acc: 0.8919 - val_loss: 0.4491 - val_acc: 0.8938\n",
      "Epoch 754/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3778 - acc: 0.8927 - val_loss: 0.4744 - val_acc: 0.8830\n",
      "Epoch 755/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3772 - acc: 0.8933 - val_loss: 0.4343 - val_acc: 0.8959\n",
      "Epoch 756/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3772 - acc: 0.8920 - val_loss: 0.4424 - val_acc: 0.8937\n",
      "Epoch 757/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3768 - acc: 0.8937 - val_loss: 0.3964 - val_acc: 0.9097\n",
      "Epoch 758/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3766 - acc: 0.8924 - val_loss: 0.4754 - val_acc: 0.8836\n",
      "Epoch 759/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3759 - acc: 0.8925 - val_loss: 0.4364 - val_acc: 0.8965\n",
      "Epoch 760/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3760 - acc: 0.8932 - val_loss: 0.4108 - val_acc: 0.9033\n",
      "Epoch 761/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3761 - acc: 0.8929 - val_loss: 0.4562 - val_acc: 0.8894\n",
      "Epoch 762/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3757 - acc: 0.8931 - val_loss: 0.4483 - val_acc: 0.8922\n",
      "Epoch 763/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3752 - acc: 0.8931 - val_loss: 0.4263 - val_acc: 0.9008\n",
      "Epoch 764/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3756 - acc: 0.8935 - val_loss: 0.4815 - val_acc: 0.8842\n",
      "Epoch 765/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3757 - acc: 0.8934 - val_loss: 0.4437 - val_acc: 0.8950\n",
      "Epoch 766/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3751 - acc: 0.8930 - val_loss: 0.4320 - val_acc: 0.8974\n",
      "Epoch 767/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3747 - acc: 0.8933 - val_loss: 0.4331 - val_acc: 0.9006\n",
      "Epoch 768/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3746 - acc: 0.8926 - val_loss: 0.4332 - val_acc: 0.8967\n",
      "Epoch 769/1500\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.3744 - acc: 0.8931 - val_loss: 0.4707 - val_acc: 0.8848\n",
      "Epoch 770/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.3745 - acc: 0.8932 - val_loss: 0.4201 - val_acc: 0.8998\n",
      "Epoch 771/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3742 - acc: 0.8933 - val_loss: 0.4177 - val_acc: 0.9004\n",
      "Epoch 772/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3736 - acc: 0.8943 - val_loss: 0.4690 - val_acc: 0.8859\n",
      "Epoch 773/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3738 - acc: 0.8929 - val_loss: 0.4699 - val_acc: 0.8861\n",
      "Epoch 774/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3738 - acc: 0.8938 - val_loss: 0.3857 - val_acc: 0.9114\n",
      "Epoch 775/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3734 - acc: 0.8938 - val_loss: 0.4627 - val_acc: 0.8891\n",
      "Epoch 776/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3730 - acc: 0.8941 - val_loss: 0.4586 - val_acc: 0.8897\n",
      "Epoch 777/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3727 - acc: 0.8933 - val_loss: 0.4255 - val_acc: 0.9010\n",
      "Epoch 778/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3722 - acc: 0.8945 - val_loss: 0.4667 - val_acc: 0.8868\n",
      "Epoch 779/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3728 - acc: 0.8933 - val_loss: 0.4663 - val_acc: 0.8861\n",
      "Epoch 780/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3723 - acc: 0.8938 - val_loss: 0.3906 - val_acc: 0.9085\n",
      "Epoch 781/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3720 - acc: 0.8945 - val_loss: 0.4527 - val_acc: 0.8892\n",
      "Epoch 782/1500\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.3716 - acc: 0.8938 - val_loss: 0.4565 - val_acc: 0.8908\n",
      "Epoch 783/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3715 - acc: 0.8941 - val_loss: 0.4534 - val_acc: 0.8930\n",
      "Epoch 784/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3712 - acc: 0.8945 - val_loss: 0.4670 - val_acc: 0.8859\n",
      "Epoch 785/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3714 - acc: 0.8947 - val_loss: 0.4712 - val_acc: 0.8867\n",
      "Epoch 786/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3711 - acc: 0.8938 - val_loss: 0.4095 - val_acc: 0.9049\n",
      "Epoch 787/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3705 - acc: 0.8939 - val_loss: 0.4578 - val_acc: 0.8904\n",
      "Epoch 788/1500\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.3707 - acc: 0.8952 - val_loss: 0.4633 - val_acc: 0.8883\n",
      "Epoch 789/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3704 - acc: 0.8949 - val_loss: 0.4336 - val_acc: 0.8972\n",
      "Epoch 790/1500\n",
      "30246/30246 [==============================] - 2s 72us/step - loss: 0.3704 - acc: 0.8935 - val_loss: 0.6034 - val_acc: 0.8462\n",
      "Epoch 791/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3704 - acc: 0.8950 - val_loss: 0.4088 - val_acc: 0.9033\n",
      "Epoch 792/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3700 - acc: 0.8949 - val_loss: 0.4636 - val_acc: 0.8873\n",
      "Epoch 793/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3699 - acc: 0.8947 - val_loss: 0.3880 - val_acc: 0.9113\n",
      "Epoch 794/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3697 - acc: 0.8931 - val_loss: 0.4917 - val_acc: 0.8789\n",
      "Epoch 795/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3695 - acc: 0.8946 - val_loss: 0.4348 - val_acc: 0.8979\n",
      "Epoch 796/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3690 - acc: 0.8956 - val_loss: 0.4654 - val_acc: 0.8871\n",
      "Epoch 797/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3692 - acc: 0.8952 - val_loss: 0.4566 - val_acc: 0.8905\n",
      "Epoch 798/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3686 - acc: 0.8959 - val_loss: 0.4530 - val_acc: 0.8920\n",
      "Epoch 799/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3687 - acc: 0.8956 - val_loss: 0.4312 - val_acc: 0.8972\n",
      "Epoch 800/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3685 - acc: 0.8944 - val_loss: 0.3712 - val_acc: 0.9160\n",
      "Epoch 801/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3682 - acc: 0.8959 - val_loss: 0.4191 - val_acc: 0.9017\n",
      "Epoch 802/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3679 - acc: 0.8952 - val_loss: 0.4108 - val_acc: 0.9041\n",
      "Epoch 803/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3676 - acc: 0.8957 - val_loss: 0.4213 - val_acc: 0.8998\n",
      "Epoch 804/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3676 - acc: 0.8953 - val_loss: 0.4370 - val_acc: 0.8969\n",
      "Epoch 805/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3674 - acc: 0.8951 - val_loss: 0.4488 - val_acc: 0.8922\n",
      "Epoch 806/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3673 - acc: 0.8955 - val_loss: 0.4694 - val_acc: 0.8852\n",
      "Epoch 807/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3671 - acc: 0.8959 - val_loss: 0.4762 - val_acc: 0.8847\n",
      "Epoch 808/1500\n",
      "30246/30246 [==============================] - 3s 91us/step - loss: 0.3668 - acc: 0.8962 - val_loss: 0.4389 - val_acc: 0.8962\n",
      "Epoch 809/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3665 - acc: 0.8953 - val_loss: 0.4510 - val_acc: 0.8909\n",
      "Epoch 810/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3665 - acc: 0.8956 - val_loss: 0.4340 - val_acc: 0.8947\n",
      "Epoch 811/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3663 - acc: 0.8961 - val_loss: 0.4747 - val_acc: 0.8836\n",
      "Epoch 812/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3663 - acc: 0.8964 - val_loss: 0.4492 - val_acc: 0.8933\n",
      "Epoch 813/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3661 - acc: 0.8965 - val_loss: 0.4396 - val_acc: 0.8951\n",
      "Epoch 814/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.3662 - acc: 0.8955 - val_loss: 0.3941 - val_acc: 0.9084\n",
      "Epoch 815/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3655 - acc: 0.8968 - val_loss: 0.4372 - val_acc: 0.8972\n",
      "Epoch 816/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3654 - acc: 0.8965 - val_loss: 0.4289 - val_acc: 0.8958\n",
      "Epoch 817/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3655 - acc: 0.8960 - val_loss: 0.4730 - val_acc: 0.8823\n",
      "Epoch 818/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3656 - acc: 0.8958 - val_loss: 0.4175 - val_acc: 0.9016\n",
      "Epoch 819/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3648 - acc: 0.8963 - val_loss: 0.4545 - val_acc: 0.8901\n",
      "Epoch 820/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3642 - acc: 0.8961 - val_loss: 0.4574 - val_acc: 0.8892\n",
      "Epoch 821/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3640 - acc: 0.8961 - val_loss: 0.4245 - val_acc: 0.8982\n",
      "Epoch 822/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3645 - acc: 0.8965 - val_loss: 0.3907 - val_acc: 0.9099\n",
      "Epoch 823/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3640 - acc: 0.8959 - val_loss: 0.3747 - val_acc: 0.9136\n",
      "Epoch 824/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3637 - acc: 0.8965 - val_loss: 0.4421 - val_acc: 0.8943\n",
      "Epoch 825/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3634 - acc: 0.8973 - val_loss: 0.4066 - val_acc: 0.9029\n",
      "Epoch 826/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3633 - acc: 0.8969 - val_loss: 0.4445 - val_acc: 0.8905\n",
      "Epoch 827/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3638 - acc: 0.8961 - val_loss: 0.4405 - val_acc: 0.8961\n",
      "Epoch 828/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3632 - acc: 0.8971 - val_loss: 0.4165 - val_acc: 0.9021\n",
      "Epoch 829/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3629 - acc: 0.8962 - val_loss: 0.4583 - val_acc: 0.8913\n",
      "Epoch 830/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3629 - acc: 0.8966 - val_loss: 0.4376 - val_acc: 0.8955\n",
      "Epoch 831/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3626 - acc: 0.8975 - val_loss: 0.4514 - val_acc: 0.8914\n",
      "Epoch 832/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3624 - acc: 0.8960 - val_loss: 0.4252 - val_acc: 0.8987\n",
      "Epoch 833/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3619 - acc: 0.8963 - val_loss: 0.4689 - val_acc: 0.8867\n",
      "Epoch 834/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3619 - acc: 0.8963 - val_loss: 0.4783 - val_acc: 0.8840\n",
      "Epoch 835/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3618 - acc: 0.8971 - val_loss: 0.4540 - val_acc: 0.8902\n",
      "Epoch 836/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3620 - acc: 0.8969 - val_loss: 0.4705 - val_acc: 0.8867\n",
      "Epoch 837/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3615 - acc: 0.8963 - val_loss: 0.4437 - val_acc: 0.8931\n",
      "Epoch 838/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3615 - acc: 0.8969 - val_loss: 0.4257 - val_acc: 0.8990\n",
      "Epoch 839/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3614 - acc: 0.8966 - val_loss: 0.4697 - val_acc: 0.8844\n",
      "Epoch 840/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3610 - acc: 0.8967 - val_loss: 0.4387 - val_acc: 0.8945\n",
      "Epoch 841/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3610 - acc: 0.8975 - val_loss: 0.4204 - val_acc: 0.9011\n",
      "Epoch 842/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.3606 - acc: 0.8976 - val_loss: 0.4979 - val_acc: 0.8730\n",
      "Epoch 843/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3606 - acc: 0.8967 - val_loss: 0.4560 - val_acc: 0.8901\n",
      "Epoch 844/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3602 - acc: 0.8969 - val_loss: 0.4624 - val_acc: 0.8876\n",
      "Epoch 845/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3605 - acc: 0.8968 - val_loss: 0.4348 - val_acc: 0.8958\n",
      "Epoch 846/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3601 - acc: 0.8969 - val_loss: 0.4806 - val_acc: 0.8823\n",
      "Epoch 847/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3597 - acc: 0.8966 - val_loss: 0.4436 - val_acc: 0.8913\n",
      "Epoch 848/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3595 - acc: 0.8972 - val_loss: 0.4240 - val_acc: 0.8991\n",
      "Epoch 849/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3596 - acc: 0.8973 - val_loss: 0.4457 - val_acc: 0.8935\n",
      "Epoch 850/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3592 - acc: 0.8967 - val_loss: 0.4222 - val_acc: 0.9011\n",
      "Epoch 851/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3591 - acc: 0.8964 - val_loss: 0.3900 - val_acc: 0.9088\n",
      "Epoch 852/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3590 - acc: 0.8975 - val_loss: 0.4013 - val_acc: 0.9060\n",
      "Epoch 853/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3587 - acc: 0.8976 - val_loss: 0.4148 - val_acc: 0.9023\n",
      "Epoch 854/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3587 - acc: 0.8971 - val_loss: 0.4045 - val_acc: 0.9053\n",
      "Epoch 855/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3584 - acc: 0.8978 - val_loss: 0.4233 - val_acc: 0.8991\n",
      "Epoch 856/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3581 - acc: 0.8988 - val_loss: 0.4347 - val_acc: 0.8955\n",
      "Epoch 857/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3577 - acc: 0.8985 - val_loss: 0.4603 - val_acc: 0.8894\n",
      "Epoch 858/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3578 - acc: 0.8979 - val_loss: 0.4720 - val_acc: 0.8842\n",
      "Epoch 859/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3577 - acc: 0.8980 - val_loss: 0.4551 - val_acc: 0.8902\n",
      "Epoch 860/1500\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 0.3578 - acc: 0.8973 - val_loss: 0.4073 - val_acc: 0.9043\n",
      "Epoch 861/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3578 - acc: 0.8976 - val_loss: 0.4352 - val_acc: 0.8971\n",
      "Epoch 862/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3568 - acc: 0.8979 - val_loss: 0.4565 - val_acc: 0.8917\n",
      "Epoch 863/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.3571 - acc: 0.8987 - val_loss: 0.4621 - val_acc: 0.8896\n",
      "Epoch 864/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3572 - acc: 0.8978 - val_loss: 0.4354 - val_acc: 0.8976\n",
      "Epoch 865/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3568 - acc: 0.8971 - val_loss: 0.3989 - val_acc: 0.9062\n",
      "Epoch 866/1500\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.3563 - acc: 0.8981 - val_loss: 0.4684 - val_acc: 0.8850\n",
      "Epoch 867/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3564 - acc: 0.8978 - val_loss: 0.4788 - val_acc: 0.8819\n",
      "Epoch 868/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3563 - acc: 0.8983 - val_loss: 0.5068 - val_acc: 0.8736\n",
      "Epoch 869/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3561 - acc: 0.8987 - val_loss: 0.4918 - val_acc: 0.8771\n",
      "Epoch 870/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3563 - acc: 0.8978 - val_loss: 0.4415 - val_acc: 0.8959\n",
      "Epoch 871/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 44us/step - loss: 0.3559 - acc: 0.8980 - val_loss: 0.4610 - val_acc: 0.8893\n",
      "Epoch 872/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3556 - acc: 0.8982 - val_loss: 0.4441 - val_acc: 0.8933\n",
      "Epoch 873/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3556 - acc: 0.8977 - val_loss: 0.4535 - val_acc: 0.8910\n",
      "Epoch 874/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3551 - acc: 0.8985 - val_loss: 0.4422 - val_acc: 0.8945\n",
      "Epoch 875/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3550 - acc: 0.8983 - val_loss: 0.4367 - val_acc: 0.8961\n",
      "Epoch 876/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3550 - acc: 0.8988 - val_loss: 0.4628 - val_acc: 0.8893\n",
      "Epoch 877/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3547 - acc: 0.8983 - val_loss: 0.4732 - val_acc: 0.8875\n",
      "Epoch 878/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3544 - acc: 0.8992 - val_loss: 0.4745 - val_acc: 0.8853\n",
      "Epoch 879/1500\n",
      "30246/30246 [==============================] - 1s 42us/step - loss: 0.3548 - acc: 0.8988 - val_loss: 0.4482 - val_acc: 0.8937\n",
      "Epoch 880/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3546 - acc: 0.8982 - val_loss: 0.4000 - val_acc: 0.9049\n",
      "Epoch 881/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3539 - acc: 0.8992 - val_loss: 0.3922 - val_acc: 0.9085\n",
      "Epoch 882/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3541 - acc: 0.8987 - val_loss: 0.4474 - val_acc: 0.8922\n",
      "Epoch 883/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3541 - acc: 0.8987 - val_loss: 0.4515 - val_acc: 0.8909\n",
      "Epoch 884/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3538 - acc: 0.8987 - val_loss: 0.4158 - val_acc: 0.9029\n",
      "Epoch 885/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3533 - acc: 0.8990 - val_loss: 0.4917 - val_acc: 0.8795\n",
      "Epoch 886/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3537 - acc: 0.8995 - val_loss: 0.4752 - val_acc: 0.8822\n",
      "Epoch 887/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3532 - acc: 0.8987 - val_loss: 0.3871 - val_acc: 0.9106\n",
      "Epoch 888/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3529 - acc: 0.8994 - val_loss: 0.4798 - val_acc: 0.8793\n",
      "Epoch 889/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3527 - acc: 0.8985 - val_loss: 0.4820 - val_acc: 0.8791\n",
      "Epoch 890/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3529 - acc: 0.8991 - val_loss: 0.4548 - val_acc: 0.8920\n",
      "Epoch 891/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3524 - acc: 0.8991 - val_loss: 0.4183 - val_acc: 0.9006\n",
      "Epoch 892/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3524 - acc: 0.8995 - val_loss: 0.4377 - val_acc: 0.8959\n",
      "Epoch 893/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3521 - acc: 0.9000 - val_loss: 0.4158 - val_acc: 0.9024\n",
      "Epoch 894/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3519 - acc: 0.8997 - val_loss: 0.4226 - val_acc: 0.8991\n",
      "Epoch 895/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3520 - acc: 0.8993 - val_loss: 0.4423 - val_acc: 0.8953\n",
      "Epoch 896/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3517 - acc: 0.8995 - val_loss: 0.3981 - val_acc: 0.9077\n",
      "Epoch 897/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3515 - acc: 0.8995 - val_loss: 0.4676 - val_acc: 0.8877\n",
      "Epoch 898/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3517 - acc: 0.8996 - val_loss: 0.4621 - val_acc: 0.8893\n",
      "Epoch 899/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3514 - acc: 0.8998 - val_loss: 0.4320 - val_acc: 0.8984\n",
      "Epoch 900/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3511 - acc: 0.8996 - val_loss: 0.4562 - val_acc: 0.8920\n",
      "Epoch 901/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3507 - acc: 0.9004 - val_loss: 0.4525 - val_acc: 0.8921\n",
      "Epoch 902/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3506 - acc: 0.8997 - val_loss: 0.4927 - val_acc: 0.8798\n",
      "Epoch 903/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3510 - acc: 0.8996 - val_loss: 0.4167 - val_acc: 0.9019\n",
      "Epoch 904/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3505 - acc: 0.9000 - val_loss: 0.4226 - val_acc: 0.9003\n",
      "Epoch 905/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3505 - acc: 0.8996 - val_loss: 0.4645 - val_acc: 0.8872\n",
      "Epoch 906/1500\n",
      "30246/30246 [==============================] - 2s 65us/step - loss: 0.3501 - acc: 0.8993 - val_loss: 0.3943 - val_acc: 0.9072\n",
      "Epoch 907/1500\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.3501 - acc: 0.9010 - val_loss: 0.4274 - val_acc: 0.8980\n",
      "Epoch 908/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3498 - acc: 0.9001 - val_loss: 0.4566 - val_acc: 0.8904\n",
      "Epoch 909/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3497 - acc: 0.8992 - val_loss: 0.4727 - val_acc: 0.8868\n",
      "Epoch 910/1500\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.3497 - acc: 0.9006 - val_loss: 0.4314 - val_acc: 0.8980\n",
      "Epoch 911/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3495 - acc: 0.9002 - val_loss: 0.4532 - val_acc: 0.8916\n",
      "Epoch 912/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3491 - acc: 0.9001 - val_loss: 0.4389 - val_acc: 0.8945\n",
      "Epoch 913/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3491 - acc: 0.9008 - val_loss: 0.4619 - val_acc: 0.8887\n",
      "Epoch 914/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3491 - acc: 0.9001 - val_loss: 0.4507 - val_acc: 0.8917\n",
      "Epoch 915/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3484 - acc: 0.9009 - val_loss: 0.4277 - val_acc: 0.8986\n",
      "Epoch 916/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3485 - acc: 0.8998 - val_loss: 0.4353 - val_acc: 0.8966\n",
      "Epoch 917/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3488 - acc: 0.9003 - val_loss: 0.4302 - val_acc: 0.8972\n",
      "Epoch 918/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3484 - acc: 0.8998 - val_loss: 0.4480 - val_acc: 0.8917\n",
      "Epoch 919/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3479 - acc: 0.9008 - val_loss: 0.4933 - val_acc: 0.8781\n",
      "Epoch 920/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3482 - acc: 0.9006 - val_loss: 0.4298 - val_acc: 0.8990\n",
      "Epoch 921/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3482 - acc: 0.9012 - val_loss: 0.4204 - val_acc: 0.9012\n",
      "Epoch 922/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3478 - acc: 0.9015 - val_loss: 0.4788 - val_acc: 0.8830\n",
      "Epoch 923/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3473 - acc: 0.9002 - val_loss: 0.4151 - val_acc: 0.8996\n",
      "Epoch 924/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3479 - acc: 0.9007 - val_loss: 0.4594 - val_acc: 0.8912\n",
      "Epoch 925/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3475 - acc: 0.9005 - val_loss: 0.4712 - val_acc: 0.8879\n",
      "Epoch 926/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3474 - acc: 0.9000 - val_loss: 0.4509 - val_acc: 0.8916\n",
      "Epoch 927/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3475 - acc: 0.9010 - val_loss: 0.4805 - val_acc: 0.8831\n",
      "Epoch 928/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3466 - acc: 0.9004 - val_loss: 0.4135 - val_acc: 0.9028\n",
      "Epoch 929/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3464 - acc: 0.9013 - val_loss: 0.4514 - val_acc: 0.8934\n",
      "Epoch 930/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3470 - acc: 0.9008 - val_loss: 0.4209 - val_acc: 0.9004\n",
      "Epoch 931/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3466 - acc: 0.9014 - val_loss: 0.4388 - val_acc: 0.8965\n",
      "Epoch 932/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3462 - acc: 0.9015 - val_loss: 0.4345 - val_acc: 0.8958\n",
      "Epoch 933/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3461 - acc: 0.9010 - val_loss: 0.4595 - val_acc: 0.8904\n",
      "Epoch 934/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3461 - acc: 0.9011 - val_loss: 0.3987 - val_acc: 0.9062\n",
      "Epoch 935/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3460 - acc: 0.9003 - val_loss: 0.4820 - val_acc: 0.8819\n",
      "Epoch 936/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3458 - acc: 0.9016 - val_loss: 0.4448 - val_acc: 0.8946\n",
      "Epoch 937/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3450 - acc: 0.9017 - val_loss: 0.4250 - val_acc: 0.9000\n",
      "Epoch 938/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3455 - acc: 0.9019 - val_loss: 0.4220 - val_acc: 0.9008\n",
      "Epoch 939/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3454 - acc: 0.9004 - val_loss: 0.3760 - val_acc: 0.9129\n",
      "Epoch 940/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3451 - acc: 0.9015 - val_loss: 0.4293 - val_acc: 0.8986\n",
      "Epoch 941/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3444 - acc: 0.9014 - val_loss: 0.4405 - val_acc: 0.8938\n",
      "Epoch 942/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3443 - acc: 0.9025 - val_loss: 0.4104 - val_acc: 0.9031\n",
      "Epoch 943/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3444 - acc: 0.9016 - val_loss: 0.4625 - val_acc: 0.8894\n",
      "Epoch 944/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3443 - acc: 0.9009 - val_loss: 0.3614 - val_acc: 0.9177\n",
      "Epoch 945/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3443 - acc: 0.9018 - val_loss: 0.4355 - val_acc: 0.8974\n",
      "Epoch 946/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3442 - acc: 0.9010 - val_loss: 0.4289 - val_acc: 0.8978\n",
      "Epoch 947/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3437 - acc: 0.9031 - val_loss: 0.4481 - val_acc: 0.8933\n",
      "Epoch 948/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3439 - acc: 0.9017 - val_loss: 0.4335 - val_acc: 0.8976\n",
      "Epoch 949/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3437 - acc: 0.9023 - val_loss: 0.4056 - val_acc: 0.9041\n",
      "Epoch 950/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3439 - acc: 0.9022 - val_loss: 0.4365 - val_acc: 0.8982\n",
      "Epoch 951/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3431 - acc: 0.9024 - val_loss: 0.4149 - val_acc: 0.9040\n",
      "Epoch 952/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3432 - acc: 0.9022 - val_loss: 0.4422 - val_acc: 0.8929\n",
      "Epoch 953/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3430 - acc: 0.9014 - val_loss: 0.5331 - val_acc: 0.8666\n",
      "Epoch 954/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3433 - acc: 0.9025 - val_loss: 0.4852 - val_acc: 0.8828\n",
      "Epoch 955/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3429 - acc: 0.9022 - val_loss: 0.4788 - val_acc: 0.8830\n",
      "Epoch 956/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.3429 - acc: 0.9015 - val_loss: 0.4465 - val_acc: 0.8909\n",
      "Epoch 957/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.3427 - acc: 0.9013 - val_loss: 0.5072 - val_acc: 0.8758\n",
      "Epoch 958/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3426 - acc: 0.9027 - val_loss: 0.4640 - val_acc: 0.8877\n",
      "Epoch 959/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3420 - acc: 0.9021 - val_loss: 0.4804 - val_acc: 0.8818\n",
      "Epoch 960/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3421 - acc: 0.9020 - val_loss: 0.4094 - val_acc: 0.9041\n",
      "Epoch 961/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3418 - acc: 0.9019 - val_loss: 0.3948 - val_acc: 0.9072\n",
      "Epoch 962/1500\n",
      "30246/30246 [==============================] - 2s 60us/step - loss: 0.3419 - acc: 0.9025 - val_loss: 0.4351 - val_acc: 0.8976\n",
      "Epoch 963/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3418 - acc: 0.9027 - val_loss: 0.3967 - val_acc: 0.9077\n",
      "Epoch 964/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3414 - acc: 0.9024 - val_loss: 0.4212 - val_acc: 0.9017\n",
      "Epoch 965/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3416 - acc: 0.9039 - val_loss: 0.4460 - val_acc: 0.8947\n",
      "Epoch 966/1500\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.3410 - acc: 0.9027 - val_loss: 0.4611 - val_acc: 0.8877\n",
      "Epoch 967/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3414 - acc: 0.9025 - val_loss: 0.4696 - val_acc: 0.8850\n",
      "Epoch 968/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3407 - acc: 0.9026 - val_loss: 0.4563 - val_acc: 0.8918\n",
      "Epoch 969/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3406 - acc: 0.9019 - val_loss: 0.4865 - val_acc: 0.8826\n",
      "Epoch 970/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3408 - acc: 0.9036 - val_loss: 0.4288 - val_acc: 0.8975\n",
      "Epoch 971/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3405 - acc: 0.9034 - val_loss: 0.4223 - val_acc: 0.9006\n",
      "Epoch 972/1500\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.3406 - acc: 0.9027 - val_loss: 0.4058 - val_acc: 0.9027\n",
      "Epoch 973/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3405 - acc: 0.9032 - val_loss: 0.4500 - val_acc: 0.8918\n",
      "Epoch 974/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3401 - acc: 0.9031 - val_loss: 0.4450 - val_acc: 0.8937\n",
      "Epoch 975/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3399 - acc: 0.9035 - val_loss: 0.4160 - val_acc: 0.9007\n",
      "Epoch 976/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3399 - acc: 0.9037 - val_loss: 0.4469 - val_acc: 0.8931\n",
      "Epoch 977/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3398 - acc: 0.9030 - val_loss: 0.4415 - val_acc: 0.8946\n",
      "Epoch 978/1500\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 0.3393 - acc: 0.9034 - val_loss: 0.4566 - val_acc: 0.8925\n",
      "Epoch 979/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3393 - acc: 0.9034 - val_loss: 0.4105 - val_acc: 0.9029\n",
      "Epoch 980/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3391 - acc: 0.9032 - val_loss: 0.4870 - val_acc: 0.8797\n",
      "Epoch 981/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3392 - acc: 0.9039 - val_loss: 0.4502 - val_acc: 0.8925\n",
      "Epoch 982/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3392 - acc: 0.9028 - val_loss: 0.5006 - val_acc: 0.8738\n",
      "Epoch 983/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3391 - acc: 0.9036 - val_loss: 0.4500 - val_acc: 0.8922\n",
      "Epoch 984/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3386 - acc: 0.9040 - val_loss: 0.4809 - val_acc: 0.8822\n",
      "Epoch 985/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3386 - acc: 0.9029 - val_loss: 0.4458 - val_acc: 0.8933\n",
      "Epoch 986/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3381 - acc: 0.9020 - val_loss: 0.4104 - val_acc: 0.9025\n",
      "Epoch 987/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3386 - acc: 0.9035 - val_loss: 0.4316 - val_acc: 0.8966\n",
      "Epoch 988/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3382 - acc: 0.9038 - val_loss: 0.4235 - val_acc: 0.8992\n",
      "Epoch 989/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3382 - acc: 0.9045 - val_loss: 0.4799 - val_acc: 0.8826\n",
      "Epoch 990/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3377 - acc: 0.9038 - val_loss: 0.4442 - val_acc: 0.8945\n",
      "Epoch 991/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3380 - acc: 0.9041 - val_loss: 0.4528 - val_acc: 0.8920\n",
      "Epoch 992/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3376 - acc: 0.9038 - val_loss: 0.4359 - val_acc: 0.8975\n",
      "Epoch 993/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3375 - acc: 0.9033 - val_loss: 0.3803 - val_acc: 0.9105\n",
      "Epoch 994/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.9038 - val_loss: 0.4378 - val_acc: 0.8947\n",
      "Epoch 995/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3372 - acc: 0.9027 - val_loss: 0.4474 - val_acc: 0.8934\n",
      "Epoch 996/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3371 - acc: 0.9044 - val_loss: 0.4276 - val_acc: 0.8995\n",
      "Epoch 997/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3375 - acc: 0.9039 - val_loss: 0.4362 - val_acc: 0.8958\n",
      "Epoch 998/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3367 - acc: 0.9039 - val_loss: 0.4281 - val_acc: 0.8980\n",
      "Epoch 999/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3369 - acc: 0.9041 - val_loss: 0.4793 - val_acc: 0.8836\n",
      "Epoch 1000/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3366 - acc: 0.9034 - val_loss: 0.4647 - val_acc: 0.8885\n",
      "Epoch 1001/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3366 - acc: 0.9049 - val_loss: 0.4268 - val_acc: 0.8983\n",
      "Epoch 1002/1500\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.3364 - acc: 0.9040 - val_loss: 0.4541 - val_acc: 0.8917\n",
      "Epoch 1003/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.3362 - acc: 0.9045 - val_loss: 0.4500 - val_acc: 0.8928\n",
      "Epoch 1004/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3358 - acc: 0.9037 - val_loss: 0.4002 - val_acc: 0.9056\n",
      "Epoch 1005/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3358 - acc: 0.9050 - val_loss: 0.4222 - val_acc: 0.9002\n",
      "Epoch 1006/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3357 - acc: 0.9041 - val_loss: 0.4822 - val_acc: 0.8838\n",
      "Epoch 1007/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3358 - acc: 0.9037 - val_loss: 0.4180 - val_acc: 0.9004\n",
      "Epoch 1008/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3352 - acc: 0.9046 - val_loss: 0.4593 - val_acc: 0.8909\n",
      "Epoch 1009/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3359 - acc: 0.9039 - val_loss: 0.3806 - val_acc: 0.9113\n",
      "Epoch 1010/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3353 - acc: 0.9034 - val_loss: 0.4468 - val_acc: 0.8928\n",
      "Epoch 1011/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3351 - acc: 0.9043 - val_loss: 0.4661 - val_acc: 0.8864\n",
      "Epoch 1012/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3351 - acc: 0.9038 - val_loss: 0.4291 - val_acc: 0.8988\n",
      "Epoch 1013/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3347 - acc: 0.9046 - val_loss: 0.4377 - val_acc: 0.8949\n",
      "Epoch 1014/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3343 - acc: 0.9046 - val_loss: 0.3914 - val_acc: 0.9070\n",
      "Epoch 1015/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3345 - acc: 0.9051 - val_loss: 0.4566 - val_acc: 0.8902\n",
      "Epoch 1016/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3340 - acc: 0.9046 - val_loss: 0.4092 - val_acc: 0.9040\n",
      "Epoch 1017/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3341 - acc: 0.9048 - val_loss: 0.4390 - val_acc: 0.8933\n",
      "Epoch 1018/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3344 - acc: 0.9040 - val_loss: 0.4902 - val_acc: 0.8811\n",
      "Epoch 1019/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3339 - acc: 0.9058 - val_loss: 0.4168 - val_acc: 0.9015\n",
      "Epoch 1020/1500\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.3342 - acc: 0.9050 - val_loss: 0.4091 - val_acc: 0.9036\n",
      "Epoch 1021/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3333 - acc: 0.9044 - val_loss: 0.4365 - val_acc: 0.8965\n",
      "Epoch 1022/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3337 - acc: 0.9053 - val_loss: 0.4152 - val_acc: 0.9020\n",
      "Epoch 1023/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3334 - acc: 0.9054 - val_loss: 0.4317 - val_acc: 0.8972\n",
      "Epoch 1024/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3336 - acc: 0.9047 - val_loss: 0.4160 - val_acc: 0.9016\n",
      "Epoch 1025/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3332 - acc: 0.9047 - val_loss: 0.4691 - val_acc: 0.8879\n",
      "Epoch 1026/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.3330 - acc: 0.9053 - val_loss: 0.3899 - val_acc: 0.9078\n",
      "Epoch 1027/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3329 - acc: 0.9055 - val_loss: 0.4487 - val_acc: 0.8922\n",
      "Epoch 1028/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3333 - acc: 0.9046 - val_loss: 0.4734 - val_acc: 0.8852\n",
      "Epoch 1029/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3330 - acc: 0.9059 - val_loss: 0.4643 - val_acc: 0.8877\n",
      "Epoch 1030/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3323 - acc: 0.9047 - val_loss: 0.4890 - val_acc: 0.8782\n",
      "Epoch 1031/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3322 - acc: 0.9051 - val_loss: 0.3825 - val_acc: 0.9105\n",
      "Epoch 1032/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3327 - acc: 0.9046 - val_loss: 0.4029 - val_acc: 0.9045\n",
      "Epoch 1033/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3324 - acc: 0.9049 - val_loss: 0.4044 - val_acc: 0.9058\n",
      "Epoch 1034/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3320 - acc: 0.9047 - val_loss: 0.4983 - val_acc: 0.8769\n",
      "Epoch 1035/1500\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.3321 - acc: 0.9056 - val_loss: 0.4627 - val_acc: 0.8867\n",
      "Epoch 1036/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3318 - acc: 0.9055 - val_loss: 0.3916 - val_acc: 0.9082\n",
      "Epoch 1037/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3320 - acc: 0.9044 - val_loss: 0.4603 - val_acc: 0.8875\n",
      "Epoch 1038/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3319 - acc: 0.9060 - val_loss: 0.4392 - val_acc: 0.8938\n",
      "Epoch 1039/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3314 - acc: 0.9053 - val_loss: 0.4792 - val_acc: 0.8811\n",
      "Epoch 1040/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3317 - acc: 0.9052 - val_loss: 0.4246 - val_acc: 0.8990\n",
      "Epoch 1041/1500\n",
      "30246/30246 [==============================] - 2s 50us/step - loss: 0.3309 - acc: 0.9060 - val_loss: 0.4234 - val_acc: 0.8992\n",
      "Epoch 1042/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3311 - acc: 0.9055 - val_loss: 0.4550 - val_acc: 0.8904\n",
      "Epoch 1043/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3310 - acc: 0.9061 - val_loss: 0.4973 - val_acc: 0.8806\n",
      "Epoch 1044/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3311 - acc: 0.9058 - val_loss: 0.4831 - val_acc: 0.8814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1045/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3312 - acc: 0.9056 - val_loss: 0.4458 - val_acc: 0.8934\n",
      "Epoch 1046/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3309 - acc: 0.9055 - val_loss: 0.3968 - val_acc: 0.9057\n",
      "Epoch 1047/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3305 - acc: 0.9057 - val_loss: 0.4435 - val_acc: 0.8926\n",
      "Epoch 1048/1500\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.3305 - acc: 0.9051 - val_loss: 0.4718 - val_acc: 0.8867\n",
      "Epoch 1049/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3303 - acc: 0.9065 - val_loss: 0.4709 - val_acc: 0.8846\n",
      "Epoch 1050/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3302 - acc: 0.9059 - val_loss: 0.4444 - val_acc: 0.8946\n",
      "Epoch 1051/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3304 - acc: 0.9058 - val_loss: 0.3987 - val_acc: 0.9066\n",
      "Epoch 1052/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3299 - acc: 0.9070 - val_loss: 0.4686 - val_acc: 0.8871\n",
      "Epoch 1053/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3298 - acc: 0.9065 - val_loss: 0.4375 - val_acc: 0.8957\n",
      "Epoch 1054/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3295 - acc: 0.9057 - val_loss: 0.4820 - val_acc: 0.8779\n",
      "Epoch 1055/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3292 - acc: 0.9061 - val_loss: 0.4163 - val_acc: 0.9006\n",
      "Epoch 1056/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3298 - acc: 0.9059 - val_loss: 0.4306 - val_acc: 0.8957\n",
      "Epoch 1057/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3295 - acc: 0.9046 - val_loss: 0.4117 - val_acc: 0.9015\n",
      "Epoch 1058/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3295 - acc: 0.9051 - val_loss: 0.4332 - val_acc: 0.8963\n",
      "Epoch 1059/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3294 - acc: 0.9059 - val_loss: 0.4218 - val_acc: 0.9002\n",
      "Epoch 1060/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3290 - acc: 0.9056 - val_loss: 0.3871 - val_acc: 0.9080\n",
      "Epoch 1061/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3288 - acc: 0.9066 - val_loss: 0.4095 - val_acc: 0.9016\n",
      "Epoch 1062/1500\n",
      "30246/30246 [==============================] - 1s 46us/step - loss: 0.3288 - acc: 0.9054 - val_loss: 0.4275 - val_acc: 0.8975\n",
      "Epoch 1063/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3289 - acc: 0.9072 - val_loss: 0.4569 - val_acc: 0.8884\n",
      "Epoch 1064/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3286 - acc: 0.9057 - val_loss: 0.4285 - val_acc: 0.8976\n",
      "Epoch 1065/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3287 - acc: 0.9056 - val_loss: 0.4204 - val_acc: 0.9010\n",
      "Epoch 1066/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3281 - acc: 0.9073 - val_loss: 0.4315 - val_acc: 0.8966\n",
      "Epoch 1067/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3279 - acc: 0.9065 - val_loss: 0.5007 - val_acc: 0.8777\n",
      "Epoch 1068/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3280 - acc: 0.9063 - val_loss: 0.4385 - val_acc: 0.8949\n",
      "Epoch 1069/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3277 - acc: 0.9070 - val_loss: 0.4823 - val_acc: 0.8790\n",
      "Epoch 1070/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3274 - acc: 0.9071 - val_loss: 0.4676 - val_acc: 0.8867\n",
      "Epoch 1071/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3277 - acc: 0.9061 - val_loss: 0.4373 - val_acc: 0.8951\n",
      "Epoch 1072/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3278 - acc: 0.9069 - val_loss: 0.4573 - val_acc: 0.8900\n",
      "Epoch 1073/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3273 - acc: 0.9074 - val_loss: 0.3986 - val_acc: 0.9057\n",
      "Epoch 1074/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3274 - acc: 0.9059 - val_loss: 0.4136 - val_acc: 0.9017\n",
      "Epoch 1075/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3268 - acc: 0.9066 - val_loss: 0.4575 - val_acc: 0.8880\n",
      "Epoch 1076/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3270 - acc: 0.9070 - val_loss: 0.4306 - val_acc: 0.8980\n",
      "Epoch 1077/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3273 - acc: 0.9069 - val_loss: 0.4113 - val_acc: 0.9019\n",
      "Epoch 1078/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3268 - acc: 0.9070 - val_loss: 0.4355 - val_acc: 0.8970\n",
      "Epoch 1079/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3267 - acc: 0.9075 - val_loss: 0.4356 - val_acc: 0.8953\n",
      "Epoch 1080/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3265 - acc: 0.9070 - val_loss: 0.4177 - val_acc: 0.9028\n",
      "Epoch 1081/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3267 - acc: 0.9071 - val_loss: 0.4569 - val_acc: 0.8875\n",
      "Epoch 1082/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3265 - acc: 0.9066 - val_loss: 0.4202 - val_acc: 0.8994\n",
      "Epoch 1083/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3261 - acc: 0.9072 - val_loss: 0.3991 - val_acc: 0.9044\n",
      "Epoch 1084/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3258 - acc: 0.9070 - val_loss: 0.4164 - val_acc: 0.8991\n",
      "Epoch 1085/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3259 - acc: 0.9073 - val_loss: 0.4334 - val_acc: 0.8947\n",
      "Epoch 1086/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3258 - acc: 0.9072 - val_loss: 0.4102 - val_acc: 0.9019\n",
      "Epoch 1087/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3253 - acc: 0.9066 - val_loss: 0.4566 - val_acc: 0.8883\n",
      "Epoch 1088/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3259 - acc: 0.9065 - val_loss: 0.4654 - val_acc: 0.8875\n",
      "Epoch 1089/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3255 - acc: 0.9080 - val_loss: 0.4453 - val_acc: 0.8931\n",
      "Epoch 1090/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3255 - acc: 0.9071 - val_loss: 0.4443 - val_acc: 0.8928\n",
      "Epoch 1091/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3255 - acc: 0.9072 - val_loss: 0.4550 - val_acc: 0.8904\n",
      "Epoch 1092/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3253 - acc: 0.9073 - val_loss: 0.4646 - val_acc: 0.8892\n",
      "Epoch 1093/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3250 - acc: 0.9078 - val_loss: 0.5234 - val_acc: 0.8680\n",
      "Epoch 1094/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3251 - acc: 0.9070 - val_loss: 0.4284 - val_acc: 0.8961\n",
      "Epoch 1095/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3250 - acc: 0.9065 - val_loss: 0.4375 - val_acc: 0.8947\n",
      "Epoch 1096/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3251 - acc: 0.9080 - val_loss: 0.4078 - val_acc: 0.9027\n",
      "Epoch 1097/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3243 - acc: 0.9075 - val_loss: 0.4521 - val_acc: 0.8901\n",
      "Epoch 1098/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3248 - acc: 0.9076 - val_loss: 0.4624 - val_acc: 0.8885\n",
      "Epoch 1099/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3248 - acc: 0.9071 - val_loss: 0.4066 - val_acc: 0.9045\n",
      "Epoch 1100/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3246 - acc: 0.9069 - val_loss: 0.4725 - val_acc: 0.8824\n",
      "Epoch 1101/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3242 - acc: 0.9072 - val_loss: 0.4121 - val_acc: 0.9017\n",
      "Epoch 1102/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3240 - acc: 0.9084 - val_loss: 0.4880 - val_acc: 0.8773\n",
      "Epoch 1103/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3241 - acc: 0.9079 - val_loss: 0.4250 - val_acc: 0.8990\n",
      "Epoch 1104/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3238 - acc: 0.9067 - val_loss: 0.4264 - val_acc: 0.8987\n",
      "Epoch 1105/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3237 - acc: 0.9077 - val_loss: 0.4881 - val_acc: 0.8818\n",
      "Epoch 1106/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3234 - acc: 0.9085 - val_loss: 0.3983 - val_acc: 0.9061\n",
      "Epoch 1107/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3240 - acc: 0.9074 - val_loss: 0.4586 - val_acc: 0.8883\n",
      "Epoch 1108/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3235 - acc: 0.9070 - val_loss: 0.5011 - val_acc: 0.8774\n",
      "Epoch 1109/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3228 - acc: 0.9079 - val_loss: 0.4234 - val_acc: 0.9007\n",
      "Epoch 1110/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3232 - acc: 0.9076 - val_loss: 0.4428 - val_acc: 0.8935\n",
      "Epoch 1111/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3227 - acc: 0.9078 - val_loss: 0.4228 - val_acc: 0.8979\n",
      "Epoch 1112/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.3229 - acc: 0.9084 - val_loss: 0.4693 - val_acc: 0.8857\n",
      "Epoch 1113/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3230 - acc: 0.9075 - val_loss: 0.4127 - val_acc: 0.8999\n",
      "Epoch 1114/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3228 - acc: 0.9082 - val_loss: 0.4400 - val_acc: 0.8938\n",
      "Epoch 1115/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3228 - acc: 0.9081 - val_loss: 0.4750 - val_acc: 0.8828\n",
      "Epoch 1116/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3225 - acc: 0.9089 - val_loss: 0.4156 - val_acc: 0.9012\n",
      "Epoch 1117/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3226 - acc: 0.9086 - val_loss: 0.5005 - val_acc: 0.8753\n",
      "Epoch 1118/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3223 - acc: 0.9074 - val_loss: 0.3843 - val_acc: 0.9099\n",
      "Epoch 1119/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3221 - acc: 0.9078 - val_loss: 0.4387 - val_acc: 0.8942\n",
      "Epoch 1120/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3224 - acc: 0.9077 - val_loss: 0.4360 - val_acc: 0.8949\n",
      "Epoch 1121/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3215 - acc: 0.9082 - val_loss: 0.4130 - val_acc: 0.9013\n",
      "Epoch 1122/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3219 - acc: 0.9084 - val_loss: 0.4336 - val_acc: 0.8955\n",
      "Epoch 1123/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3217 - acc: 0.9083 - val_loss: 0.3735 - val_acc: 0.9121\n",
      "Epoch 1124/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.9074 - val_loss: 0.3985 - val_acc: 0.9052\n",
      "Epoch 1125/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.9084 - val_loss: 0.4172 - val_acc: 0.8975\n",
      "Epoch 1126/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3216 - acc: 0.9087 - val_loss: 0.4662 - val_acc: 0.8879\n",
      "Epoch 1127/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3215 - acc: 0.9086 - val_loss: 0.4570 - val_acc: 0.8875\n",
      "Epoch 1128/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3214 - acc: 0.9090 - val_loss: 0.4071 - val_acc: 0.9024\n",
      "Epoch 1129/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3211 - acc: 0.9070 - val_loss: 0.5287 - val_acc: 0.8696\n",
      "Epoch 1130/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3211 - acc: 0.9081 - val_loss: 0.4703 - val_acc: 0.8842\n",
      "Epoch 1131/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3211 - acc: 0.9086 - val_loss: 0.3832 - val_acc: 0.9089\n",
      "Epoch 1132/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3205 - acc: 0.9083 - val_loss: 0.4071 - val_acc: 0.9036\n",
      "Epoch 1133/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3208 - acc: 0.9080 - val_loss: 0.4013 - val_acc: 0.9041\n",
      "Epoch 1134/1500\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3201 - acc: 0.9076 - val_loss: 0.4794 - val_acc: 0.8842\n",
      "Epoch 1135/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3208 - acc: 0.9082 - val_loss: 0.4508 - val_acc: 0.8908\n",
      "Epoch 1136/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3208 - acc: 0.9090 - val_loss: 0.4448 - val_acc: 0.8921\n",
      "Epoch 1137/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3201 - acc: 0.9075 - val_loss: 0.4621 - val_acc: 0.8861\n",
      "Epoch 1138/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3201 - acc: 0.9084 - val_loss: 0.4770 - val_acc: 0.8803\n",
      "Epoch 1139/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3202 - acc: 0.9085 - val_loss: 0.4717 - val_acc: 0.8851\n",
      "Epoch 1140/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3198 - acc: 0.9087 - val_loss: 0.4516 - val_acc: 0.8898\n",
      "Epoch 1141/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3200 - acc: 0.9088 - val_loss: 0.4239 - val_acc: 0.8984\n",
      "Epoch 1142/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3201 - acc: 0.9079 - val_loss: 0.4272 - val_acc: 0.8965\n",
      "Epoch 1143/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3200 - acc: 0.9081 - val_loss: 0.4473 - val_acc: 0.8908\n",
      "Epoch 1144/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3194 - acc: 0.9085 - val_loss: 0.4379 - val_acc: 0.8930\n",
      "Epoch 1145/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3198 - acc: 0.9085 - val_loss: 0.4541 - val_acc: 0.8892\n",
      "Epoch 1146/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3189 - acc: 0.9086 - val_loss: 0.4322 - val_acc: 0.8974\n",
      "Epoch 1147/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3194 - acc: 0.9084 - val_loss: 0.4369 - val_acc: 0.8953\n",
      "Epoch 1148/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3192 - acc: 0.9087 - val_loss: 0.3704 - val_acc: 0.9135\n",
      "Epoch 1149/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3189 - acc: 0.9087 - val_loss: 0.4748 - val_acc: 0.8846\n",
      "Epoch 1150/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3193 - acc: 0.9096 - val_loss: 0.4419 - val_acc: 0.8939\n",
      "Epoch 1151/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3185 - acc: 0.9090 - val_loss: 0.5110 - val_acc: 0.8719\n",
      "Epoch 1152/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3187 - acc: 0.9085 - val_loss: 0.4164 - val_acc: 0.9000\n",
      "Epoch 1153/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3187 - acc: 0.9088 - val_loss: 0.4810 - val_acc: 0.8811\n",
      "Epoch 1154/1500\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.3183 - acc: 0.9090 - val_loss: 0.4379 - val_acc: 0.8942\n",
      "Epoch 1155/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3184 - acc: 0.9085 - val_loss: 0.3981 - val_acc: 0.9060\n",
      "Epoch 1156/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3178 - acc: 0.9098 - val_loss: 0.4274 - val_acc: 0.8965\n",
      "Epoch 1157/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3183 - acc: 0.9095 - val_loss: 0.4205 - val_acc: 0.8991\n",
      "Epoch 1158/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3181 - acc: 0.9095 - val_loss: 0.3865 - val_acc: 0.9089\n",
      "Epoch 1159/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3179 - acc: 0.9085 - val_loss: 0.3996 - val_acc: 0.9053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1160/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3180 - acc: 0.9092 - val_loss: 0.4450 - val_acc: 0.8914\n",
      "Epoch 1161/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3178 - acc: 0.9088 - val_loss: 0.4455 - val_acc: 0.8922\n",
      "Epoch 1162/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3177 - acc: 0.9092 - val_loss: 0.4698 - val_acc: 0.8834\n",
      "Epoch 1163/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.9093 - val_loss: 0.4412 - val_acc: 0.8931\n",
      "Epoch 1164/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3173 - acc: 0.9102 - val_loss: 0.3741 - val_acc: 0.9119\n",
      "Epoch 1165/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3176 - acc: 0.9103 - val_loss: 0.4743 - val_acc: 0.8816\n",
      "Epoch 1166/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3176 - acc: 0.9104 - val_loss: 0.4247 - val_acc: 0.8991\n",
      "Epoch 1167/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3169 - acc: 0.9104 - val_loss: 0.4631 - val_acc: 0.8876\n",
      "Epoch 1168/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3168 - acc: 0.9089 - val_loss: 0.4591 - val_acc: 0.8883\n",
      "Epoch 1169/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3170 - acc: 0.9089 - val_loss: 0.4119 - val_acc: 0.9010\n",
      "Epoch 1170/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3168 - acc: 0.9098 - val_loss: 0.4631 - val_acc: 0.8880\n",
      "Epoch 1171/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3169 - acc: 0.9100 - val_loss: 0.4283 - val_acc: 0.8969\n",
      "Epoch 1172/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3168 - acc: 0.9103 - val_loss: 0.4025 - val_acc: 0.9044\n",
      "Epoch 1173/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3167 - acc: 0.9090 - val_loss: 0.4169 - val_acc: 0.9000\n",
      "Epoch 1174/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3162 - acc: 0.9095 - val_loss: 0.3991 - val_acc: 0.9062\n",
      "Epoch 1175/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3160 - acc: 0.9092 - val_loss: 0.4686 - val_acc: 0.8847\n",
      "Epoch 1176/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3162 - acc: 0.9096 - val_loss: 0.4560 - val_acc: 0.8885\n",
      "Epoch 1177/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3162 - acc: 0.9094 - val_loss: 0.4859 - val_acc: 0.8802\n",
      "Epoch 1178/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3161 - acc: 0.9094 - val_loss: 0.4384 - val_acc: 0.8931\n",
      "Epoch 1179/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.3158 - acc: 0.9103 - val_loss: 0.4687 - val_acc: 0.8847\n",
      "Epoch 1180/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3160 - acc: 0.9091 - val_loss: 0.4569 - val_acc: 0.8893\n",
      "Epoch 1181/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3157 - acc: 0.9095 - val_loss: 0.4374 - val_acc: 0.8959\n",
      "Epoch 1182/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3154 - acc: 0.9101 - val_loss: 0.4119 - val_acc: 0.9006\n",
      "Epoch 1183/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3150 - acc: 0.9098 - val_loss: 0.3719 - val_acc: 0.9142\n",
      "Epoch 1184/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3153 - acc: 0.9102 - val_loss: 0.4144 - val_acc: 0.8992\n",
      "Epoch 1185/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3157 - acc: 0.9095 - val_loss: 0.4636 - val_acc: 0.8864\n",
      "Epoch 1186/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3153 - acc: 0.9101 - val_loss: 0.4539 - val_acc: 0.8905\n",
      "Epoch 1187/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3153 - acc: 0.9100 - val_loss: 0.4178 - val_acc: 0.8991\n",
      "Epoch 1188/1500\n",
      "30246/30246 [==============================] - 1s 46us/step - loss: 0.3153 - acc: 0.9096 - val_loss: 0.4557 - val_acc: 0.8889\n",
      "Epoch 1189/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3150 - acc: 0.9107 - val_loss: 0.3972 - val_acc: 0.9062\n",
      "Epoch 1190/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3149 - acc: 0.9102 - val_loss: 0.4582 - val_acc: 0.8839\n",
      "Epoch 1191/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3146 - acc: 0.9103 - val_loss: 0.4464 - val_acc: 0.8920\n",
      "Epoch 1192/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3148 - acc: 0.9096 - val_loss: 0.4184 - val_acc: 0.8991\n",
      "Epoch 1193/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3143 - acc: 0.9095 - val_loss: 0.4322 - val_acc: 0.8951\n",
      "Epoch 1194/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3144 - acc: 0.9104 - val_loss: 0.4148 - val_acc: 0.9011\n",
      "Epoch 1195/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3143 - acc: 0.9093 - val_loss: 0.4105 - val_acc: 0.9013\n",
      "Epoch 1196/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3142 - acc: 0.9098 - val_loss: 0.4154 - val_acc: 0.9019\n",
      "Epoch 1197/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3138 - acc: 0.9101 - val_loss: 0.3899 - val_acc: 0.9076\n",
      "Epoch 1198/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3143 - acc: 0.9094 - val_loss: 0.4151 - val_acc: 0.8998\n",
      "Epoch 1199/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3139 - acc: 0.9096 - val_loss: 0.4180 - val_acc: 0.9002\n",
      "Epoch 1200/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3140 - acc: 0.9096 - val_loss: 0.4225 - val_acc: 0.8979\n",
      "Epoch 1201/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3137 - acc: 0.9100 - val_loss: 0.4589 - val_acc: 0.8843\n",
      "Epoch 1202/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3142 - acc: 0.9105 - val_loss: 0.4312 - val_acc: 0.8965\n",
      "Epoch 1203/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3137 - acc: 0.9098 - val_loss: 0.4311 - val_acc: 0.8965\n",
      "Epoch 1204/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3131 - acc: 0.9104 - val_loss: 0.4305 - val_acc: 0.8958\n",
      "Epoch 1205/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3134 - acc: 0.9099 - val_loss: 0.4535 - val_acc: 0.8908\n",
      "Epoch 1206/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3132 - acc: 0.9105 - val_loss: 0.4009 - val_acc: 0.9047\n",
      "Epoch 1207/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3132 - acc: 0.9097 - val_loss: 0.4455 - val_acc: 0.8918\n",
      "Epoch 1208/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3132 - acc: 0.9099 - val_loss: 0.4419 - val_acc: 0.8929\n",
      "Epoch 1209/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3130 - acc: 0.9099 - val_loss: 0.4357 - val_acc: 0.8935\n",
      "Epoch 1210/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3129 - acc: 0.9116 - val_loss: 0.4229 - val_acc: 0.8990\n",
      "Epoch 1211/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3131 - acc: 0.9103 - val_loss: 0.3746 - val_acc: 0.9129\n",
      "Epoch 1212/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3132 - acc: 0.9101 - val_loss: 0.4257 - val_acc: 0.8967\n",
      "Epoch 1213/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3123 - acc: 0.9104 - val_loss: 0.4068 - val_acc: 0.9025\n",
      "Epoch 1214/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3125 - acc: 0.9107 - val_loss: 0.4504 - val_acc: 0.8873\n",
      "Epoch 1215/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3128 - acc: 0.9107 - val_loss: 0.4244 - val_acc: 0.8982\n",
      "Epoch 1216/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3125 - acc: 0.9106 - val_loss: 0.4013 - val_acc: 0.9044\n",
      "Epoch 1217/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3125 - acc: 0.9098 - val_loss: 0.4337 - val_acc: 0.8955\n",
      "Epoch 1218/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3121 - acc: 0.9100 - val_loss: 0.4136 - val_acc: 0.8994\n",
      "Epoch 1219/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3122 - acc: 0.9101 - val_loss: 0.3839 - val_acc: 0.9107\n",
      "Epoch 1220/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3118 - acc: 0.9102 - val_loss: 0.3461 - val_acc: 0.9212\n",
      "Epoch 1221/1500\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3123 - acc: 0.9101 - val_loss: 0.4809 - val_acc: 0.8809\n",
      "Epoch 1222/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3120 - acc: 0.9106 - val_loss: 0.4168 - val_acc: 0.8990\n",
      "Epoch 1223/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.9110 - val_loss: 0.4640 - val_acc: 0.8875\n",
      "Epoch 1224/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.9102 - val_loss: 0.4319 - val_acc: 0.8951\n",
      "Epoch 1225/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3116 - acc: 0.9108 - val_loss: 0.4284 - val_acc: 0.8971\n",
      "Epoch 1226/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3117 - acc: 0.9101 - val_loss: 0.4215 - val_acc: 0.8998\n",
      "Epoch 1227/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3114 - acc: 0.9111 - val_loss: 0.4255 - val_acc: 0.8967\n",
      "Epoch 1228/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.9110 - val_loss: 0.4959 - val_acc: 0.8779\n",
      "Epoch 1229/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3112 - acc: 0.9104 - val_loss: 0.4249 - val_acc: 0.8983\n",
      "Epoch 1230/1500\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 0.3114 - acc: 0.9108 - val_loss: 0.4312 - val_acc: 0.8934\n",
      "Epoch 1231/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3111 - acc: 0.9105 - val_loss: 0.4279 - val_acc: 0.8965\n",
      "Epoch 1232/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.9110 - val_loss: 0.4245 - val_acc: 0.8959\n",
      "Epoch 1233/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.3110 - acc: 0.9100 - val_loss: 0.3862 - val_acc: 0.9081\n",
      "Epoch 1234/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3109 - acc: 0.9114 - val_loss: 0.4372 - val_acc: 0.8928\n",
      "Epoch 1235/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3107 - acc: 0.9110 - val_loss: 0.4415 - val_acc: 0.8921\n",
      "Epoch 1236/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3105 - acc: 0.9109 - val_loss: 0.3898 - val_acc: 0.9064\n",
      "Epoch 1237/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3104 - acc: 0.9103 - val_loss: 0.4285 - val_acc: 0.8957\n",
      "Epoch 1238/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3102 - acc: 0.9114 - val_loss: 0.4284 - val_acc: 0.8967\n",
      "Epoch 1239/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3101 - acc: 0.9105 - val_loss: 0.4699 - val_acc: 0.8856\n",
      "Epoch 1240/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3098 - acc: 0.9113 - val_loss: 0.5016 - val_acc: 0.8754\n",
      "Epoch 1241/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3102 - acc: 0.9110 - val_loss: 0.4325 - val_acc: 0.8946\n",
      "Epoch 1242/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3101 - acc: 0.9108 - val_loss: 0.4338 - val_acc: 0.8951\n",
      "Epoch 1243/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3096 - acc: 0.9119 - val_loss: 0.4283 - val_acc: 0.8971\n",
      "Epoch 1244/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.9102 - val_loss: 0.4051 - val_acc: 0.9025\n",
      "Epoch 1245/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.9104 - val_loss: 0.4001 - val_acc: 0.9039\n",
      "Epoch 1246/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3098 - acc: 0.9110 - val_loss: 0.4254 - val_acc: 0.8965\n",
      "Epoch 1247/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.9105 - val_loss: 0.3637 - val_acc: 0.9146\n",
      "Epoch 1248/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3097 - acc: 0.9120 - val_loss: 0.5007 - val_acc: 0.8749\n",
      "Epoch 1249/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3095 - acc: 0.9111 - val_loss: 0.4587 - val_acc: 0.8884\n",
      "Epoch 1250/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3093 - acc: 0.9110 - val_loss: 0.4543 - val_acc: 0.8906\n",
      "Epoch 1251/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.9108 - val_loss: 0.4621 - val_acc: 0.8885\n",
      "Epoch 1252/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3088 - acc: 0.9108 - val_loss: 0.3857 - val_acc: 0.9086\n",
      "Epoch 1253/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3090 - acc: 0.9106 - val_loss: 0.4887 - val_acc: 0.8762\n",
      "Epoch 1254/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3090 - acc: 0.9120 - val_loss: 0.4047 - val_acc: 0.9041\n",
      "Epoch 1255/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3087 - acc: 0.9110 - val_loss: 0.4617 - val_acc: 0.8883\n",
      "Epoch 1256/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3086 - acc: 0.9118 - val_loss: 0.4730 - val_acc: 0.8842\n",
      "Epoch 1257/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3086 - acc: 0.9112 - val_loss: 0.4485 - val_acc: 0.8872\n",
      "Epoch 1258/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3088 - acc: 0.9106 - val_loss: 0.3982 - val_acc: 0.9051\n",
      "Epoch 1259/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3088 - acc: 0.9112 - val_loss: 0.4647 - val_acc: 0.8871\n",
      "Epoch 1260/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3084 - acc: 0.9117 - val_loss: 0.4628 - val_acc: 0.8871\n",
      "Epoch 1261/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3083 - acc: 0.9109 - val_loss: 0.4411 - val_acc: 0.8928\n",
      "Epoch 1262/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3080 - acc: 0.9113 - val_loss: 0.4443 - val_acc: 0.8918\n",
      "Epoch 1263/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3085 - acc: 0.9116 - val_loss: 0.4025 - val_acc: 0.9044\n",
      "Epoch 1264/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.9121 - val_loss: 0.3961 - val_acc: 0.9037\n",
      "Epoch 1265/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3078 - acc: 0.9119 - val_loss: 0.4993 - val_acc: 0.8754\n",
      "Epoch 1266/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3079 - acc: 0.9112 - val_loss: 0.4968 - val_acc: 0.8771\n",
      "Epoch 1267/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3076 - acc: 0.9112 - val_loss: 0.4507 - val_acc: 0.8881\n",
      "Epoch 1268/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3077 - acc: 0.9115 - val_loss: 0.4333 - val_acc: 0.8957\n",
      "Epoch 1269/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.9121 - val_loss: 0.4856 - val_acc: 0.8807\n",
      "Epoch 1270/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.9115 - val_loss: 0.4125 - val_acc: 0.8999\n",
      "Epoch 1271/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3075 - acc: 0.9118 - val_loss: 0.4688 - val_acc: 0.8832\n",
      "Epoch 1272/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3073 - acc: 0.9119 - val_loss: 0.4123 - val_acc: 0.8992\n",
      "Epoch 1273/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.9115 - val_loss: 0.4648 - val_acc: 0.8863\n",
      "Epoch 1274/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3068 - acc: 0.9109 - val_loss: 0.4681 - val_acc: 0.8848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1275/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3070 - acc: 0.9117 - val_loss: 0.4747 - val_acc: 0.8806\n",
      "Epoch 1276/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.9121 - val_loss: 0.3874 - val_acc: 0.9082\n",
      "Epoch 1277/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.9122 - val_loss: 0.4494 - val_acc: 0.8914\n",
      "Epoch 1278/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3069 - acc: 0.9116 - val_loss: 0.4127 - val_acc: 0.9003\n",
      "Epoch 1279/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3067 - acc: 0.9117 - val_loss: 0.3878 - val_acc: 0.9062\n",
      "Epoch 1280/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3070 - acc: 0.9114 - val_loss: 0.4245 - val_acc: 0.8965\n",
      "Epoch 1281/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3064 - acc: 0.9128 - val_loss: 0.4296 - val_acc: 0.8963\n",
      "Epoch 1282/1500\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.3064 - acc: 0.9123 - val_loss: 0.4712 - val_acc: 0.8856\n",
      "Epoch 1283/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3066 - acc: 0.9120 - val_loss: 0.4709 - val_acc: 0.8846\n",
      "Epoch 1284/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3064 - acc: 0.9117 - val_loss: 0.4190 - val_acc: 0.8992\n",
      "Epoch 1285/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3064 - acc: 0.9125 - val_loss: 0.4699 - val_acc: 0.8831\n",
      "Epoch 1286/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.9113 - val_loss: 0.4032 - val_acc: 0.9032\n",
      "Epoch 1287/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3060 - acc: 0.9123 - val_loss: 0.4491 - val_acc: 0.8909\n",
      "Epoch 1288/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3057 - acc: 0.9121 - val_loss: 0.4640 - val_acc: 0.8859\n",
      "Epoch 1289/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3061 - acc: 0.9116 - val_loss: 0.4291 - val_acc: 0.8954\n",
      "Epoch 1290/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.9120 - val_loss: 0.5220 - val_acc: 0.8674\n",
      "Epoch 1291/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3060 - acc: 0.9116 - val_loss: 0.4579 - val_acc: 0.8875\n",
      "Epoch 1292/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3056 - acc: 0.9133 - val_loss: 0.4712 - val_acc: 0.8831\n",
      "Epoch 1293/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3050 - acc: 0.9133 - val_loss: 0.4481 - val_acc: 0.8924\n",
      "Epoch 1294/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3052 - acc: 0.9126 - val_loss: 0.4844 - val_acc: 0.8820\n",
      "Epoch 1295/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3053 - acc: 0.9130 - val_loss: 0.4526 - val_acc: 0.8897\n",
      "Epoch 1296/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3055 - acc: 0.9117 - val_loss: 0.4454 - val_acc: 0.8913\n",
      "Epoch 1297/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3051 - acc: 0.9124 - val_loss: 0.4212 - val_acc: 0.8972\n",
      "Epoch 1298/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3048 - acc: 0.9133 - val_loss: 0.4340 - val_acc: 0.8942\n",
      "Epoch 1299/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3051 - acc: 0.9122 - val_loss: 0.4029 - val_acc: 0.9043\n",
      "Epoch 1300/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3050 - acc: 0.9124 - val_loss: 0.4838 - val_acc: 0.8831\n",
      "Epoch 1301/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.9123 - val_loss: 0.4349 - val_acc: 0.8953\n",
      "Epoch 1302/1500\n",
      "30246/30246 [==============================] - 1s 46us/step - loss: 0.3050 - acc: 0.9123 - val_loss: 0.4082 - val_acc: 0.9019\n",
      "Epoch 1303/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3045 - acc: 0.9123 - val_loss: 0.4125 - val_acc: 0.9011\n",
      "Epoch 1304/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3049 - acc: 0.9124 - val_loss: 0.4559 - val_acc: 0.8896\n",
      "Epoch 1305/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3048 - acc: 0.9119 - val_loss: 0.4428 - val_acc: 0.8924\n",
      "Epoch 1306/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3040 - acc: 0.9126 - val_loss: 0.4119 - val_acc: 0.9007\n",
      "Epoch 1307/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.9124 - val_loss: 0.4483 - val_acc: 0.8921\n",
      "Epoch 1308/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3045 - acc: 0.9120 - val_loss: 0.4521 - val_acc: 0.8904\n",
      "Epoch 1309/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.9121 - val_loss: 0.4450 - val_acc: 0.8908\n",
      "Epoch 1310/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3041 - acc: 0.9123 - val_loss: 0.4251 - val_acc: 0.8961\n",
      "Epoch 1311/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3043 - acc: 0.9123 - val_loss: 0.4261 - val_acc: 0.8961\n",
      "Epoch 1312/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.9123 - val_loss: 0.4272 - val_acc: 0.8963\n",
      "Epoch 1313/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3039 - acc: 0.9126 - val_loss: 0.4147 - val_acc: 0.8991\n",
      "Epoch 1314/1500\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 0.3039 - acc: 0.9129 - val_loss: 0.3959 - val_acc: 0.9041\n",
      "Epoch 1315/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.9129 - val_loss: 0.4349 - val_acc: 0.8941\n",
      "Epoch 1316/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.9127 - val_loss: 0.4289 - val_acc: 0.8949\n",
      "Epoch 1317/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3035 - acc: 0.9122 - val_loss: 0.4482 - val_acc: 0.8909\n",
      "Epoch 1318/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.9119 - val_loss: 0.4716 - val_acc: 0.8807\n",
      "Epoch 1319/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3036 - acc: 0.9127 - val_loss: 0.4473 - val_acc: 0.8917\n",
      "Epoch 1320/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3033 - acc: 0.9132 - val_loss: 0.4831 - val_acc: 0.8832\n",
      "Epoch 1321/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3037 - acc: 0.9139 - val_loss: 0.4501 - val_acc: 0.8920\n",
      "Epoch 1322/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3032 - acc: 0.9121 - val_loss: 0.4296 - val_acc: 0.8962\n",
      "Epoch 1323/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.9122 - val_loss: 0.4463 - val_acc: 0.8913\n",
      "Epoch 1324/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3030 - acc: 0.9136 - val_loss: 0.3996 - val_acc: 0.9033\n",
      "Epoch 1325/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3026 - acc: 0.9123 - val_loss: 0.4629 - val_acc: 0.8879\n",
      "Epoch 1326/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3031 - acc: 0.9126 - val_loss: 0.4277 - val_acc: 0.8953\n",
      "Epoch 1327/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.9134 - val_loss: 0.4735 - val_acc: 0.8843\n",
      "Epoch 1328/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3028 - acc: 0.9121 - val_loss: 0.4486 - val_acc: 0.8898\n",
      "Epoch 1329/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3024 - acc: 0.9126 - val_loss: 0.4661 - val_acc: 0.8850\n",
      "Epoch 1330/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.9134 - val_loss: 0.3996 - val_acc: 0.9032\n",
      "Epoch 1331/1500\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 0.3022 - acc: 0.9123 - val_loss: 0.4245 - val_acc: 0.8967\n",
      "Epoch 1332/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3023 - acc: 0.9123 - val_loss: 0.4393 - val_acc: 0.8906\n",
      "Epoch 1333/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3025 - acc: 0.9129 - val_loss: 0.4506 - val_acc: 0.8909\n",
      "Epoch 1334/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3019 - acc: 0.9127 - val_loss: 0.4384 - val_acc: 0.8937\n",
      "Epoch 1335/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3017 - acc: 0.9133 - val_loss: 0.3989 - val_acc: 0.9048\n",
      "Epoch 1336/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3021 - acc: 0.9133 - val_loss: 0.4374 - val_acc: 0.8939\n",
      "Epoch 1337/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3018 - acc: 0.9137 - val_loss: 0.4600 - val_acc: 0.8888\n",
      "Epoch 1338/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3018 - acc: 0.9134 - val_loss: 0.4303 - val_acc: 0.8946\n",
      "Epoch 1339/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3022 - acc: 0.9125 - val_loss: 0.4823 - val_acc: 0.8798\n",
      "Epoch 1340/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3010 - acc: 0.9132 - val_loss: 0.4146 - val_acc: 0.8998\n",
      "Epoch 1341/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3017 - acc: 0.9128 - val_loss: 0.4841 - val_acc: 0.8781\n",
      "Epoch 1342/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.9120 - val_loss: 0.4587 - val_acc: 0.8888\n",
      "Epoch 1343/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3014 - acc: 0.9130 - val_loss: 0.3842 - val_acc: 0.9085\n",
      "Epoch 1344/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3016 - acc: 0.9128 - val_loss: 0.4003 - val_acc: 0.9056\n",
      "Epoch 1345/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3012 - acc: 0.9128 - val_loss: 0.4401 - val_acc: 0.8929\n",
      "Epoch 1346/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3012 - acc: 0.9134 - val_loss: 0.4305 - val_acc: 0.8953\n",
      "Epoch 1347/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3011 - acc: 0.9130 - val_loss: 0.4362 - val_acc: 0.8946\n",
      "Epoch 1348/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3011 - acc: 0.9135 - val_loss: 0.4120 - val_acc: 0.8998\n",
      "Epoch 1349/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3013 - acc: 0.9135 - val_loss: 0.4093 - val_acc: 0.9004\n",
      "Epoch 1350/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3009 - acc: 0.9148 - val_loss: 0.4367 - val_acc: 0.8937\n",
      "Epoch 1351/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3006 - acc: 0.9135 - val_loss: 0.4280 - val_acc: 0.8963\n",
      "Epoch 1352/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3008 - acc: 0.9137 - val_loss: 0.4051 - val_acc: 0.9028\n",
      "Epoch 1353/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3007 - acc: 0.9129 - val_loss: 0.4551 - val_acc: 0.8879\n",
      "Epoch 1354/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3009 - acc: 0.9132 - val_loss: 0.4586 - val_acc: 0.8883\n",
      "Epoch 1355/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3003 - acc: 0.9138 - val_loss: 0.4027 - val_acc: 0.9044\n",
      "Epoch 1356/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3002 - acc: 0.9139 - val_loss: 0.4553 - val_acc: 0.8905\n",
      "Epoch 1357/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3002 - acc: 0.9144 - val_loss: 0.4729 - val_acc: 0.8847\n",
      "Epoch 1358/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3004 - acc: 0.9133 - val_loss: 0.4039 - val_acc: 0.9031\n",
      "Epoch 1359/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2999 - acc: 0.9131 - val_loss: 0.4162 - val_acc: 0.9008\n",
      "Epoch 1360/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3001 - acc: 0.9136 - val_loss: 0.4066 - val_acc: 0.9027\n",
      "Epoch 1361/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.9133 - val_loss: 0.4016 - val_acc: 0.9040\n",
      "Epoch 1362/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.9136 - val_loss: 0.4223 - val_acc: 0.8974\n",
      "Epoch 1363/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.2999 - acc: 0.9132 - val_loss: 0.3910 - val_acc: 0.9061\n",
      "Epoch 1364/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.9130 - val_loss: 0.4363 - val_acc: 0.8918\n",
      "Epoch 1365/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.9144 - val_loss: 0.3496 - val_acc: 0.9195\n",
      "Epoch 1366/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2999 - acc: 0.9130 - val_loss: 0.4295 - val_acc: 0.8947\n",
      "Epoch 1367/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2998 - acc: 0.9131 - val_loss: 0.4264 - val_acc: 0.8965\n",
      "Epoch 1368/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2995 - acc: 0.9139 - val_loss: 0.4133 - val_acc: 0.9003\n",
      "Epoch 1369/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.9137 - val_loss: 0.4883 - val_acc: 0.8820\n",
      "Epoch 1370/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2994 - acc: 0.9129 - val_loss: 0.4785 - val_acc: 0.8816\n",
      "Epoch 1371/1500\n",
      "30246/30246 [==============================] - 1s 35us/step - loss: 0.2997 - acc: 0.9139 - val_loss: 0.4232 - val_acc: 0.8974\n",
      "Epoch 1372/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2991 - acc: 0.9142 - val_loss: 0.4859 - val_acc: 0.8797\n",
      "Epoch 1373/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2993 - acc: 0.9144 - val_loss: 0.5063 - val_acc: 0.8711\n",
      "Epoch 1374/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2988 - acc: 0.9139 - val_loss: 0.3755 - val_acc: 0.9113\n",
      "Epoch 1375/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2986 - acc: 0.9136 - val_loss: 0.5254 - val_acc: 0.8667\n",
      "Epoch 1376/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.9136 - val_loss: 0.4673 - val_acc: 0.8856\n",
      "Epoch 1377/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2989 - acc: 0.9136 - val_loss: 0.3862 - val_acc: 0.9070\n",
      "Epoch 1378/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2991 - acc: 0.9139 - val_loss: 0.4346 - val_acc: 0.8941\n",
      "Epoch 1379/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2987 - acc: 0.9143 - val_loss: 0.3937 - val_acc: 0.9057\n",
      "Epoch 1380/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2981 - acc: 0.9135 - val_loss: 0.3801 - val_acc: 0.9073\n",
      "Epoch 1381/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2986 - acc: 0.9141 - val_loss: 0.3995 - val_acc: 0.9027\n",
      "Epoch 1382/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2987 - acc: 0.9139 - val_loss: 0.4009 - val_acc: 0.9039\n",
      "Epoch 1383/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2984 - acc: 0.9140 - val_loss: 0.4254 - val_acc: 0.8957\n",
      "Epoch 1384/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2982 - acc: 0.9143 - val_loss: 0.4460 - val_acc: 0.8922\n",
      "Epoch 1385/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.9138 - val_loss: 0.4756 - val_acc: 0.8840\n",
      "Epoch 1386/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2983 - acc: 0.9139 - val_loss: 0.4182 - val_acc: 0.8982\n",
      "Epoch 1387/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.9141 - val_loss: 0.4472 - val_acc: 0.8916\n",
      "Epoch 1388/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.9134 - val_loss: 0.4114 - val_acc: 0.9017\n",
      "Epoch 1389/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2979 - acc: 0.9139 - val_loss: 0.3769 - val_acc: 0.9098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1390/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.9143 - val_loss: 0.4134 - val_acc: 0.9020\n",
      "Epoch 1391/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2980 - acc: 0.9137 - val_loss: 0.4358 - val_acc: 0.8942\n",
      "Epoch 1392/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2977 - acc: 0.9135 - val_loss: 0.4148 - val_acc: 0.8988\n",
      "Epoch 1393/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2977 - acc: 0.9140 - val_loss: 0.4173 - val_acc: 0.8983\n",
      "Epoch 1394/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2978 - acc: 0.9143 - val_loss: 0.4150 - val_acc: 0.9011\n",
      "Epoch 1395/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2975 - acc: 0.9139 - val_loss: 0.4487 - val_acc: 0.8902\n",
      "Epoch 1396/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2970 - acc: 0.9140 - val_loss: 0.4243 - val_acc: 0.8965\n",
      "Epoch 1397/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2975 - acc: 0.9138 - val_loss: 0.4501 - val_acc: 0.8906\n",
      "Epoch 1398/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2971 - acc: 0.9152 - val_loss: 0.4570 - val_acc: 0.8863\n",
      "Epoch 1399/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2970 - acc: 0.9134 - val_loss: 0.4717 - val_acc: 0.8827\n",
      "Epoch 1400/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2973 - acc: 0.9150 - val_loss: 0.4967 - val_acc: 0.8766\n",
      "Epoch 1401/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2968 - acc: 0.9154 - val_loss: 0.4184 - val_acc: 0.8986\n",
      "Epoch 1402/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.9149 - val_loss: 0.3806 - val_acc: 0.9094\n",
      "Epoch 1403/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.9156 - val_loss: 0.4478 - val_acc: 0.8904\n",
      "Epoch 1404/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2965 - acc: 0.9155 - val_loss: 0.4649 - val_acc: 0.8861\n",
      "Epoch 1405/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.9146 - val_loss: 0.3810 - val_acc: 0.9073\n",
      "Epoch 1406/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2968 - acc: 0.9150 - val_loss: 0.4284 - val_acc: 0.8955\n",
      "Epoch 1407/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2966 - acc: 0.9152 - val_loss: 0.4506 - val_acc: 0.8906\n",
      "Epoch 1408/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2962 - acc: 0.9150 - val_loss: 0.4087 - val_acc: 0.9010\n",
      "Epoch 1409/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2965 - acc: 0.9147 - val_loss: 0.5022 - val_acc: 0.8774\n",
      "Epoch 1410/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2964 - acc: 0.9147 - val_loss: 0.4495 - val_acc: 0.8900\n",
      "Epoch 1411/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2960 - acc: 0.9146 - val_loss: 0.4275 - val_acc: 0.8974\n",
      "Epoch 1412/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2961 - acc: 0.9141 - val_loss: 0.4222 - val_acc: 0.8983\n",
      "Epoch 1413/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2960 - acc: 0.9145 - val_loss: 0.4717 - val_acc: 0.8864\n",
      "Epoch 1414/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2957 - acc: 0.9145 - val_loss: 0.4677 - val_acc: 0.8865\n",
      "Epoch 1415/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2965 - acc: 0.9153 - val_loss: 0.4060 - val_acc: 0.9010\n",
      "Epoch 1416/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2957 - acc: 0.9148 - val_loss: 0.4160 - val_acc: 0.8988\n",
      "Epoch 1417/1500\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.2958 - acc: 0.9150 - val_loss: 0.4030 - val_acc: 0.9025\n",
      "Epoch 1418/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.9142 - val_loss: 0.4257 - val_acc: 0.8967\n",
      "Epoch 1419/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2958 - acc: 0.9144 - val_loss: 0.4013 - val_acc: 0.9032\n",
      "Epoch 1420/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2956 - acc: 0.9143 - val_loss: 0.4104 - val_acc: 0.9012\n",
      "Epoch 1421/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2954 - acc: 0.9144 - val_loss: 0.4459 - val_acc: 0.8913\n",
      "Epoch 1422/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.9150 - val_loss: 0.4106 - val_acc: 0.8998\n",
      "Epoch 1423/1500\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.2959 - acc: 0.9149 - val_loss: 0.4550 - val_acc: 0.8894\n",
      "Epoch 1424/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2949 - acc: 0.9146 - val_loss: 0.3484 - val_acc: 0.9177\n",
      "Epoch 1425/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.9138 - val_loss: 0.4432 - val_acc: 0.8921\n",
      "Epoch 1426/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2955 - acc: 0.9145 - val_loss: 0.4415 - val_acc: 0.8912\n",
      "Epoch 1427/1500\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 0.2952 - acc: 0.9140 - val_loss: 0.4253 - val_acc: 0.8955\n",
      "Epoch 1428/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.9145 - val_loss: 0.4269 - val_acc: 0.8955\n",
      "Epoch 1429/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2948 - acc: 0.9151 - val_loss: 0.3883 - val_acc: 0.9074\n",
      "Epoch 1430/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2945 - acc: 0.9155 - val_loss: 0.4321 - val_acc: 0.8949\n",
      "Epoch 1431/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2950 - acc: 0.9155 - val_loss: 0.4443 - val_acc: 0.8918\n",
      "Epoch 1432/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2945 - acc: 0.9146 - val_loss: 0.4163 - val_acc: 0.8992\n",
      "Epoch 1433/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2947 - acc: 0.9151 - val_loss: 0.4386 - val_acc: 0.8934\n",
      "Epoch 1434/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2948 - acc: 0.9148 - val_loss: 0.4457 - val_acc: 0.8926\n",
      "Epoch 1435/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2942 - acc: 0.9154 - val_loss: 0.4187 - val_acc: 0.8983\n",
      "Epoch 1436/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2947 - acc: 0.9147 - val_loss: 0.4323 - val_acc: 0.8941\n",
      "Epoch 1437/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.2941 - acc: 0.9155 - val_loss: 0.4193 - val_acc: 0.8996\n",
      "Epoch 1438/1500\n",
      "30246/30246 [==============================] - 2s 58us/step - loss: 0.2945 - acc: 0.9146 - val_loss: 0.4452 - val_acc: 0.8916\n",
      "Epoch 1439/1500\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.2942 - acc: 0.9148 - val_loss: 0.4807 - val_acc: 0.8832\n",
      "Epoch 1440/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2941 - acc: 0.9151 - val_loss: 0.4341 - val_acc: 0.8950\n",
      "Epoch 1441/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2943 - acc: 0.9157 - val_loss: 0.4789 - val_acc: 0.8827\n",
      "Epoch 1442/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2940 - acc: 0.9154 - val_loss: 0.4213 - val_acc: 0.8967\n",
      "Epoch 1443/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.2939 - acc: 0.9156 - val_loss: 0.4315 - val_acc: 0.8945\n",
      "Epoch 1444/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.2940 - acc: 0.9146 - val_loss: 0.4434 - val_acc: 0.8917\n",
      "Epoch 1445/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.2937 - acc: 0.9152 - val_loss: 0.4411 - val_acc: 0.8922\n",
      "Epoch 1446/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2938 - acc: 0.9155 - val_loss: 0.4230 - val_acc: 0.8970\n",
      "Epoch 1447/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2936 - acc: 0.9158 - val_loss: 0.4548 - val_acc: 0.8887\n",
      "Epoch 1448/1500\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.2939 - acc: 0.9157 - val_loss: 0.3710 - val_acc: 0.9107\n",
      "Epoch 1449/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.2933 - acc: 0.9158 - val_loss: 0.4031 - val_acc: 0.9043\n",
      "Epoch 1450/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2933 - acc: 0.9155 - val_loss: 0.4040 - val_acc: 0.9041\n",
      "Epoch 1451/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.2936 - acc: 0.9147 - val_loss: 0.4223 - val_acc: 0.8980\n",
      "Epoch 1452/1500\n",
      "30246/30246 [==============================] - 1s 32us/step - loss: 0.2936 - acc: 0.9149 - val_loss: 0.4459 - val_acc: 0.8914\n",
      "Epoch 1453/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.9154 - val_loss: 0.4045 - val_acc: 0.9028\n",
      "Epoch 1454/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2937 - acc: 0.9145 - val_loss: 0.4919 - val_acc: 0.8794\n",
      "Epoch 1455/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2931 - acc: 0.9155 - val_loss: 0.4372 - val_acc: 0.8934\n",
      "Epoch 1456/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2932 - acc: 0.9153 - val_loss: 0.4731 - val_acc: 0.8842\n",
      "Epoch 1457/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2933 - acc: 0.9154 - val_loss: 0.4390 - val_acc: 0.8926\n",
      "Epoch 1458/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.9157 - val_loss: 0.4321 - val_acc: 0.8931\n",
      "Epoch 1459/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2928 - acc: 0.9156 - val_loss: 0.4522 - val_acc: 0.8898\n",
      "Epoch 1460/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2929 - acc: 0.9149 - val_loss: 0.4325 - val_acc: 0.8945\n",
      "Epoch 1461/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2928 - acc: 0.9154 - val_loss: 0.4428 - val_acc: 0.8942\n",
      "Epoch 1462/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.2925 - acc: 0.9160 - val_loss: 0.4492 - val_acc: 0.8908\n",
      "Epoch 1463/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.2926 - acc: 0.9149 - val_loss: 0.4654 - val_acc: 0.8851\n",
      "Epoch 1464/1500\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.2924 - acc: 0.9151 - val_loss: 0.5002 - val_acc: 0.8760\n",
      "Epoch 1465/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2925 - acc: 0.9151 - val_loss: 0.4473 - val_acc: 0.8910\n",
      "Epoch 1466/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2925 - acc: 0.9156 - val_loss: 0.4203 - val_acc: 0.8966\n",
      "Epoch 1467/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.2923 - acc: 0.9154 - val_loss: 0.4056 - val_acc: 0.9029\n",
      "Epoch 1468/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2922 - acc: 0.9156 - val_loss: 0.4083 - val_acc: 0.9013\n",
      "Epoch 1469/1500\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.2921 - acc: 0.9155 - val_loss: 0.4605 - val_acc: 0.8871\n",
      "Epoch 1470/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2920 - acc: 0.9152 - val_loss: 0.4644 - val_acc: 0.8875\n",
      "Epoch 1471/1500\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.2921 - acc: 0.9149 - val_loss: 0.4204 - val_acc: 0.8982\n",
      "Epoch 1472/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.2922 - acc: 0.9154 - val_loss: 0.4795 - val_acc: 0.8801\n",
      "Epoch 1473/1500\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.2915 - acc: 0.9154 - val_loss: 0.4349 - val_acc: 0.8939\n",
      "Epoch 1474/1500\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 0.2918 - acc: 0.9155 - val_loss: 0.4183 - val_acc: 0.8982\n",
      "Epoch 1475/1500\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.2916 - acc: 0.9156 - val_loss: 0.4396 - val_acc: 0.8933\n",
      "Epoch 1476/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2917 - acc: 0.9162 - val_loss: 0.4057 - val_acc: 0.9019\n",
      "Epoch 1477/1500\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.2920 - acc: 0.9155 - val_loss: 0.4066 - val_acc: 0.9016\n",
      "Epoch 1478/1500\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.2921 - acc: 0.9158 - val_loss: 0.4522 - val_acc: 0.8902\n",
      "Epoch 1479/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2914 - acc: 0.9164 - val_loss: 0.3495 - val_acc: 0.9187\n",
      "Epoch 1480/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2915 - acc: 0.9164 - val_loss: 0.4516 - val_acc: 0.8920\n",
      "Epoch 1481/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2914 - acc: 0.9155 - val_loss: 0.4214 - val_acc: 0.8984\n",
      "Epoch 1482/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2912 - acc: 0.9165 - val_loss: 0.4089 - val_acc: 0.9012\n",
      "Epoch 1483/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2913 - acc: 0.9157 - val_loss: 0.4819 - val_acc: 0.8832\n",
      "Epoch 1484/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2910 - acc: 0.9158 - val_loss: 0.4553 - val_acc: 0.8880\n",
      "Epoch 1485/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2912 - acc: 0.9158 - val_loss: 0.4363 - val_acc: 0.8942\n",
      "Epoch 1486/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2911 - acc: 0.9158 - val_loss: 0.4296 - val_acc: 0.8958\n",
      "Epoch 1487/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2906 - acc: 0.9164 - val_loss: 0.4420 - val_acc: 0.8921\n",
      "Epoch 1488/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2911 - acc: 0.9152 - val_loss: 0.4167 - val_acc: 0.8995\n",
      "Epoch 1489/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2907 - acc: 0.9155 - val_loss: 0.3791 - val_acc: 0.9111\n",
      "Epoch 1490/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2909 - acc: 0.9163 - val_loss: 0.4549 - val_acc: 0.8884\n",
      "Epoch 1491/1500\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.2908 - acc: 0.9157 - val_loss: 0.4683 - val_acc: 0.8861\n",
      "Epoch 1492/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2909 - acc: 0.9158 - val_loss: 0.4369 - val_acc: 0.8931\n",
      "Epoch 1493/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2908 - acc: 0.9149 - val_loss: 0.4326 - val_acc: 0.8946\n",
      "Epoch 1494/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2904 - acc: 0.9160 - val_loss: 0.4459 - val_acc: 0.8908\n",
      "Epoch 1495/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2907 - acc: 0.9157 - val_loss: 0.4470 - val_acc: 0.8909\n",
      "Epoch 1496/1500\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.2905 - acc: 0.9160 - val_loss: 0.4522 - val_acc: 0.8887\n",
      "Epoch 1497/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2902 - acc: 0.9161 - val_loss: 0.4436 - val_acc: 0.8916\n",
      "Epoch 1498/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2901 - acc: 0.9162 - val_loss: 0.4495 - val_acc: 0.8897\n",
      "Epoch 1499/1500\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.2905 - acc: 0.9163 - val_loss: 0.4343 - val_acc: 0.8950\n",
      "Epoch 1500/1500\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.2903 - acc: 0.9152 - val_loss: 0.4561 - val_acc: 0.8867\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_shape=(256,)),\n",
    "    Activation('relu'),\n",
    "    Dense(5),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1500, validation_split=0.2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ph = plot_history(history, \"Perceptron model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yhat_test= model.predict_on_batch(x_test)\n",
    "error = np.linalg.norm(((yhat_test.round()) - y_test),axis=1)\n",
    "error_indices = np.nonzero(error)[0]\n",
    "\n",
    "ps = plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) \n",
    "\n",
    "B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
