{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miniproject 1: Image Classification\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Description\n",
    "\n",
    "One of the deepest traditions in learning about deep learning is to first [tackle the exciting problem of MNIST classification](http://deeplearning.net/tutorial/logreg.html). [The MNIST database](https://en.wikipedia.org/wiki/MNIST_database) (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that was [recently extended](https://arxiv.org/abs/1702.05373). We break with this tradition (just a little bit) and tackle first the related problem of classifying cropped, downsampled and grayscaled images of house numbers in the [The Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/).\n",
    "\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- You should have a running installation of [tensorflow](https://www.tensorflow.org/install/) and [keras](https://keras.io/).\n",
    "- You should know the concepts \"multilayer perceptron\", \"stochastic gradient descent with minibatches\", \"training and validation data\", \"overfitting\" and \"early stopping\".\n",
    "\n",
    "### What you will learn\n",
    "\n",
    "- You will learn how to define feedforward neural networks in keras and fit them to data.\n",
    "- You will be guided through a prototyping procedure for the application of deep learning to a specific domain.\n",
    "- You will get in contact with concepts discussed later in the lecture, like \"regularization\", \"batch normalization\" and \"convolutional networks\".\n",
    "- You will gain some experience on the influence of network architecture, optimizer and regularization choices on the goodness of fit.\n",
    "- You will learn to be more patient :) Some fits may take your computer quite a bit of time; run them over night.\n",
    "\n",
    "### Evaluation criteria\n",
    "\n",
    "The evaluation is (mostly) based on the figures you submit and your answer sentences. \n",
    "We will only do random tests of your code and not re-run the full notebook.\n",
    "\n",
    "### Your names\n",
    "\n",
    "Before you start, please enter your full name(s) in the field below; they are used to load the data. The variable student2 may remain empty, if you work alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:08:24.514461Z",
     "start_time": "2018-03-09T09:08:24.506410Z"
    }
   },
   "outputs": [],
   "source": [
    "student1 = \"Pau Argelaguet Franquelo\"\n",
    "student2 = \"Natalie Bolon Brun\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some helper functions\n",
    "\n",
    "For your convenience we provide here some functions to preprocess the data and plot the results later. Simply run the following cells with `Shift-Enter`.\n",
    "\n",
    "### Dependencies and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-09T09:09:16.113721Z",
     "start_time": "2018-03-09T09:09:16.100520Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "# you may experiment with different subsets, \n",
    "# but make sure in the submission \n",
    "# it is generated with the correct random seed for all exercises.\n",
    "np.random.seed(hash(student1 + student2) % 2**32)\n",
    "subset_of_classes = np.random.choice(range(10), 5, replace = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "def plot_some_samples(x, y = [], yhat = [], select_from = [], \n",
    "                      ncols = 6, nrows = 4, xdim = 16, ydim = 16,\n",
    "                      label_mapping = range(10)):\n",
    "    \"\"\"plot some input vectors as grayscale images (optionally together with their assigned or predicted labels).\n",
    "    \n",
    "    x is an NxD - dimensional array, where D is the length of an input vector and N is the number of samples.\n",
    "    Out of the N samples, ncols x nrows indices are randomly selected from the list select_from (if it is empty, select_from becomes range(N)).\n",
    "    \n",
    "    Keyword arguments:\n",
    "    y             -- corresponding labels to plot in green below each image.\n",
    "    yhat          -- corresponding predicted labels to plot in red below each image.\n",
    "    select_from   -- list of indices from which to select the images.\n",
    "    ncols, nrows  -- number of columns and rows to plot.\n",
    "    xdim, ydim    -- number of pixels of the images in x- and y-direction.\n",
    "    label_mapping -- map labels to digits.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(nrows, ncols)\n",
    "    if len(select_from) == 0:\n",
    "        select_from = range(x.shape[0])\n",
    "    indices = np.random.choice(select_from, size = min(ncols * nrows, len(select_from)), replace = False)\n",
    "    for i, ind in enumerate(indices):\n",
    "        thisax = ax[i//ncols,i%ncols]\n",
    "        thisax.matshow(x[ind].reshape(xdim, ydim), cmap='gray')\n",
    "        thisax.set_axis_off()\n",
    "        if len(y) != 0:\n",
    "            j = y[ind] if type(y[ind]) != np.ndarray else y[ind].argmax()\n",
    "            thisax.text(0, 0, (label_mapping[j]+1)%10, color='green', \n",
    "                                                       verticalalignment='top',\n",
    "                                                       transform=thisax.transAxes)\n",
    "        if len(yhat) != 0:\n",
    "            k = yhat[ind] if type(yhat[ind]) != np.ndarray else yhat[ind].argmax()\n",
    "            thisax.text(1, 0, (label_mapping[k]+1)%10, color='red',\n",
    "                                             verticalalignment='top',\n",
    "                                             horizontalalignment='right',\n",
    "                                             transform=thisax.transAxes)\n",
    "    return fig\n",
    "\n",
    "def prepare_standardplot(title, xlabel):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.suptitle(title)\n",
    "    ax1.set_ylabel('categorical cross entropy')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_ylabel('accuracy [% correct]')\n",
    "    ax2.set_xlabel(xlabel)\n",
    "    return fig, ax1, ax2\n",
    "\n",
    "def finalize_standardplot(fig, ax1, ax2):\n",
    "    ax1handles, ax1labels = ax1.get_legend_handles_labels()\n",
    "    if len(ax1labels) > 0:\n",
    "        ax1.legend(ax1handles, ax1labels)\n",
    "    ax2handles, ax2labels = ax2.get_legend_handles_labels()\n",
    "    if len(ax2labels) > 0:\n",
    "        ax2.legend(ax2handles, ax2labels)\n",
    "    fig.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "\n",
    "def plot_history(history, title):\n",
    "    fig, ax1, ax2 = prepare_standardplot(title, 'epoch')\n",
    "    ax1.plot(history.history['loss'], label = \"training\")\n",
    "    ax1.plot(history.history['val_loss'], label = \"validation\")\n",
    "    ax2.plot(history.history['acc'], label = \"training\")\n",
    "    ax2.plot(history.history['val_acc'], label = \"validation\")\n",
    "    finalize_standardplot(fig, ax1, ax2)\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and preprocessing the data\n",
    "\n",
    "The data consists of RGB color images with 32x32 pixels, loaded into an array of dimension 32x32x3x(number of images). We convert them to grayscale (using [this method](https://en.wikipedia.org/wiki/SRGB#The_reverse_transformation)) and we downsample them to images of 16x16 pixels by averaging over patches of 2x2 pixels.\n",
    "\n",
    "With these preprocessing steps we obviously remove some information that could be helpful in classifying the images. But, since the processed data is much lower dimensional, the fitting procedures converge faster. This is an advantage in situations like here (or generally when prototyping), were we want to try many different things without having to wait too long for computations to finish. After having gained some experience, one may want to go back to work on the 32x32 RGB images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert RGB images x to grayscale using the formula for Y_linear in https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale\n",
    "def grayscale(x):\n",
    "    x = x.astype('float32')/255\n",
    "    x = np.piecewise(x, [x <= 0.04045, x > 0.04045], \n",
    "                        [lambda x: x/12.92, lambda x: ((x + .055)/1.055)**2.4])\n",
    "    return .2126 * x[:,:,0,:] + .7152 * x[:,:,1,:]  + .07152 * x[:,:,2,:]\n",
    "\n",
    "def downsample(x):\n",
    "    return sum([x[i::2,j::2,:] for i in range(2) for j in range(2)])/4\n",
    "\n",
    "def preprocess(data):\n",
    "    gray = grayscale(data['X'])\n",
    "    downsampled = downsample(gray)\n",
    "    return (downsampled.reshape(16*16, gray.shape[2]).transpose(),\n",
    "            data['y'].flatten() - 1)\n",
    "\n",
    "\n",
    "data_train = scipy.io.loadmat('data/train_32x32.mat')\n",
    "data_test = scipy.io.loadmat('data/test_32x32.mat')\n",
    "\n",
    "x_train_all, y_train_all = preprocess(data_train)\n",
    "x_test_all, y_test_all = preprocess(data_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a subset of classes\n",
    "\n",
    "We furter reduce the size of the dataset (and thus reduce computation time) by selecting only the 5 (out of 10 digits) in subset_of_classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_classes(x, y, classes):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for c in classes:\n",
    "        tmp = np.where(y == c)[0]\n",
    "        indices.extend(tmp)\n",
    "        labels.extend(np.ones(len(tmp), dtype='uint8') * count)\n",
    "        count += 1\n",
    "    return x[indices], labels\n",
    "\n",
    "x_train, y_train = extract_classes(x_train_all, y_train_all, subset_of_classes)\n",
    "x_test, y_test = extract_classes(x_test_all, y_test_all, subset_of_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some examples now. The green digit at the bottom left of each image indicates the corresponding label in y_test.\n",
    "For further usage of the function plot_some_samples, please have a look at its definition in the plotting section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_some_samples(x_test, y_test, label_mapping = subset_of_classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To prepare for fitting we transform the labels to one hot coding, i.e. for 5 classes, label 2 becomes the vector [0, 0, 1, 0, 0] (python uses 0-indexing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)\n",
    "y_test = keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: No hidden layer\n",
    "\n",
    "### Description\n",
    "\n",
    "Define and fit a model without a hidden layer. \n",
    "\n",
    "1. Use the softmax activation for the output layer.\n",
    "2. Use the categorical_crossentropy loss.\n",
    "3. Add the accuracy metric to the metrics.\n",
    "4. Choose stochastic gradient descent for the optimizer.\n",
    "5. Choose a minibatch size of 128.\n",
    "6. Fit for as many epochs as needed to see no further decrease in the validation loss.\n",
    "7. Plot the output of the fitting procedure (a history object) using the function plot_history defined above.\n",
    "8. Determine the indices of all test images that are misclassified by the fitted model and plot some of them using the function \n",
    "   `plot_some_samples(x_test, y_test, yhat_test, error_indices, label_mapping = subset_of_classes)`\n",
    "\n",
    "\n",
    "Hints:\n",
    "* Read the keras docs, in particular [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/).\n",
    "* Have a look at the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_mlp](https://github.com/keras-team/keras/blob/master/examples/mnist_mlp.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30246 samples, validate on 7562 samples\n",
      "Epoch 1/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 1.6134 - acc: 0.2293 - val_loss: 1.5770 - val_acc: 0.1010\n",
      "Epoch 2/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.5897 - acc: 0.2643 - val_loss: 1.5685 - val_acc: 0.0803\n",
      "Epoch 3/1000\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 1.5822 - acc: 0.2786 - val_loss: 1.5574 - val_acc: 0.1037\n",
      "Epoch 4/1000\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 1.5751 - acc: 0.2925 - val_loss: 1.5577 - val_acc: 0.1083\n",
      "Epoch 5/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5682 - acc: 0.3038 - val_loss: 1.5421 - val_acc: 0.1139\n",
      "Epoch 6/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.5611 - acc: 0.3099 - val_loss: 1.5116 - val_acc: 0.1953\n",
      "Epoch 7/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5539 - acc: 0.3208 - val_loss: 1.5150 - val_acc: 0.1750\n",
      "Epoch 8/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5464 - acc: 0.3282 - val_loss: 1.5225 - val_acc: 0.1689\n",
      "Epoch 9/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.5384 - acc: 0.3321 - val_loss: 1.4857 - val_acc: 0.2436\n",
      "Epoch 10/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.5301 - acc: 0.3423 - val_loss: 1.4969 - val_acc: 0.2142\n",
      "Epoch 11/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 1.5210 - acc: 0.3465 - val_loss: 1.4716 - val_acc: 0.2655\n",
      "Epoch 12/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 1.5113 - acc: 0.3566 - val_loss: 1.5135 - val_acc: 0.1952\n",
      "Epoch 13/1000\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 1.5007 - acc: 0.3656 - val_loss: 1.4823 - val_acc: 0.2367\n",
      "Epoch 14/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.4892 - acc: 0.3754 - val_loss: 1.4441 - val_acc: 0.3008\n",
      "Epoch 15/1000\n",
      "30246/30246 [==============================] - 2s 50us/step - loss: 1.4772 - acc: 0.3872 - val_loss: 1.4239 - val_acc: 0.3441\n",
      "Epoch 16/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.4645 - acc: 0.3985 - val_loss: 1.3882 - val_acc: 0.4185\n",
      "Epoch 17/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 1.4517 - acc: 0.4117 - val_loss: 1.4075 - val_acc: 0.3734\n",
      "Epoch 18/1000\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 1.4382 - acc: 0.4223 - val_loss: 1.3568 - val_acc: 0.4587\n",
      "Epoch 19/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 1.4246 - acc: 0.4363 - val_loss: 1.3736 - val_acc: 0.4398\n",
      "Epoch 20/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.4107 - acc: 0.4494 - val_loss: 1.3300 - val_acc: 0.5078\n",
      "Epoch 21/1000\n",
      "30246/30246 [==============================] - 1s 30us/step - loss: 1.3966 - acc: 0.4630 - val_loss: 1.3153 - val_acc: 0.5323\n",
      "Epoch 22/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3824 - acc: 0.4761 - val_loss: 1.3192 - val_acc: 0.5161\n",
      "Epoch 23/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3681 - acc: 0.4869 - val_loss: 1.2982 - val_acc: 0.5741\n",
      "Epoch 24/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3540 - acc: 0.5001 - val_loss: 1.2393 - val_acc: 0.6485\n",
      "Epoch 25/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3397 - acc: 0.5122 - val_loss: 1.2449 - val_acc: 0.6296\n",
      "Epoch 26/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3256 - acc: 0.5224 - val_loss: 1.2332 - val_acc: 0.6300\n",
      "Epoch 27/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.3115 - acc: 0.5325 - val_loss: 1.2064 - val_acc: 0.6817\n",
      "Epoch 28/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2974 - acc: 0.5433 - val_loss: 1.1980 - val_acc: 0.6595\n",
      "Epoch 29/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2835 - acc: 0.5513 - val_loss: 1.1936 - val_acc: 0.6658\n",
      "Epoch 30/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2697 - acc: 0.5586 - val_loss: 1.1738 - val_acc: 0.6828\n",
      "Epoch 31/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2561 - acc: 0.5669 - val_loss: 1.1827 - val_acc: 0.6587\n",
      "Epoch 32/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2426 - acc: 0.5733 - val_loss: 1.1266 - val_acc: 0.7067\n",
      "Epoch 33/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2291 - acc: 0.5807 - val_loss: 1.1100 - val_acc: 0.7261\n",
      "Epoch 34/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2158 - acc: 0.5868 - val_loss: 1.1148 - val_acc: 0.7072\n",
      "Epoch 35/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.2027 - acc: 0.5939 - val_loss: 1.0712 - val_acc: 0.7312\n",
      "Epoch 36/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.1897 - acc: 0.5993 - val_loss: 1.0762 - val_acc: 0.7294\n",
      "Epoch 37/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.1769 - acc: 0.6060 - val_loss: 1.0620 - val_acc: 0.7390\n",
      "Epoch 38/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.1642 - acc: 0.6118 - val_loss: 1.0081 - val_acc: 0.7724\n",
      "Epoch 39/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.1519 - acc: 0.6172 - val_loss: 1.0261 - val_acc: 0.7379\n",
      "Epoch 40/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 1.1397 - acc: 0.6216 - val_loss: 1.0500 - val_acc: 0.7331\n",
      "Epoch 41/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.1274 - acc: 0.6276 - val_loss: 1.0292 - val_acc: 0.7456\n",
      "Epoch 42/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 1.1154 - acc: 0.6327 - val_loss: 1.0106 - val_acc: 0.7421\n",
      "Epoch 43/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 1.1037 - acc: 0.6364 - val_loss: 1.0147 - val_acc: 0.7378\n",
      "Epoch 44/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.0923 - acc: 0.6421 - val_loss: 0.9776 - val_acc: 0.7589\n",
      "Epoch 45/1000\n",
      "30246/30246 [==============================] - ETA: 0s - loss: 1.0814 - acc: 0.647 - 1s 17us/step - loss: 1.0809 - acc: 0.6473 - val_loss: 0.9915 - val_acc: 0.7575\n",
      "Epoch 46/1000\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 1.0701 - acc: 0.6522 - val_loss: 0.9581 - val_acc: 0.7468\n",
      "Epoch 47/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 1.0590 - acc: 0.6568 - val_loss: 0.9115 - val_acc: 0.7776\n",
      "Epoch 48/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 1.0485 - acc: 0.6622 - val_loss: 0.9250 - val_acc: 0.7765\n",
      "Epoch 49/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.0381 - acc: 0.6657 - val_loss: 0.9589 - val_acc: 0.7617\n",
      "Epoch 50/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 1.0276 - acc: 0.6708 - val_loss: 0.9081 - val_acc: 0.7769\n",
      "Epoch 51/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.0177 - acc: 0.6753 - val_loss: 0.8743 - val_acc: 0.7965\n",
      "Epoch 52/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 1.0081 - acc: 0.6798 - val_loss: 0.8642 - val_acc: 0.7949\n",
      "Epoch 53/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.9985 - acc: 0.6830 - val_loss: 0.9174 - val_acc: 0.7633\n",
      "Epoch 54/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9891 - acc: 0.6861 - val_loss: 0.8996 - val_acc: 0.7757\n",
      "Epoch 55/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9804 - acc: 0.6914 - val_loss: 0.9087 - val_acc: 0.7677\n",
      "Epoch 56/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9715 - acc: 0.6935 - val_loss: 0.8825 - val_acc: 0.7837\n",
      "Epoch 57/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.9630 - acc: 0.6979 - val_loss: 0.8711 - val_acc: 0.7830\n",
      "Epoch 58/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9546 - acc: 0.7015 - val_loss: 0.8913 - val_acc: 0.7806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9465 - acc: 0.7049 - val_loss: 0.8400 - val_acc: 0.7982\n",
      "Epoch 60/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9384 - acc: 0.7086 - val_loss: 0.8269 - val_acc: 0.8102\n",
      "Epoch 61/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.9306 - acc: 0.7123 - val_loss: 0.8461 - val_acc: 0.7936\n",
      "Epoch 62/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9232 - acc: 0.7147 - val_loss: 0.8632 - val_acc: 0.7862\n",
      "Epoch 63/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9156 - acc: 0.7182 - val_loss: 0.8318 - val_acc: 0.7958\n",
      "Epoch 64/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9089 - acc: 0.7198 - val_loss: 0.8596 - val_acc: 0.7854\n",
      "Epoch 65/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.9022 - acc: 0.7241 - val_loss: 0.8284 - val_acc: 0.7974\n",
      "Epoch 66/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8956 - acc: 0.7248 - val_loss: 0.7913 - val_acc: 0.8116\n",
      "Epoch 67/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8891 - acc: 0.7277 - val_loss: 0.7550 - val_acc: 0.8276\n",
      "Epoch 68/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8830 - acc: 0.7293 - val_loss: 0.7889 - val_acc: 0.8106\n",
      "Epoch 69/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.8772 - acc: 0.7327 - val_loss: 0.8150 - val_acc: 0.7977\n",
      "Epoch 70/1000\n",
      "30246/30246 [==============================] - 2s 58us/step - loss: 0.8714 - acc: 0.7338 - val_loss: 0.7698 - val_acc: 0.8159\n",
      "Epoch 71/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8658 - acc: 0.7358 - val_loss: 0.7668 - val_acc: 0.8179\n",
      "Epoch 72/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.8605 - acc: 0.7385 - val_loss: 0.7301 - val_acc: 0.8322\n",
      "Epoch 73/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8552 - acc: 0.7411 - val_loss: 0.7693 - val_acc: 0.8165\n",
      "Epoch 74/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.8502 - acc: 0.7424 - val_loss: 0.7893 - val_acc: 0.8015\n",
      "Epoch 75/1000\n",
      "30246/30246 [==============================] - 2s 50us/step - loss: 0.8455 - acc: 0.7447 - val_loss: 0.7428 - val_acc: 0.8254\n",
      "Epoch 76/1000\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.8408 - acc: 0.7453 - val_loss: 0.7814 - val_acc: 0.8061\n",
      "Epoch 77/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.8361 - acc: 0.7471 - val_loss: 0.7431 - val_acc: 0.8184\n",
      "Epoch 78/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.8318 - acc: 0.7485 - val_loss: 0.7470 - val_acc: 0.8203\n",
      "Epoch 79/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8272 - acc: 0.7507 - val_loss: 0.7207 - val_acc: 0.8302\n",
      "Epoch 80/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.8232 - acc: 0.7516 - val_loss: 0.7472 - val_acc: 0.8166\n",
      "Epoch 81/1000\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.8190 - acc: 0.7529 - val_loss: 0.7739 - val_acc: 0.8076\n",
      "Epoch 82/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.8153 - acc: 0.7549 - val_loss: 0.7351 - val_acc: 0.8199\n",
      "Epoch 83/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8112 - acc: 0.7558 - val_loss: 0.7730 - val_acc: 0.8045\n",
      "Epoch 84/1000\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.8079 - acc: 0.7572 - val_loss: 0.7098 - val_acc: 0.8309\n",
      "Epoch 85/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8040 - acc: 0.7580 - val_loss: 0.7636 - val_acc: 0.8109\n",
      "Epoch 86/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.8004 - acc: 0.7602 - val_loss: 0.7825 - val_acc: 0.7977\n",
      "Epoch 87/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7970 - acc: 0.7623 - val_loss: 0.7408 - val_acc: 0.8153\n",
      "Epoch 88/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7935 - acc: 0.7617 - val_loss: 0.7625 - val_acc: 0.8080\n",
      "Epoch 89/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7905 - acc: 0.7626 - val_loss: 0.7241 - val_acc: 0.8232\n",
      "Epoch 90/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7874 - acc: 0.7645 - val_loss: 0.7299 - val_acc: 0.8184\n",
      "Epoch 91/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.7845 - acc: 0.7650 - val_loss: 0.7567 - val_acc: 0.8056\n",
      "Epoch 92/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.7816 - acc: 0.7664 - val_loss: 0.6844 - val_acc: 0.8355\n",
      "Epoch 93/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7786 - acc: 0.7671 - val_loss: 0.7019 - val_acc: 0.8318\n",
      "Epoch 94/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7755 - acc: 0.7675 - val_loss: 0.7527 - val_acc: 0.8102\n",
      "Epoch 95/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7727 - acc: 0.7688 - val_loss: 0.7235 - val_acc: 0.8198\n",
      "Epoch 96/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7702 - acc: 0.7693 - val_loss: 0.7694 - val_acc: 0.8059\n",
      "Epoch 97/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7679 - acc: 0.7709 - val_loss: 0.6970 - val_acc: 0.8310\n",
      "Epoch 98/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7650 - acc: 0.7715 - val_loss: 0.6888 - val_acc: 0.8325\n",
      "Epoch 99/1000\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.7626 - acc: 0.7715 - val_loss: 0.6943 - val_acc: 0.8297\n",
      "Epoch 100/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7600 - acc: 0.7724 - val_loss: 0.6756 - val_acc: 0.8372\n",
      "Epoch 101/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7577 - acc: 0.7735 - val_loss: 0.7068 - val_acc: 0.8253\n",
      "Epoch 102/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.7551 - acc: 0.7739 - val_loss: 0.6916 - val_acc: 0.8302\n",
      "Epoch 103/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7530 - acc: 0.7743 - val_loss: 0.7079 - val_acc: 0.8236\n",
      "Epoch 104/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7505 - acc: 0.7765 - val_loss: 0.7426 - val_acc: 0.8047\n",
      "Epoch 105/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7484 - acc: 0.7757 - val_loss: 0.6419 - val_acc: 0.8486\n",
      "Epoch 106/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7467 - acc: 0.7766 - val_loss: 0.7164 - val_acc: 0.8166\n",
      "Epoch 107/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.7444 - acc: 0.7776 - val_loss: 0.7186 - val_acc: 0.8190\n",
      "Epoch 108/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.7422 - acc: 0.7778 - val_loss: 0.6949 - val_acc: 0.8256\n",
      "Epoch 109/1000\n",
      "30246/30246 [==============================] - 2s 50us/step - loss: 0.7403 - acc: 0.7785 - val_loss: 0.7298 - val_acc: 0.8089\n",
      "Epoch 110/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.7385 - acc: 0.7796 - val_loss: 0.6741 - val_acc: 0.8342\n",
      "Epoch 111/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.7366 - acc: 0.7795 - val_loss: 0.7256 - val_acc: 0.8129\n",
      "Epoch 112/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.7344 - acc: 0.7820 - val_loss: 0.6916 - val_acc: 0.8262\n",
      "Epoch 113/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.7324 - acc: 0.7810 - val_loss: 0.6761 - val_acc: 0.8295\n",
      "Epoch 114/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7309 - acc: 0.7813 - val_loss: 0.6887 - val_acc: 0.8277\n",
      "Epoch 115/1000\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.7289 - acc: 0.7827 - val_loss: 0.6871 - val_acc: 0.8268\n",
      "Epoch 116/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.7275 - acc: 0.7824 - val_loss: 0.6534 - val_acc: 0.8407\n",
      "Epoch 117/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7258 - acc: 0.7831 - val_loss: 0.7060 - val_acc: 0.8211\n",
      "Epoch 118/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7240 - acc: 0.7836 - val_loss: 0.6730 - val_acc: 0.8335\n",
      "Epoch 119/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7224 - acc: 0.7840 - val_loss: 0.7290 - val_acc: 0.8094\n",
      "Epoch 120/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.7205 - acc: 0.7847 - val_loss: 0.6340 - val_acc: 0.8471\n",
      "Epoch 121/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7187 - acc: 0.7852 - val_loss: 0.6799 - val_acc: 0.8265\n",
      "Epoch 122/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7172 - acc: 0.7863 - val_loss: 0.6976 - val_acc: 0.8223\n",
      "Epoch 123/1000\n",
      "30246/30246 [==============================] - 2s 53us/step - loss: 0.7158 - acc: 0.7863 - val_loss: 0.7010 - val_acc: 0.8213\n",
      "Epoch 124/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.7142 - acc: 0.7870 - val_loss: 0.6682 - val_acc: 0.8336\n",
      "Epoch 125/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.7128 - acc: 0.7881 - val_loss: 0.6621 - val_acc: 0.8355\n",
      "Epoch 126/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.7114 - acc: 0.7884 - val_loss: 0.6503 - val_acc: 0.8396\n",
      "Epoch 127/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7097 - acc: 0.7887 - val_loss: 0.6493 - val_acc: 0.8412\n",
      "Epoch 128/1000\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.7083 - acc: 0.7885 - val_loss: 0.6629 - val_acc: 0.8350\n",
      "Epoch 129/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.7067 - acc: 0.7891 - val_loss: 0.6670 - val_acc: 0.8310\n",
      "Epoch 130/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.7053 - acc: 0.7898 - val_loss: 0.6786 - val_acc: 0.8285\n",
      "Epoch 131/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.7041 - acc: 0.7903 - val_loss: 0.6322 - val_acc: 0.8449\n",
      "Epoch 132/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.7029 - acc: 0.7907 - val_loss: 0.6754 - val_acc: 0.8293\n",
      "Epoch 133/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7010 - acc: 0.7916 - val_loss: 0.6631 - val_acc: 0.8319\n",
      "Epoch 134/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.7003 - acc: 0.7907 - val_loss: 0.6792 - val_acc: 0.8253\n",
      "Epoch 135/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6988 - acc: 0.7916 - val_loss: 0.6422 - val_acc: 0.8401\n",
      "Epoch 136/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6975 - acc: 0.7918 - val_loss: 0.6302 - val_acc: 0.8440\n",
      "Epoch 137/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6963 - acc: 0.7924 - val_loss: 0.6544 - val_acc: 0.8363\n",
      "Epoch 138/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6950 - acc: 0.7927 - val_loss: 0.6485 - val_acc: 0.8372\n",
      "Epoch 139/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.6940 - acc: 0.7934 - val_loss: 0.6541 - val_acc: 0.8355\n",
      "Epoch 140/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6926 - acc: 0.7938 - val_loss: 0.6427 - val_acc: 0.8393\n",
      "Epoch 141/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6914 - acc: 0.7938 - val_loss: 0.6801 - val_acc: 0.8229\n",
      "Epoch 142/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6901 - acc: 0.7940 - val_loss: 0.6322 - val_acc: 0.8425\n",
      "Epoch 143/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6888 - acc: 0.7934 - val_loss: 0.6357 - val_acc: 0.8417\n",
      "Epoch 144/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.6879 - acc: 0.7944 - val_loss: 0.6607 - val_acc: 0.8306\n",
      "Epoch 145/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6868 - acc: 0.7942 - val_loss: 0.6763 - val_acc: 0.8264\n",
      "Epoch 146/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6856 - acc: 0.7960 - val_loss: 0.6470 - val_acc: 0.8358\n",
      "Epoch 147/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6846 - acc: 0.7959 - val_loss: 0.6620 - val_acc: 0.8314\n",
      "Epoch 148/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6834 - acc: 0.7960 - val_loss: 0.6125 - val_acc: 0.8475\n",
      "Epoch 149/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6822 - acc: 0.7957 - val_loss: 0.6590 - val_acc: 0.8285\n",
      "Epoch 150/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6815 - acc: 0.7964 - val_loss: 0.6487 - val_acc: 0.8351\n",
      "Epoch 151/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.6801 - acc: 0.7964 - val_loss: 0.6730 - val_acc: 0.8240\n",
      "Epoch 152/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6791 - acc: 0.7964 - val_loss: 0.6104 - val_acc: 0.8495\n",
      "Epoch 153/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6782 - acc: 0.7980 - val_loss: 0.6423 - val_acc: 0.8381\n",
      "Epoch 154/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6771 - acc: 0.7986 - val_loss: 0.6319 - val_acc: 0.8424\n",
      "Epoch 155/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6759 - acc: 0.7985 - val_loss: 0.6611 - val_acc: 0.8294\n",
      "Epoch 156/1000\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.6749 - acc: 0.7981 - val_loss: 0.6817 - val_acc: 0.8199\n",
      "Epoch 157/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6736 - acc: 0.7980 - val_loss: 0.5720 - val_acc: 0.8647\n",
      "Epoch 158/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.6728 - acc: 0.8002 - val_loss: 0.6658 - val_acc: 0.8266\n",
      "Epoch 159/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6721 - acc: 0.7997 - val_loss: 0.6387 - val_acc: 0.8389\n",
      "Epoch 160/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6711 - acc: 0.7992 - val_loss: 0.6215 - val_acc: 0.8458\n",
      "Epoch 161/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.6700 - acc: 0.8000 - val_loss: 0.6423 - val_acc: 0.8369\n",
      "Epoch 162/1000\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.6693 - acc: 0.8009 - val_loss: 0.6174 - val_acc: 0.8471\n",
      "Epoch 163/1000\n",
      "30246/30246 [==============================] - 1s 38us/step - loss: 0.6682 - acc: 0.8010 - val_loss: 0.6392 - val_acc: 0.8375\n",
      "Epoch 164/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6672 - acc: 0.8014 - val_loss: 0.6263 - val_acc: 0.8426\n",
      "Epoch 165/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6661 - acc: 0.8014 - val_loss: 0.6244 - val_acc: 0.8440\n",
      "Epoch 166/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6651 - acc: 0.8013 - val_loss: 0.6287 - val_acc: 0.8400\n",
      "Epoch 167/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.6644 - acc: 0.8028 - val_loss: 0.6096 - val_acc: 0.8485\n",
      "Epoch 168/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.6632 - acc: 0.8026 - val_loss: 0.6095 - val_acc: 0.8514\n",
      "Epoch 169/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.6623 - acc: 0.8018 - val_loss: 0.6297 - val_acc: 0.8404\n",
      "Epoch 170/1000\n",
      "30246/30246 [==============================] - 2s 66us/step - loss: 0.6613 - acc: 0.8037 - val_loss: 0.6814 - val_acc: 0.8178\n",
      "Epoch 171/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6605 - acc: 0.8042 - val_loss: 0.6404 - val_acc: 0.8379\n",
      "Epoch 172/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6596 - acc: 0.8044 - val_loss: 0.6415 - val_acc: 0.8336\n",
      "Epoch 173/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6590 - acc: 0.8034 - val_loss: 0.6281 - val_acc: 0.8449\n",
      "Epoch 174/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6575 - acc: 0.8045 - val_loss: 0.6033 - val_acc: 0.8496\n",
      "Epoch 175/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6568 - acc: 0.8052 - val_loss: 0.6246 - val_acc: 0.8414\n",
      "Epoch 176/1000\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.6558 - acc: 0.8049 - val_loss: 0.6489 - val_acc: 0.8343\n",
      "Epoch 177/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6550 - acc: 0.8052 - val_loss: 0.6217 - val_acc: 0.8432\n",
      "Epoch 178/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6539 - acc: 0.8062 - val_loss: 0.6202 - val_acc: 0.8441\n",
      "Epoch 179/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6532 - acc: 0.8064 - val_loss: 0.6618 - val_acc: 0.8256\n",
      "Epoch 180/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6520 - acc: 0.8065 - val_loss: 0.6157 - val_acc: 0.8453\n",
      "Epoch 181/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6512 - acc: 0.8069 - val_loss: 0.6563 - val_acc: 0.8290\n",
      "Epoch 182/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6503 - acc: 0.8070 - val_loss: 0.6382 - val_acc: 0.8354\n",
      "Epoch 183/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6494 - acc: 0.8068 - val_loss: 0.6489 - val_acc: 0.8303\n",
      "Epoch 184/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6486 - acc: 0.8084 - val_loss: 0.6484 - val_acc: 0.8313\n",
      "Epoch 185/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6477 - acc: 0.8081 - val_loss: 0.6091 - val_acc: 0.8463\n",
      "Epoch 186/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6470 - acc: 0.8074 - val_loss: 0.6377 - val_acc: 0.8362\n",
      "Epoch 187/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6458 - acc: 0.8092 - val_loss: 0.5945 - val_acc: 0.8527\n",
      "Epoch 188/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6451 - acc: 0.8092 - val_loss: 0.6261 - val_acc: 0.8408\n",
      "Epoch 189/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6444 - acc: 0.8091 - val_loss: 0.5888 - val_acc: 0.8540\n",
      "Epoch 190/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6432 - acc: 0.8103 - val_loss: 0.5999 - val_acc: 0.8506\n",
      "Epoch 191/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6421 - acc: 0.8099 - val_loss: 0.6375 - val_acc: 0.8359\n",
      "Epoch 192/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6413 - acc: 0.8094 - val_loss: 0.6261 - val_acc: 0.8409\n",
      "Epoch 193/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6406 - acc: 0.8100 - val_loss: 0.6485 - val_acc: 0.8307\n",
      "Epoch 194/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6400 - acc: 0.8111 - val_loss: 0.6563 - val_acc: 0.8282\n",
      "Epoch 195/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6391 - acc: 0.8104 - val_loss: 0.6264 - val_acc: 0.8376\n",
      "Epoch 196/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6384 - acc: 0.8118 - val_loss: 0.5964 - val_acc: 0.8504\n",
      "Epoch 197/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6373 - acc: 0.8117 - val_loss: 0.5955 - val_acc: 0.8503\n",
      "Epoch 198/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6363 - acc: 0.8117 - val_loss: 0.6280 - val_acc: 0.8410\n",
      "Epoch 199/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6354 - acc: 0.8118 - val_loss: 0.6526 - val_acc: 0.8295\n",
      "Epoch 200/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6347 - acc: 0.8126 - val_loss: 0.5911 - val_acc: 0.8526\n",
      "Epoch 201/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6338 - acc: 0.8124 - val_loss: 0.6016 - val_acc: 0.8496\n",
      "Epoch 202/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6330 - acc: 0.8130 - val_loss: 0.6195 - val_acc: 0.8421\n",
      "Epoch 203/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6318 - acc: 0.8130 - val_loss: 0.5814 - val_acc: 0.8608\n",
      "Epoch 204/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6313 - acc: 0.8131 - val_loss: 0.6412 - val_acc: 0.8318\n",
      "Epoch 205/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6305 - acc: 0.8143 - val_loss: 0.6620 - val_acc: 0.8266\n",
      "Epoch 206/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6295 - acc: 0.8135 - val_loss: 0.6220 - val_acc: 0.8391\n",
      "Epoch 207/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6288 - acc: 0.8136 - val_loss: 0.6144 - val_acc: 0.8433\n",
      "Epoch 208/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6276 - acc: 0.8149 - val_loss: 0.6622 - val_acc: 0.8261\n",
      "Epoch 209/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6268 - acc: 0.8150 - val_loss: 0.6030 - val_acc: 0.8467\n",
      "Epoch 210/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6264 - acc: 0.8146 - val_loss: 0.6209 - val_acc: 0.8418\n",
      "Epoch 211/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6248 - acc: 0.8155 - val_loss: 0.5975 - val_acc: 0.8502\n",
      "Epoch 212/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6245 - acc: 0.8160 - val_loss: 0.6513 - val_acc: 0.8306\n",
      "Epoch 213/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6237 - acc: 0.8162 - val_loss: 0.6423 - val_acc: 0.8348\n",
      "Epoch 214/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6229 - acc: 0.8157 - val_loss: 0.5942 - val_acc: 0.8507\n",
      "Epoch 215/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.6219 - acc: 0.8167 - val_loss: 0.5864 - val_acc: 0.8549\n",
      "Epoch 216/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.6209 - acc: 0.8168 - val_loss: 0.5607 - val_acc: 0.8651\n",
      "Epoch 217/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6202 - acc: 0.8169 - val_loss: 0.5985 - val_acc: 0.8504\n",
      "Epoch 218/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6195 - acc: 0.8168 - val_loss: 0.6130 - val_acc: 0.8458\n",
      "Epoch 219/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6186 - acc: 0.8167 - val_loss: 0.6133 - val_acc: 0.8440\n",
      "Epoch 220/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6176 - acc: 0.8172 - val_loss: 0.6191 - val_acc: 0.8422\n",
      "Epoch 221/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6170 - acc: 0.8178 - val_loss: 0.5613 - val_acc: 0.8618\n",
      "Epoch 222/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6162 - acc: 0.8177 - val_loss: 0.6199 - val_acc: 0.8430\n",
      "Epoch 223/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6154 - acc: 0.8179 - val_loss: 0.6005 - val_acc: 0.8478\n",
      "Epoch 224/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6144 - acc: 0.8184 - val_loss: 0.5503 - val_acc: 0.8670\n",
      "Epoch 225/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.6138 - acc: 0.8188 - val_loss: 0.5479 - val_acc: 0.8659\n",
      "Epoch 226/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6128 - acc: 0.8205 - val_loss: 0.6406 - val_acc: 0.8344\n",
      "Epoch 227/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6123 - acc: 0.8193 - val_loss: 0.6129 - val_acc: 0.8448\n",
      "Epoch 228/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6110 - acc: 0.8193 - val_loss: 0.5738 - val_acc: 0.8560\n",
      "Epoch 229/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6103 - acc: 0.8208 - val_loss: 0.5777 - val_acc: 0.8577\n",
      "Epoch 230/1000\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.6096 - acc: 0.8205 - val_loss: 0.5900 - val_acc: 0.8523\n",
      "Epoch 231/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6088 - acc: 0.8216 - val_loss: 0.5663 - val_acc: 0.8585\n",
      "Epoch 232/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6080 - acc: 0.8200 - val_loss: 0.6090 - val_acc: 0.8446\n",
      "Epoch 233/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.6073 - acc: 0.8209 - val_loss: 0.5864 - val_acc: 0.8544\n",
      "Epoch 234/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6063 - acc: 0.8213 - val_loss: 0.6203 - val_acc: 0.8422\n",
      "Epoch 235/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6056 - acc: 0.8212 - val_loss: 0.6108 - val_acc: 0.8440\n",
      "Epoch 236/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6048 - acc: 0.8215 - val_loss: 0.6103 - val_acc: 0.8445\n",
      "Epoch 237/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6040 - acc: 0.8223 - val_loss: 0.5740 - val_acc: 0.8580\n",
      "Epoch 238/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6029 - acc: 0.8226 - val_loss: 0.6122 - val_acc: 0.8463\n",
      "Epoch 239/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6026 - acc: 0.8223 - val_loss: 0.5908 - val_acc: 0.8516\n",
      "Epoch 240/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6010 - acc: 0.8230 - val_loss: 0.6015 - val_acc: 0.8483\n",
      "Epoch 241/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.6008 - acc: 0.8231 - val_loss: 0.5706 - val_acc: 0.8565\n",
      "Epoch 242/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.6000 - acc: 0.8234 - val_loss: 0.6120 - val_acc: 0.8459\n",
      "Epoch 243/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5992 - acc: 0.8231 - val_loss: 0.5637 - val_acc: 0.8602\n",
      "Epoch 244/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5981 - acc: 0.8240 - val_loss: 0.6145 - val_acc: 0.8449\n",
      "Epoch 245/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5975 - acc: 0.8238 - val_loss: 0.5875 - val_acc: 0.8529\n",
      "Epoch 246/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5964 - acc: 0.8240 - val_loss: 0.6008 - val_acc: 0.8482\n",
      "Epoch 247/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5958 - acc: 0.8249 - val_loss: 0.5959 - val_acc: 0.8520\n",
      "Epoch 248/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5951 - acc: 0.8252 - val_loss: 0.5918 - val_acc: 0.8544\n",
      "Epoch 249/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5941 - acc: 0.8254 - val_loss: 0.5721 - val_acc: 0.8602\n",
      "Epoch 250/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5930 - acc: 0.8255 - val_loss: 0.6178 - val_acc: 0.8446\n",
      "Epoch 251/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5924 - acc: 0.8254 - val_loss: 0.5949 - val_acc: 0.8526\n",
      "Epoch 252/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5916 - acc: 0.8259 - val_loss: 0.5530 - val_acc: 0.8667\n",
      "Epoch 253/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5905 - acc: 0.8267 - val_loss: 0.5765 - val_acc: 0.8567\n",
      "Epoch 254/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.5901 - acc: 0.8269 - val_loss: 0.5858 - val_acc: 0.8543\n",
      "Epoch 255/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5894 - acc: 0.8272 - val_loss: 0.5944 - val_acc: 0.8510\n",
      "Epoch 256/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5882 - acc: 0.8270 - val_loss: 0.5919 - val_acc: 0.8500\n",
      "Epoch 257/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5876 - acc: 0.8273 - val_loss: 0.5808 - val_acc: 0.8561\n",
      "Epoch 258/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5865 - acc: 0.8287 - val_loss: 0.5641 - val_acc: 0.8637\n",
      "Epoch 259/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5856 - acc: 0.8275 - val_loss: 0.5827 - val_acc: 0.8536\n",
      "Epoch 260/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5849 - acc: 0.8283 - val_loss: 0.5646 - val_acc: 0.8611\n",
      "Epoch 261/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5841 - acc: 0.8277 - val_loss: 0.5842 - val_acc: 0.8536\n",
      "Epoch 262/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5832 - acc: 0.8292 - val_loss: 0.5476 - val_acc: 0.8672\n",
      "Epoch 263/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5825 - acc: 0.8288 - val_loss: 0.5611 - val_acc: 0.8630\n",
      "Epoch 264/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5817 - acc: 0.8290 - val_loss: 0.5614 - val_acc: 0.8634\n",
      "Epoch 265/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5810 - acc: 0.8296 - val_loss: 0.5662 - val_acc: 0.8586\n",
      "Epoch 266/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.5802 - acc: 0.8301 - val_loss: 0.5428 - val_acc: 0.8675\n",
      "Epoch 267/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5792 - acc: 0.8298 - val_loss: 0.5374 - val_acc: 0.8727\n",
      "Epoch 268/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5783 - acc: 0.8307 - val_loss: 0.5507 - val_acc: 0.8663\n",
      "Epoch 269/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5778 - acc: 0.8304 - val_loss: 0.5631 - val_acc: 0.8601\n",
      "Epoch 270/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5771 - acc: 0.8311 - val_loss: 0.6179 - val_acc: 0.8422\n",
      "Epoch 271/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5762 - acc: 0.8315 - val_loss: 0.5323 - val_acc: 0.8699\n",
      "Epoch 272/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5754 - acc: 0.8307 - val_loss: 0.5803 - val_acc: 0.8553\n",
      "Epoch 273/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5745 - acc: 0.8310 - val_loss: 0.5902 - val_acc: 0.8512\n",
      "Epoch 274/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5740 - acc: 0.8323 - val_loss: 0.6119 - val_acc: 0.8459\n",
      "Epoch 275/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5733 - acc: 0.8325 - val_loss: 0.5247 - val_acc: 0.8752\n",
      "Epoch 276/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5725 - acc: 0.8320 - val_loss: 0.5775 - val_acc: 0.8545\n",
      "Epoch 277/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5717 - acc: 0.8321 - val_loss: 0.5528 - val_acc: 0.8637\n",
      "Epoch 278/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5709 - acc: 0.8332 - val_loss: 0.5780 - val_acc: 0.8555\n",
      "Epoch 279/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5701 - acc: 0.8335 - val_loss: 0.5248 - val_acc: 0.8732\n",
      "Epoch 280/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5693 - acc: 0.8329 - val_loss: 0.5645 - val_acc: 0.8606\n",
      "Epoch 281/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5686 - acc: 0.8329 - val_loss: 0.5336 - val_acc: 0.8717\n",
      "Epoch 282/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5680 - acc: 0.8345 - val_loss: 0.5879 - val_acc: 0.8532\n",
      "Epoch 283/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5668 - acc: 0.8348 - val_loss: 0.5419 - val_acc: 0.8686\n",
      "Epoch 284/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5663 - acc: 0.8339 - val_loss: 0.5475 - val_acc: 0.8664\n",
      "Epoch 285/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5654 - acc: 0.8347 - val_loss: 0.6066 - val_acc: 0.8478\n",
      "Epoch 286/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5648 - acc: 0.8342 - val_loss: 0.5948 - val_acc: 0.8490\n",
      "Epoch 287/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5639 - acc: 0.8354 - val_loss: 0.5368 - val_acc: 0.8690\n",
      "Epoch 288/1000\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.5636 - acc: 0.8354 - val_loss: 0.5798 - val_acc: 0.8551\n",
      "Epoch 289/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5623 - acc: 0.8354 - val_loss: 0.5543 - val_acc: 0.8631\n",
      "Epoch 290/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5618 - acc: 0.8358 - val_loss: 0.5849 - val_acc: 0.8539\n",
      "Epoch 291/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5611 - acc: 0.8360 - val_loss: 0.5654 - val_acc: 0.8581\n",
      "Epoch 292/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5605 - acc: 0.8365 - val_loss: 0.5358 - val_acc: 0.8701\n",
      "Epoch 293/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5592 - acc: 0.8366 - val_loss: 0.5709 - val_acc: 0.8570\n",
      "Epoch 294/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5586 - acc: 0.8370 - val_loss: 0.5665 - val_acc: 0.8582\n",
      "Epoch 295/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5581 - acc: 0.8374 - val_loss: 0.5771 - val_acc: 0.8549\n",
      "Epoch 296/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5574 - acc: 0.8370 - val_loss: 0.5192 - val_acc: 0.8765\n",
      "Epoch 297/1000\n",
      "30246/30246 [==============================] - 1s 43us/step - loss: 0.5563 - acc: 0.8374 - val_loss: 0.5833 - val_acc: 0.8508\n",
      "Epoch 298/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5558 - acc: 0.8381 - val_loss: 0.5502 - val_acc: 0.8633\n",
      "Epoch 299/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5552 - acc: 0.8380 - val_loss: 0.5630 - val_acc: 0.8564\n",
      "Epoch 300/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.5547 - acc: 0.8380 - val_loss: 0.5824 - val_acc: 0.8522\n",
      "Epoch 301/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5539 - acc: 0.8388 - val_loss: 0.5271 - val_acc: 0.8730\n",
      "Epoch 302/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5531 - acc: 0.8388 - val_loss: 0.5483 - val_acc: 0.8678\n",
      "Epoch 303/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5522 - acc: 0.8395 - val_loss: 0.5532 - val_acc: 0.8615\n",
      "Epoch 304/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5519 - acc: 0.8394 - val_loss: 0.5597 - val_acc: 0.8622\n",
      "Epoch 305/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5510 - acc: 0.8398 - val_loss: 0.5415 - val_acc: 0.8672\n",
      "Epoch 306/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5506 - acc: 0.8398 - val_loss: 0.5136 - val_acc: 0.8791\n",
      "Epoch 307/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5498 - acc: 0.8388 - val_loss: 0.5746 - val_acc: 0.8560\n",
      "Epoch 308/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5491 - acc: 0.8399 - val_loss: 0.5264 - val_acc: 0.8724\n",
      "Epoch 309/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5481 - acc: 0.8407 - val_loss: 0.5213 - val_acc: 0.8761\n",
      "Epoch 310/1000\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.5476 - acc: 0.8402 - val_loss: 0.5532 - val_acc: 0.8638\n",
      "Epoch 311/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5471 - acc: 0.8412 - val_loss: 0.5704 - val_acc: 0.8581\n",
      "Epoch 312/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5463 - acc: 0.8409 - val_loss: 0.5789 - val_acc: 0.8528\n",
      "Epoch 313/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5456 - acc: 0.8415 - val_loss: 0.5815 - val_acc: 0.8523\n",
      "Epoch 314/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5448 - acc: 0.8413 - val_loss: 0.5959 - val_acc: 0.8473\n",
      "Epoch 315/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5443 - acc: 0.8413 - val_loss: 0.5416 - val_acc: 0.8670\n",
      "Epoch 316/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5439 - acc: 0.8424 - val_loss: 0.5922 - val_acc: 0.8488\n",
      "Epoch 317/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5430 - acc: 0.8417 - val_loss: 0.5824 - val_acc: 0.8545\n",
      "Epoch 318/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5425 - acc: 0.8431 - val_loss: 0.4963 - val_acc: 0.8831\n",
      "Epoch 319/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5414 - acc: 0.8422 - val_loss: 0.5408 - val_acc: 0.8683\n",
      "Epoch 320/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5412 - acc: 0.8436 - val_loss: 0.5475 - val_acc: 0.8668\n",
      "Epoch 321/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5402 - acc: 0.8427 - val_loss: 0.4914 - val_acc: 0.8851\n",
      "Epoch 322/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5399 - acc: 0.8438 - val_loss: 0.5660 - val_acc: 0.8576\n",
      "Epoch 323/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5393 - acc: 0.8435 - val_loss: 0.5176 - val_acc: 0.8749\n",
      "Epoch 324/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5386 - acc: 0.8432 - val_loss: 0.5155 - val_acc: 0.8783\n",
      "Epoch 325/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5375 - acc: 0.8439 - val_loss: 0.4692 - val_acc: 0.8922\n",
      "Epoch 326/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5371 - acc: 0.8441 - val_loss: 0.5409 - val_acc: 0.8683\n",
      "Epoch 327/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5368 - acc: 0.8438 - val_loss: 0.5214 - val_acc: 0.8734\n",
      "Epoch 328/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5360 - acc: 0.8444 - val_loss: 0.5809 - val_acc: 0.8498\n",
      "Epoch 329/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5353 - acc: 0.8450 - val_loss: 0.5626 - val_acc: 0.8557\n",
      "Epoch 330/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5351 - acc: 0.8451 - val_loss: 0.5294 - val_acc: 0.8725\n",
      "Epoch 331/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.5342 - acc: 0.8453 - val_loss: 0.5250 - val_acc: 0.8721\n",
      "Epoch 332/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5338 - acc: 0.8450 - val_loss: 0.5325 - val_acc: 0.8711\n",
      "Epoch 333/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5327 - acc: 0.8445 - val_loss: 0.5448 - val_acc: 0.8666\n",
      "Epoch 334/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5319 - acc: 0.8459 - val_loss: 0.5504 - val_acc: 0.8656\n",
      "Epoch 335/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5315 - acc: 0.8459 - val_loss: 0.4934 - val_acc: 0.8831\n",
      "Epoch 336/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5310 - acc: 0.8468 - val_loss: 0.4997 - val_acc: 0.8807\n",
      "Epoch 337/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.5304 - acc: 0.8461 - val_loss: 0.5019 - val_acc: 0.8798\n",
      "Epoch 338/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.5297 - acc: 0.8461 - val_loss: 0.5227 - val_acc: 0.8736\n",
      "Epoch 339/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5287 - acc: 0.8475 - val_loss: 0.5452 - val_acc: 0.8678\n",
      "Epoch 340/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5286 - acc: 0.8469 - val_loss: 0.5905 - val_acc: 0.8504\n",
      "Epoch 341/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5275 - acc: 0.8470 - val_loss: 0.5697 - val_acc: 0.8590\n",
      "Epoch 342/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5270 - acc: 0.8476 - val_loss: 0.5612 - val_acc: 0.8576\n",
      "Epoch 343/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5263 - acc: 0.8478 - val_loss: 0.5471 - val_acc: 0.8662\n",
      "Epoch 344/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5256 - acc: 0.8473 - val_loss: 0.5290 - val_acc: 0.8711\n",
      "Epoch 345/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5254 - acc: 0.8479 - val_loss: 0.5532 - val_acc: 0.8638\n",
      "Epoch 346/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5248 - acc: 0.8479 - val_loss: 0.4931 - val_acc: 0.8855\n",
      "Epoch 347/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5238 - acc: 0.8473 - val_loss: 0.5767 - val_acc: 0.8543\n",
      "Epoch 348/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5232 - acc: 0.8478 - val_loss: 0.4992 - val_acc: 0.8836\n",
      "Epoch 349/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5229 - acc: 0.8479 - val_loss: 0.5020 - val_acc: 0.8791\n",
      "Epoch 350/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5221 - acc: 0.8482 - val_loss: 0.5096 - val_acc: 0.8787\n",
      "Epoch 351/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5217 - acc: 0.8485 - val_loss: 0.5165 - val_acc: 0.8764\n",
      "Epoch 352/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5207 - acc: 0.8492 - val_loss: 0.5192 - val_acc: 0.8716\n",
      "Epoch 353/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5203 - acc: 0.8498 - val_loss: 0.5310 - val_acc: 0.8732\n",
      "Epoch 354/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5195 - acc: 0.8500 - val_loss: 0.5741 - val_acc: 0.8584\n",
      "Epoch 355/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5189 - acc: 0.8494 - val_loss: 0.5784 - val_acc: 0.8559\n",
      "Epoch 356/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.5183 - acc: 0.8500 - val_loss: 0.5098 - val_acc: 0.8775\n",
      "Epoch 357/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5176 - acc: 0.8496 - val_loss: 0.5180 - val_acc: 0.8786\n",
      "Epoch 358/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5170 - acc: 0.8509 - val_loss: 0.5183 - val_acc: 0.8753\n",
      "Epoch 359/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5166 - acc: 0.8512 - val_loss: 0.5502 - val_acc: 0.8655\n",
      "Epoch 360/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5161 - acc: 0.8509 - val_loss: 0.5243 - val_acc: 0.8736\n",
      "Epoch 361/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5155 - acc: 0.8512 - val_loss: 0.5203 - val_acc: 0.8774\n",
      "Epoch 362/1000\n",
      "30246/30246 [==============================] - 1s 45us/step - loss: 0.5146 - acc: 0.8516 - val_loss: 0.5035 - val_acc: 0.8809\n",
      "Epoch 363/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5144 - acc: 0.8512 - val_loss: 0.5255 - val_acc: 0.8724\n",
      "Epoch 364/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.5133 - acc: 0.8504 - val_loss: 0.5095 - val_acc: 0.8799\n",
      "Epoch 365/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5131 - acc: 0.8527 - val_loss: 0.4686 - val_acc: 0.8914\n",
      "Epoch 366/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5124 - acc: 0.8527 - val_loss: 0.5610 - val_acc: 0.8608\n",
      "Epoch 367/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.5117 - acc: 0.8522 - val_loss: 0.5278 - val_acc: 0.8719\n",
      "Epoch 368/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5112 - acc: 0.8534 - val_loss: 0.4930 - val_acc: 0.8852\n",
      "Epoch 369/1000\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.5108 - acc: 0.8530 - val_loss: 0.5041 - val_acc: 0.8814\n",
      "Epoch 370/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5099 - acc: 0.8531 - val_loss: 0.4898 - val_acc: 0.8853\n",
      "Epoch 371/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5094 - acc: 0.8536 - val_loss: 0.5142 - val_acc: 0.8754\n",
      "Epoch 372/1000\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 0.5087 - acc: 0.8543 - val_loss: 0.5336 - val_acc: 0.8675\n",
      "Epoch 373/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5084 - acc: 0.8542 - val_loss: 0.4772 - val_acc: 0.8901\n",
      "Epoch 374/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5078 - acc: 0.8534 - val_loss: 0.5423 - val_acc: 0.8670\n",
      "Epoch 375/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5072 - acc: 0.8549 - val_loss: 0.5470 - val_acc: 0.8671\n",
      "Epoch 376/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5064 - acc: 0.8546 - val_loss: 0.5254 - val_acc: 0.8742\n",
      "Epoch 377/1000\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.5058 - acc: 0.8543 - val_loss: 0.4887 - val_acc: 0.8868\n",
      "Epoch 378/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5054 - acc: 0.8551 - val_loss: 0.5256 - val_acc: 0.8733\n",
      "Epoch 379/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5049 - acc: 0.8550 - val_loss: 0.5042 - val_acc: 0.8811\n",
      "Epoch 380/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5042 - acc: 0.8551 - val_loss: 0.5290 - val_acc: 0.8721\n",
      "Epoch 381/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.5034 - acc: 0.8549 - val_loss: 0.5143 - val_acc: 0.8758\n",
      "Epoch 382/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5035 - acc: 0.8556 - val_loss: 0.5503 - val_acc: 0.8615\n",
      "Epoch 383/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.5025 - acc: 0.8559 - val_loss: 0.5132 - val_acc: 0.8764\n",
      "Epoch 384/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5017 - acc: 0.8565 - val_loss: 0.4782 - val_acc: 0.8893\n",
      "Epoch 385/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5015 - acc: 0.8560 - val_loss: 0.4659 - val_acc: 0.8925\n",
      "Epoch 386/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5010 - acc: 0.8563 - val_loss: 0.4910 - val_acc: 0.8815\n",
      "Epoch 387/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.5005 - acc: 0.8557 - val_loss: 0.5007 - val_acc: 0.8803\n",
      "Epoch 388/1000\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.4999 - acc: 0.8569 - val_loss: 0.4778 - val_acc: 0.8884\n",
      "Epoch 389/1000\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.4990 - acc: 0.8573 - val_loss: 0.5119 - val_acc: 0.8757\n",
      "Epoch 390/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4985 - acc: 0.8570 - val_loss: 0.4947 - val_acc: 0.8797\n",
      "Epoch 391/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4984 - acc: 0.8572 - val_loss: 0.4827 - val_acc: 0.8871\n",
      "Epoch 392/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4977 - acc: 0.8576 - val_loss: 0.5278 - val_acc: 0.8750\n",
      "Epoch 393/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4968 - acc: 0.8578 - val_loss: 0.5080 - val_acc: 0.8782\n",
      "Epoch 394/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4964 - acc: 0.8578 - val_loss: 0.5427 - val_acc: 0.8634\n",
      "Epoch 395/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4961 - acc: 0.8582 - val_loss: 0.5054 - val_acc: 0.8805\n",
      "Epoch 396/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4953 - acc: 0.8581 - val_loss: 0.4836 - val_acc: 0.8873\n",
      "Epoch 397/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4947 - acc: 0.8581 - val_loss: 0.4894 - val_acc: 0.8867\n",
      "Epoch 398/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4945 - acc: 0.8581 - val_loss: 0.4809 - val_acc: 0.8893\n",
      "Epoch 399/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4937 - acc: 0.8584 - val_loss: 0.5185 - val_acc: 0.8764\n",
      "Epoch 400/1000\n",
      "30246/30246 [==============================] - 1s 37us/step - loss: 0.4933 - acc: 0.8588 - val_loss: 0.4902 - val_acc: 0.8864\n",
      "Epoch 401/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4929 - acc: 0.8582 - val_loss: 0.5343 - val_acc: 0.8695\n",
      "Epoch 402/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4923 - acc: 0.8588 - val_loss: 0.5175 - val_acc: 0.8721\n",
      "Epoch 403/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4918 - acc: 0.8590 - val_loss: 0.5187 - val_acc: 0.8737\n",
      "Epoch 404/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4912 - acc: 0.8588 - val_loss: 0.5127 - val_acc: 0.8768\n",
      "Epoch 405/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4907 - acc: 0.8597 - val_loss: 0.5338 - val_acc: 0.8697\n",
      "Epoch 406/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4901 - acc: 0.8594 - val_loss: 0.5121 - val_acc: 0.8758\n",
      "Epoch 407/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4892 - acc: 0.8602 - val_loss: 0.5244 - val_acc: 0.8690\n",
      "Epoch 408/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4890 - acc: 0.8607 - val_loss: 0.4859 - val_acc: 0.8868\n",
      "Epoch 409/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4882 - acc: 0.8598 - val_loss: 0.5430 - val_acc: 0.8655\n",
      "Epoch 410/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4884 - acc: 0.8597 - val_loss: 0.5422 - val_acc: 0.8682\n",
      "Epoch 411/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4876 - acc: 0.8600 - val_loss: 0.4899 - val_acc: 0.8840\n",
      "Epoch 412/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4871 - acc: 0.8602 - val_loss: 0.4709 - val_acc: 0.8884\n",
      "Epoch 413/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4866 - acc: 0.8606 - val_loss: 0.5150 - val_acc: 0.8740\n",
      "Epoch 414/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4858 - acc: 0.8603 - val_loss: 0.4826 - val_acc: 0.8851\n",
      "Epoch 415/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4858 - acc: 0.8611 - val_loss: 0.4841 - val_acc: 0.8836\n",
      "Epoch 416/1000\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.4852 - acc: 0.8609 - val_loss: 0.4856 - val_acc: 0.8848\n",
      "Epoch 417/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4845 - acc: 0.8606 - val_loss: 0.4829 - val_acc: 0.8867\n",
      "Epoch 418/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4840 - acc: 0.8610 - val_loss: 0.4517 - val_acc: 0.8945\n",
      "Epoch 419/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4835 - acc: 0.8615 - val_loss: 0.5284 - val_acc: 0.8700\n",
      "Epoch 420/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4829 - acc: 0.8621 - val_loss: 0.4991 - val_acc: 0.8778\n",
      "Epoch 421/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4826 - acc: 0.8614 - val_loss: 0.4863 - val_acc: 0.8842\n",
      "Epoch 422/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4819 - acc: 0.8625 - val_loss: 0.5087 - val_acc: 0.8773\n",
      "Epoch 423/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4815 - acc: 0.8612 - val_loss: 0.4903 - val_acc: 0.8822\n",
      "Epoch 424/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4812 - acc: 0.8622 - val_loss: 0.4955 - val_acc: 0.8799\n",
      "Epoch 425/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4805 - acc: 0.8621 - val_loss: 0.4932 - val_acc: 0.8809\n",
      "Epoch 426/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4799 - acc: 0.8625 - val_loss: 0.4809 - val_acc: 0.8863\n",
      "Epoch 427/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4795 - acc: 0.8635 - val_loss: 0.5196 - val_acc: 0.8729\n",
      "Epoch 428/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4791 - acc: 0.8626 - val_loss: 0.5673 - val_acc: 0.8541\n",
      "Epoch 429/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4786 - acc: 0.8629 - val_loss: 0.4880 - val_acc: 0.8840\n",
      "Epoch 430/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4782 - acc: 0.8623 - val_loss: 0.4979 - val_acc: 0.8826\n",
      "Epoch 431/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4780 - acc: 0.8633 - val_loss: 0.4690 - val_acc: 0.8900\n",
      "Epoch 432/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4770 - acc: 0.8632 - val_loss: 0.5235 - val_acc: 0.8690\n",
      "Epoch 433/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4766 - acc: 0.8628 - val_loss: 0.4772 - val_acc: 0.8883\n",
      "Epoch 434/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4764 - acc: 0.8633 - val_loss: 0.4340 - val_acc: 0.8992\n",
      "Epoch 435/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4760 - acc: 0.8639 - val_loss: 0.4425 - val_acc: 0.8974\n",
      "Epoch 436/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4755 - acc: 0.8633 - val_loss: 0.4811 - val_acc: 0.8846\n",
      "Epoch 437/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4747 - acc: 0.8649 - val_loss: 0.4667 - val_acc: 0.8905\n",
      "Epoch 438/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4747 - acc: 0.8638 - val_loss: 0.4929 - val_acc: 0.8811\n",
      "Epoch 439/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4738 - acc: 0.8644 - val_loss: 0.5132 - val_acc: 0.8757\n",
      "Epoch 440/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4736 - acc: 0.8652 - val_loss: 0.5327 - val_acc: 0.8687\n",
      "Epoch 441/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4731 - acc: 0.8646 - val_loss: 0.5297 - val_acc: 0.8719\n",
      "Epoch 442/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4728 - acc: 0.8648 - val_loss: 0.5054 - val_acc: 0.8781\n",
      "Epoch 443/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4724 - acc: 0.8648 - val_loss: 0.4985 - val_acc: 0.8801\n",
      "Epoch 444/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4711 - acc: 0.8654 - val_loss: 0.5271 - val_acc: 0.8699\n",
      "Epoch 445/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4714 - acc: 0.8660 - val_loss: 0.4950 - val_acc: 0.8830\n",
      "Epoch 446/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4705 - acc: 0.8652 - val_loss: 0.4464 - val_acc: 0.8966\n",
      "Epoch 447/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4707 - acc: 0.8655 - val_loss: 0.4961 - val_acc: 0.8801\n",
      "Epoch 448/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4701 - acc: 0.8660 - val_loss: 0.5282 - val_acc: 0.8682\n",
      "Epoch 449/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4693 - acc: 0.8655 - val_loss: 0.5714 - val_acc: 0.8520\n",
      "Epoch 450/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4691 - acc: 0.8658 - val_loss: 0.5287 - val_acc: 0.8695\n",
      "Epoch 451/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4687 - acc: 0.8664 - val_loss: 0.4359 - val_acc: 0.8982\n",
      "Epoch 452/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.4680 - acc: 0.8663 - val_loss: 0.5187 - val_acc: 0.8725\n",
      "Epoch 453/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4680 - acc: 0.8666 - val_loss: 0.4839 - val_acc: 0.8839\n",
      "Epoch 454/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4678 - acc: 0.8668 - val_loss: 0.4556 - val_acc: 0.8939\n",
      "Epoch 455/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4671 - acc: 0.8669 - val_loss: 0.4710 - val_acc: 0.8871\n",
      "Epoch 456/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4665 - acc: 0.8671 - val_loss: 0.5640 - val_acc: 0.8600\n",
      "Epoch 457/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4663 - acc: 0.8669 - val_loss: 0.4806 - val_acc: 0.8850\n",
      "Epoch 458/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4658 - acc: 0.8679 - val_loss: 0.5594 - val_acc: 0.8602\n",
      "Epoch 459/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4653 - acc: 0.8666 - val_loss: 0.4569 - val_acc: 0.8946\n",
      "Epoch 460/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4646 - acc: 0.8666 - val_loss: 0.4723 - val_acc: 0.8894\n",
      "Epoch 461/1000\n",
      "30246/30246 [==============================] - 1s 41us/step - loss: 0.4646 - acc: 0.8675 - val_loss: 0.5091 - val_acc: 0.8766\n",
      "Epoch 462/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4642 - acc: 0.8676 - val_loss: 0.5368 - val_acc: 0.8660\n",
      "Epoch 463/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4640 - acc: 0.8672 - val_loss: 0.5092 - val_acc: 0.8770\n",
      "Epoch 464/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4634 - acc: 0.8678 - val_loss: 0.5296 - val_acc: 0.8680\n",
      "Epoch 465/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4629 - acc: 0.8672 - val_loss: 0.5028 - val_acc: 0.8775\n",
      "Epoch 466/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4625 - acc: 0.8677 - val_loss: 0.4785 - val_acc: 0.8853\n",
      "Epoch 467/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4620 - acc: 0.8680 - val_loss: 0.4909 - val_acc: 0.8818\n",
      "Epoch 468/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4618 - acc: 0.8673 - val_loss: 0.4990 - val_acc: 0.8799\n",
      "Epoch 469/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4614 - acc: 0.8679 - val_loss: 0.5265 - val_acc: 0.8711\n",
      "Epoch 470/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.4610 - acc: 0.8675 - val_loss: 0.5070 - val_acc: 0.8750\n",
      "Epoch 471/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4607 - acc: 0.8677 - val_loss: 0.5089 - val_acc: 0.8720\n",
      "Epoch 472/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4602 - acc: 0.8688 - val_loss: 0.5075 - val_acc: 0.8765\n",
      "Epoch 473/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4599 - acc: 0.8685 - val_loss: 0.4713 - val_acc: 0.8896\n",
      "Epoch 474/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4595 - acc: 0.8680 - val_loss: 0.4770 - val_acc: 0.8867\n",
      "Epoch 475/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4593 - acc: 0.8683 - val_loss: 0.4904 - val_acc: 0.8830\n",
      "Epoch 476/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4587 - acc: 0.8690 - val_loss: 0.5178 - val_acc: 0.8734\n",
      "Epoch 477/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4581 - acc: 0.8692 - val_loss: 0.5068 - val_acc: 0.8764\n",
      "Epoch 478/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4581 - acc: 0.8683 - val_loss: 0.4692 - val_acc: 0.8873\n",
      "Epoch 479/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4577 - acc: 0.8687 - val_loss: 0.4728 - val_acc: 0.8865\n",
      "Epoch 480/1000\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.4573 - acc: 0.8692 - val_loss: 0.4852 - val_acc: 0.8839\n",
      "Epoch 481/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4568 - acc: 0.8695 - val_loss: 0.4607 - val_acc: 0.8904\n",
      "Epoch 482/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4562 - acc: 0.8689 - val_loss: 0.4657 - val_acc: 0.8894\n",
      "Epoch 483/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4563 - acc: 0.8691 - val_loss: 0.5004 - val_acc: 0.8761\n",
      "Epoch 484/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4560 - acc: 0.8696 - val_loss: 0.4467 - val_acc: 0.8928\n",
      "Epoch 485/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4552 - acc: 0.8693 - val_loss: 0.4326 - val_acc: 0.8979\n",
      "Epoch 486/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4548 - acc: 0.8693 - val_loss: 0.4854 - val_acc: 0.8839\n",
      "Epoch 487/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4546 - acc: 0.8703 - val_loss: 0.4950 - val_acc: 0.8799\n",
      "Epoch 488/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4542 - acc: 0.8697 - val_loss: 0.4462 - val_acc: 0.8933\n",
      "Epoch 489/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4539 - acc: 0.8701 - val_loss: 0.4660 - val_acc: 0.8908\n",
      "Epoch 490/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4533 - acc: 0.8705 - val_loss: 0.4804 - val_acc: 0.8840\n",
      "Epoch 491/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4531 - acc: 0.8710 - val_loss: 0.4616 - val_acc: 0.8922\n",
      "Epoch 492/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4530 - acc: 0.8707 - val_loss: 0.4749 - val_acc: 0.8860\n",
      "Epoch 493/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4526 - acc: 0.8693 - val_loss: 0.4579 - val_acc: 0.8928\n",
      "Epoch 494/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4520 - acc: 0.8709 - val_loss: 0.4586 - val_acc: 0.8892\n",
      "Epoch 495/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4517 - acc: 0.8708 - val_loss: 0.4975 - val_acc: 0.8795\n",
      "Epoch 496/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4512 - acc: 0.8703 - val_loss: 0.5086 - val_acc: 0.8736\n",
      "Epoch 497/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4512 - acc: 0.8709 - val_loss: 0.4994 - val_acc: 0.8775\n",
      "Epoch 498/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4507 - acc: 0.8706 - val_loss: 0.4822 - val_acc: 0.8834\n",
      "Epoch 499/1000\n",
      "30246/30246 [==============================] - 2s 56us/step - loss: 0.4504 - acc: 0.8710 - val_loss: 0.4571 - val_acc: 0.8917\n",
      "Epoch 500/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4502 - acc: 0.8705 - val_loss: 0.4682 - val_acc: 0.8861\n",
      "Epoch 501/1000\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 0.4500 - acc: 0.8709 - val_loss: 0.5132 - val_acc: 0.8727\n",
      "Epoch 502/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4493 - acc: 0.8709 - val_loss: 0.4365 - val_acc: 0.8970\n",
      "Epoch 503/1000\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.4492 - acc: 0.8707 - val_loss: 0.5398 - val_acc: 0.8635\n",
      "Epoch 504/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4486 - acc: 0.8706 - val_loss: 0.5215 - val_acc: 0.8719\n",
      "Epoch 505/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4486 - acc: 0.8721 - val_loss: 0.4612 - val_acc: 0.8906\n",
      "Epoch 506/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4482 - acc: 0.8720 - val_loss: 0.5096 - val_acc: 0.8756\n",
      "Epoch 507/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4476 - acc: 0.8716 - val_loss: 0.4527 - val_acc: 0.8910\n",
      "Epoch 508/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4471 - acc: 0.8715 - val_loss: 0.4672 - val_acc: 0.8888\n",
      "Epoch 509/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4471 - acc: 0.8722 - val_loss: 0.5248 - val_acc: 0.8716\n",
      "Epoch 510/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4470 - acc: 0.8720 - val_loss: 0.5005 - val_acc: 0.8785\n",
      "Epoch 511/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4462 - acc: 0.8722 - val_loss: 0.4664 - val_acc: 0.8848\n",
      "Epoch 512/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4464 - acc: 0.8717 - val_loss: 0.4821 - val_acc: 0.8844\n",
      "Epoch 513/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4456 - acc: 0.8723 - val_loss: 0.4463 - val_acc: 0.8949\n",
      "Epoch 514/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4453 - acc: 0.8723 - val_loss: 0.4924 - val_acc: 0.8814\n",
      "Epoch 515/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4448 - acc: 0.8724 - val_loss: 0.4757 - val_acc: 0.8856\n",
      "Epoch 516/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4448 - acc: 0.8728 - val_loss: 0.4638 - val_acc: 0.8872\n",
      "Epoch 517/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.4444 - acc: 0.8722 - val_loss: 0.4377 - val_acc: 0.8945\n",
      "Epoch 518/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4442 - acc: 0.8731 - val_loss: 0.4786 - val_acc: 0.8872\n",
      "Epoch 519/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4437 - acc: 0.8724 - val_loss: 0.4624 - val_acc: 0.8904\n",
      "Epoch 520/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4435 - acc: 0.8731 - val_loss: 0.4983 - val_acc: 0.8768\n",
      "Epoch 521/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.4432 - acc: 0.8734 - val_loss: 0.4986 - val_acc: 0.8790\n",
      "Epoch 522/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4430 - acc: 0.8732 - val_loss: 0.4969 - val_acc: 0.8790\n",
      "Epoch 523/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4424 - acc: 0.8732 - val_loss: 0.4590 - val_acc: 0.8929\n",
      "Epoch 524/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4421 - acc: 0.8722 - val_loss: 0.4430 - val_acc: 0.8947\n",
      "Epoch 525/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4420 - acc: 0.8736 - val_loss: 0.4488 - val_acc: 0.8925\n",
      "Epoch 526/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4417 - acc: 0.8736 - val_loss: 0.4790 - val_acc: 0.8835\n",
      "Epoch 527/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4415 - acc: 0.8737 - val_loss: 0.4726 - val_acc: 0.8875\n",
      "Epoch 528/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4411 - acc: 0.8733 - val_loss: 0.4937 - val_acc: 0.8810\n",
      "Epoch 529/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4408 - acc: 0.8743 - val_loss: 0.4693 - val_acc: 0.8881\n",
      "Epoch 530/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4403 - acc: 0.8737 - val_loss: 0.4751 - val_acc: 0.8860\n",
      "Epoch 531/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4401 - acc: 0.8736 - val_loss: 0.4539 - val_acc: 0.8902\n",
      "Epoch 532/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4395 - acc: 0.8743 - val_loss: 0.5293 - val_acc: 0.8672\n",
      "Epoch 533/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4393 - acc: 0.8738 - val_loss: 0.4984 - val_acc: 0.8791\n",
      "Epoch 534/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4389 - acc: 0.8745 - val_loss: 0.4845 - val_acc: 0.8832\n",
      "Epoch 535/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4389 - acc: 0.8745 - val_loss: 0.4873 - val_acc: 0.8835\n",
      "Epoch 536/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4385 - acc: 0.8752 - val_loss: 0.4850 - val_acc: 0.8851\n",
      "Epoch 537/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4381 - acc: 0.8746 - val_loss: 0.4177 - val_acc: 0.9040\n",
      "Epoch 538/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4379 - acc: 0.8739 - val_loss: 0.4716 - val_acc: 0.8876\n",
      "Epoch 539/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4377 - acc: 0.8748 - val_loss: 0.4938 - val_acc: 0.8812\n",
      "Epoch 540/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4374 - acc: 0.8754 - val_loss: 0.4556 - val_acc: 0.8925\n",
      "Epoch 541/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4369 - acc: 0.8742 - val_loss: 0.4337 - val_acc: 0.8967\n",
      "Epoch 542/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4367 - acc: 0.8757 - val_loss: 0.4375 - val_acc: 0.8965\n",
      "Epoch 543/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4367 - acc: 0.8747 - val_loss: 0.4626 - val_acc: 0.8906\n",
      "Epoch 544/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4365 - acc: 0.8748 - val_loss: 0.4746 - val_acc: 0.8848\n",
      "Epoch 545/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4360 - acc: 0.8756 - val_loss: 0.4654 - val_acc: 0.8881\n",
      "Epoch 546/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4356 - acc: 0.8753 - val_loss: 0.4853 - val_acc: 0.8815\n",
      "Epoch 547/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4353 - acc: 0.8755 - val_loss: 0.4620 - val_acc: 0.8906\n",
      "Epoch 548/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4351 - acc: 0.8753 - val_loss: 0.4753 - val_acc: 0.8846\n",
      "Epoch 549/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4346 - acc: 0.8753 - val_loss: 0.4156 - val_acc: 0.9048\n",
      "Epoch 550/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4342 - acc: 0.8753 - val_loss: 0.4990 - val_acc: 0.8781\n",
      "Epoch 551/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4343 - acc: 0.8755 - val_loss: 0.4750 - val_acc: 0.8861\n",
      "Epoch 552/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4337 - acc: 0.8750 - val_loss: 0.4604 - val_acc: 0.8900\n",
      "Epoch 553/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4335 - acc: 0.8759 - val_loss: 0.4611 - val_acc: 0.8888\n",
      "Epoch 554/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4335 - acc: 0.8760 - val_loss: 0.4468 - val_acc: 0.8942\n",
      "Epoch 555/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4329 - acc: 0.8757 - val_loss: 0.4700 - val_acc: 0.8876\n",
      "Epoch 556/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4324 - acc: 0.8758 - val_loss: 0.5304 - val_acc: 0.8697\n",
      "Epoch 557/1000\n",
      "30246/30246 [==============================] - 2s 52us/step - loss: 0.4325 - acc: 0.8759 - val_loss: 0.4582 - val_acc: 0.8901\n",
      "Epoch 558/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4320 - acc: 0.8756 - val_loss: 0.4851 - val_acc: 0.8824\n",
      "Epoch 559/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4319 - acc: 0.8764 - val_loss: 0.4467 - val_acc: 0.8933\n",
      "Epoch 560/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4312 - acc: 0.8762 - val_loss: 0.4828 - val_acc: 0.8839\n",
      "Epoch 561/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4306 - acc: 0.8769 - val_loss: 0.4596 - val_acc: 0.8892\n",
      "Epoch 562/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4314 - acc: 0.8769 - val_loss: 0.4792 - val_acc: 0.8838\n",
      "Epoch 563/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4307 - acc: 0.8775 - val_loss: 0.4136 - val_acc: 0.9035\n",
      "Epoch 564/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4302 - acc: 0.8769 - val_loss: 0.4480 - val_acc: 0.8930\n",
      "Epoch 565/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4301 - acc: 0.8768 - val_loss: 0.4512 - val_acc: 0.8906\n",
      "Epoch 566/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4297 - acc: 0.8769 - val_loss: 0.5031 - val_acc: 0.8770\n",
      "Epoch 567/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4296 - acc: 0.8767 - val_loss: 0.4810 - val_acc: 0.8838\n",
      "Epoch 568/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4290 - acc: 0.8771 - val_loss: 0.4017 - val_acc: 0.9066\n",
      "Epoch 569/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4296 - acc: 0.8770 - val_loss: 0.4505 - val_acc: 0.8918\n",
      "Epoch 570/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.4287 - acc: 0.8777 - val_loss: 0.4729 - val_acc: 0.8843\n",
      "Epoch 571/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4285 - acc: 0.8775 - val_loss: 0.4207 - val_acc: 0.9013\n",
      "Epoch 572/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4285 - acc: 0.8774 - val_loss: 0.4811 - val_acc: 0.8830\n",
      "Epoch 573/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4279 - acc: 0.8788 - val_loss: 0.4588 - val_acc: 0.8898\n",
      "Epoch 574/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4275 - acc: 0.8781 - val_loss: 0.4994 - val_acc: 0.8783\n",
      "Epoch 575/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4275 - acc: 0.8782 - val_loss: 0.4545 - val_acc: 0.8897\n",
      "Epoch 576/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4274 - acc: 0.8770 - val_loss: 0.4524 - val_acc: 0.8897\n",
      "Epoch 577/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4268 - acc: 0.8785 - val_loss: 0.4923 - val_acc: 0.8812\n",
      "Epoch 578/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4268 - acc: 0.8772 - val_loss: 0.4576 - val_acc: 0.8905\n",
      "Epoch 579/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4262 - acc: 0.8783 - val_loss: 0.4552 - val_acc: 0.8898\n",
      "Epoch 580/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4263 - acc: 0.8782 - val_loss: 0.4648 - val_acc: 0.8876\n",
      "Epoch 581/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4261 - acc: 0.8784 - val_loss: 0.4838 - val_acc: 0.8826\n",
      "Epoch 582/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4258 - acc: 0.8793 - val_loss: 0.4669 - val_acc: 0.8892\n",
      "Epoch 583/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4255 - acc: 0.8779 - val_loss: 0.5073 - val_acc: 0.8720\n",
      "Epoch 584/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4251 - acc: 0.8785 - val_loss: 0.4364 - val_acc: 0.8970\n",
      "Epoch 585/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4249 - acc: 0.8784 - val_loss: 0.4475 - val_acc: 0.8926\n",
      "Epoch 586/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4246 - acc: 0.8788 - val_loss: 0.4580 - val_acc: 0.8889\n",
      "Epoch 587/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4244 - acc: 0.8783 - val_loss: 0.4439 - val_acc: 0.8930\n",
      "Epoch 588/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4241 - acc: 0.8787 - val_loss: 0.4995 - val_acc: 0.8779\n",
      "Epoch 589/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4238 - acc: 0.8788 - val_loss: 0.4413 - val_acc: 0.8953\n",
      "Epoch 590/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4235 - acc: 0.8785 - val_loss: 0.4945 - val_acc: 0.8811\n",
      "Epoch 591/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4234 - acc: 0.8790 - val_loss: 0.4771 - val_acc: 0.8855\n",
      "Epoch 592/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4233 - acc: 0.8787 - val_loss: 0.4705 - val_acc: 0.8865\n",
      "Epoch 593/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4228 - acc: 0.8788 - val_loss: 0.4477 - val_acc: 0.8937\n",
      "Epoch 594/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4224 - acc: 0.8799 - val_loss: 0.4580 - val_acc: 0.8904\n",
      "Epoch 595/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4222 - acc: 0.8795 - val_loss: 0.5197 - val_acc: 0.8686\n",
      "Epoch 596/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4222 - acc: 0.8791 - val_loss: 0.4702 - val_acc: 0.8856\n",
      "Epoch 597/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4221 - acc: 0.8793 - val_loss: 0.4914 - val_acc: 0.8799\n",
      "Epoch 598/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4215 - acc: 0.8796 - val_loss: 0.4762 - val_acc: 0.8860\n",
      "Epoch 599/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4215 - acc: 0.8793 - val_loss: 0.4567 - val_acc: 0.8896\n",
      "Epoch 600/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.4209 - acc: 0.8799 - val_loss: 0.4677 - val_acc: 0.8872\n",
      "Epoch 601/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4207 - acc: 0.8798 - val_loss: 0.4543 - val_acc: 0.8909\n",
      "Epoch 602/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4204 - acc: 0.8804 - val_loss: 0.5063 - val_acc: 0.8754\n",
      "Epoch 603/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4206 - acc: 0.8800 - val_loss: 0.4821 - val_acc: 0.8819\n",
      "Epoch 604/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4201 - acc: 0.8798 - val_loss: 0.4707 - val_acc: 0.8860\n",
      "Epoch 605/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4200 - acc: 0.8800 - val_loss: 0.4208 - val_acc: 0.9017\n",
      "Epoch 606/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4196 - acc: 0.8794 - val_loss: 0.4762 - val_acc: 0.8844\n",
      "Epoch 607/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4194 - acc: 0.8805 - val_loss: 0.4812 - val_acc: 0.8819\n",
      "Epoch 608/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4189 - acc: 0.8799 - val_loss: 0.4614 - val_acc: 0.8901\n",
      "Epoch 609/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4188 - acc: 0.8801 - val_loss: 0.4998 - val_acc: 0.8766\n",
      "Epoch 610/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4185 - acc: 0.8807 - val_loss: 0.4436 - val_acc: 0.8930\n",
      "Epoch 611/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4186 - acc: 0.8808 - val_loss: 0.4995 - val_acc: 0.8765\n",
      "Epoch 612/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4187 - acc: 0.8808 - val_loss: 0.4648 - val_acc: 0.8861\n",
      "Epoch 613/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4183 - acc: 0.8808 - val_loss: 0.4462 - val_acc: 0.8922\n",
      "Epoch 614/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4178 - acc: 0.8811 - val_loss: 0.4765 - val_acc: 0.8853\n",
      "Epoch 615/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4173 - acc: 0.8814 - val_loss: 0.4836 - val_acc: 0.8814\n",
      "Epoch 616/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4173 - acc: 0.8813 - val_loss: 0.4699 - val_acc: 0.8868\n",
      "Epoch 617/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4171 - acc: 0.8812 - val_loss: 0.4626 - val_acc: 0.8883\n",
      "Epoch 618/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4172 - acc: 0.8805 - val_loss: 0.4526 - val_acc: 0.8914\n",
      "Epoch 619/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4169 - acc: 0.8801 - val_loss: 0.4597 - val_acc: 0.8896\n",
      "Epoch 620/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4165 - acc: 0.8811 - val_loss: 0.4323 - val_acc: 0.8974\n",
      "Epoch 621/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4160 - acc: 0.8811 - val_loss: 0.4252 - val_acc: 0.8991\n",
      "Epoch 622/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4160 - acc: 0.8815 - val_loss: 0.4884 - val_acc: 0.8806\n",
      "Epoch 623/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4160 - acc: 0.8809 - val_loss: 0.4346 - val_acc: 0.8969\n",
      "Epoch 624/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4157 - acc: 0.8816 - val_loss: 0.4429 - val_acc: 0.8949\n",
      "Epoch 625/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4152 - acc: 0.8822 - val_loss: 0.4246 - val_acc: 0.9003\n",
      "Epoch 626/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4150 - acc: 0.8812 - val_loss: 0.4364 - val_acc: 0.8986\n",
      "Epoch 627/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4151 - acc: 0.8807 - val_loss: 0.4718 - val_acc: 0.8840\n",
      "Epoch 628/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4147 - acc: 0.8814 - val_loss: 0.4804 - val_acc: 0.8838\n",
      "Epoch 629/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4147 - acc: 0.8810 - val_loss: 0.4520 - val_acc: 0.8916\n",
      "Epoch 630/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4140 - acc: 0.8816 - val_loss: 0.4239 - val_acc: 0.9000\n",
      "Epoch 631/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4140 - acc: 0.8820 - val_loss: 0.4499 - val_acc: 0.8917\n",
      "Epoch 632/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4140 - acc: 0.8818 - val_loss: 0.4428 - val_acc: 0.8939\n",
      "Epoch 633/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4136 - acc: 0.8817 - val_loss: 0.4154 - val_acc: 0.9012\n",
      "Epoch 634/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4133 - acc: 0.8824 - val_loss: 0.4559 - val_acc: 0.8910\n",
      "Epoch 635/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.4129 - acc: 0.8819 - val_loss: 0.4860 - val_acc: 0.8819\n",
      "Epoch 636/1000\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.4126 - acc: 0.8822 - val_loss: 0.4599 - val_acc: 0.8892\n",
      "Epoch 637/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4126 - acc: 0.8823 - val_loss: 0.4494 - val_acc: 0.8918\n",
      "Epoch 638/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4125 - acc: 0.8823 - val_loss: 0.5143 - val_acc: 0.8729\n",
      "Epoch 639/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.4125 - acc: 0.8820 - val_loss: 0.4771 - val_acc: 0.8842\n",
      "Epoch 640/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4118 - acc: 0.8818 - val_loss: 0.4530 - val_acc: 0.8917\n",
      "Epoch 641/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4116 - acc: 0.8819 - val_loss: 0.4862 - val_acc: 0.8818\n",
      "Epoch 642/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4113 - acc: 0.8816 - val_loss: 0.5211 - val_acc: 0.8688\n",
      "Epoch 643/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4114 - acc: 0.8822 - val_loss: 0.4617 - val_acc: 0.8880\n",
      "Epoch 644/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4111 - acc: 0.8826 - val_loss: 0.4244 - val_acc: 0.9012\n",
      "Epoch 645/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4108 - acc: 0.8823 - val_loss: 0.4622 - val_acc: 0.8893\n",
      "Epoch 646/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4108 - acc: 0.8821 - val_loss: 0.4404 - val_acc: 0.8950\n",
      "Epoch 647/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4104 - acc: 0.8828 - val_loss: 0.4675 - val_acc: 0.8880\n",
      "Epoch 648/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4101 - acc: 0.8831 - val_loss: 0.4804 - val_acc: 0.8818\n",
      "Epoch 649/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4098 - acc: 0.8820 - val_loss: 0.4303 - val_acc: 0.8967\n",
      "Epoch 650/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4102 - acc: 0.8824 - val_loss: 0.4388 - val_acc: 0.8967\n",
      "Epoch 651/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4095 - acc: 0.8825 - val_loss: 0.4833 - val_acc: 0.8819\n",
      "Epoch 652/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4095 - acc: 0.8833 - val_loss: 0.4728 - val_acc: 0.8857\n",
      "Epoch 653/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4092 - acc: 0.8829 - val_loss: 0.4417 - val_acc: 0.8946\n",
      "Epoch 654/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4087 - acc: 0.8827 - val_loss: 0.4689 - val_acc: 0.8876\n",
      "Epoch 655/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4091 - acc: 0.8828 - val_loss: 0.4682 - val_acc: 0.8869\n",
      "Epoch 656/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4086 - acc: 0.8823 - val_loss: 0.4835 - val_acc: 0.8830\n",
      "Epoch 657/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4079 - acc: 0.8832 - val_loss: 0.4754 - val_acc: 0.8847\n",
      "Epoch 658/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4080 - acc: 0.8832 - val_loss: 0.4316 - val_acc: 0.8970\n",
      "Epoch 659/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4079 - acc: 0.8836 - val_loss: 0.4665 - val_acc: 0.8877\n",
      "Epoch 660/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4073 - acc: 0.8838 - val_loss: 0.4661 - val_acc: 0.8875\n",
      "Epoch 661/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4076 - acc: 0.8831 - val_loss: 0.4579 - val_acc: 0.8892\n",
      "Epoch 662/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4074 - acc: 0.8836 - val_loss: 0.4885 - val_acc: 0.8806\n",
      "Epoch 663/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4069 - acc: 0.8831 - val_loss: 0.4726 - val_acc: 0.8857\n",
      "Epoch 664/1000\n",
      "30246/30246 [==============================] - 1s 45us/step - loss: 0.4069 - acc: 0.8829 - val_loss: 0.5057 - val_acc: 0.8746\n",
      "Epoch 665/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4066 - acc: 0.8838 - val_loss: 0.4465 - val_acc: 0.8929\n",
      "Epoch 666/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4063 - acc: 0.8848 - val_loss: 0.4136 - val_acc: 0.9043\n",
      "Epoch 667/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4060 - acc: 0.8846 - val_loss: 0.4685 - val_acc: 0.8875\n",
      "Epoch 668/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4061 - acc: 0.8832 - val_loss: 0.3987 - val_acc: 0.9062\n",
      "Epoch 669/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4059 - acc: 0.8843 - val_loss: 0.4716 - val_acc: 0.8857\n",
      "Epoch 670/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4055 - acc: 0.8851 - val_loss: 0.4629 - val_acc: 0.8900\n",
      "Epoch 671/1000\n",
      "30246/30246 [==============================] - 1s 44us/step - loss: 0.4056 - acc: 0.8830 - val_loss: 0.4708 - val_acc: 0.8864\n",
      "Epoch 672/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4054 - acc: 0.8837 - val_loss: 0.4373 - val_acc: 0.8965\n",
      "Epoch 673/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4047 - acc: 0.8843 - val_loss: 0.4325 - val_acc: 0.8987\n",
      "Epoch 674/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4047 - acc: 0.8853 - val_loss: 0.4373 - val_acc: 0.8975\n",
      "Epoch 675/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4047 - acc: 0.8841 - val_loss: 0.4499 - val_acc: 0.8926\n",
      "Epoch 676/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4041 - acc: 0.8848 - val_loss: 0.4899 - val_acc: 0.8785\n",
      "Epoch 677/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4044 - acc: 0.8836 - val_loss: 0.4498 - val_acc: 0.8928\n",
      "Epoch 678/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4040 - acc: 0.8850 - val_loss: 0.4546 - val_acc: 0.8912\n",
      "Epoch 679/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4038 - acc: 0.8845 - val_loss: 0.4494 - val_acc: 0.8921\n",
      "Epoch 680/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4037 - acc: 0.8850 - val_loss: 0.4648 - val_acc: 0.8880\n",
      "Epoch 681/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4036 - acc: 0.8851 - val_loss: 0.4836 - val_acc: 0.8805\n",
      "Epoch 682/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.4035 - acc: 0.8846 - val_loss: 0.4627 - val_acc: 0.8869\n",
      "Epoch 683/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4029 - acc: 0.8841 - val_loss: 0.4518 - val_acc: 0.8926\n",
      "Epoch 684/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4033 - acc: 0.8847 - val_loss: 0.4666 - val_acc: 0.8875\n",
      "Epoch 685/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.4028 - acc: 0.8841 - val_loss: 0.4405 - val_acc: 0.8967\n",
      "Epoch 686/1000\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.4023 - acc: 0.8848 - val_loss: 0.4692 - val_acc: 0.8871\n",
      "Epoch 687/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4025 - acc: 0.8846 - val_loss: 0.4472 - val_acc: 0.8933\n",
      "Epoch 688/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4022 - acc: 0.8849 - val_loss: 0.4373 - val_acc: 0.8958\n",
      "Epoch 689/1000\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.4020 - acc: 0.8842 - val_loss: 0.4881 - val_acc: 0.8807\n",
      "Epoch 690/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4017 - acc: 0.8848 - val_loss: 0.4602 - val_acc: 0.8910\n",
      "Epoch 691/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4013 - acc: 0.8850 - val_loss: 0.4571 - val_acc: 0.8918\n",
      "Epoch 692/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4017 - acc: 0.8851 - val_loss: 0.4563 - val_acc: 0.8909\n",
      "Epoch 693/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4011 - acc: 0.8849 - val_loss: 0.4368 - val_acc: 0.8970\n",
      "Epoch 694/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4010 - acc: 0.8847 - val_loss: 0.3908 - val_acc: 0.9084\n",
      "Epoch 695/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4007 - acc: 0.8848 - val_loss: 0.4828 - val_acc: 0.8843\n",
      "Epoch 696/1000\n",
      "30246/30246 [==============================] - 1s 28us/step - loss: 0.4007 - acc: 0.8848 - val_loss: 0.4577 - val_acc: 0.8900\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4002 - acc: 0.8862 - val_loss: 0.4599 - val_acc: 0.8901\n",
      "Epoch 698/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.4001 - acc: 0.8856 - val_loss: 0.4278 - val_acc: 0.9006\n",
      "Epoch 699/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.4001 - acc: 0.8847 - val_loss: 0.4610 - val_acc: 0.8887\n",
      "Epoch 700/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3996 - acc: 0.8852 - val_loss: 0.4560 - val_acc: 0.8906\n",
      "Epoch 701/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3997 - acc: 0.8857 - val_loss: 0.4725 - val_acc: 0.8860\n",
      "Epoch 702/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3995 - acc: 0.8850 - val_loss: 0.4443 - val_acc: 0.8925\n",
      "Epoch 703/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3992 - acc: 0.8856 - val_loss: 0.4625 - val_acc: 0.8888\n",
      "Epoch 704/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3990 - acc: 0.8859 - val_loss: 0.4666 - val_acc: 0.8880\n",
      "Epoch 705/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3989 - acc: 0.8861 - val_loss: 0.4584 - val_acc: 0.8900\n",
      "Epoch 706/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3986 - acc: 0.8858 - val_loss: 0.4907 - val_acc: 0.8812\n",
      "Epoch 707/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3985 - acc: 0.8854 - val_loss: 0.4740 - val_acc: 0.8869\n",
      "Epoch 708/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3981 - acc: 0.8855 - val_loss: 0.4833 - val_acc: 0.8815\n",
      "Epoch 709/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3981 - acc: 0.8861 - val_loss: 0.4516 - val_acc: 0.8910\n",
      "Epoch 710/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3978 - acc: 0.8856 - val_loss: 0.4083 - val_acc: 0.9043\n",
      "Epoch 711/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3978 - acc: 0.8855 - val_loss: 0.4091 - val_acc: 0.9028\n",
      "Epoch 712/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3974 - acc: 0.8857 - val_loss: 0.4008 - val_acc: 0.9082\n",
      "Epoch 713/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3978 - acc: 0.8865 - val_loss: 0.4688 - val_acc: 0.8868\n",
      "Epoch 714/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3974 - acc: 0.8862 - val_loss: 0.4872 - val_acc: 0.8824\n",
      "Epoch 715/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3972 - acc: 0.8861 - val_loss: 0.4124 - val_acc: 0.9035\n",
      "Epoch 716/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3967 - acc: 0.8865 - val_loss: 0.4389 - val_acc: 0.8974\n",
      "Epoch 717/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3964 - acc: 0.8858 - val_loss: 0.4680 - val_acc: 0.8881\n",
      "Epoch 718/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3965 - acc: 0.8869 - val_loss: 0.4155 - val_acc: 0.9029\n",
      "Epoch 719/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3964 - acc: 0.8859 - val_loss: 0.4487 - val_acc: 0.8921\n",
      "Epoch 720/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3957 - acc: 0.8864 - val_loss: 0.3994 - val_acc: 0.9086\n",
      "Epoch 721/1000\n",
      "30246/30246 [==============================] - 1s 31us/step - loss: 0.3959 - acc: 0.8868 - val_loss: 0.4851 - val_acc: 0.8811\n",
      "Epoch 722/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3958 - acc: 0.8867 - val_loss: 0.5117 - val_acc: 0.8748\n",
      "Epoch 723/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3955 - acc: 0.8863 - val_loss: 0.4236 - val_acc: 0.9008\n",
      "Epoch 724/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3956 - acc: 0.8867 - val_loss: 0.4744 - val_acc: 0.8851\n",
      "Epoch 725/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8871 - val_loss: 0.4514 - val_acc: 0.8904\n",
      "Epoch 726/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3953 - acc: 0.8865 - val_loss: 0.4759 - val_acc: 0.8855\n",
      "Epoch 727/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3950 - acc: 0.8869 - val_loss: 0.4712 - val_acc: 0.8857\n",
      "Epoch 728/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3951 - acc: 0.8865 - val_loss: 0.4474 - val_acc: 0.8926\n",
      "Epoch 729/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3949 - acc: 0.8864 - val_loss: 0.4607 - val_acc: 0.8883\n",
      "Epoch 730/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3948 - acc: 0.8861 - val_loss: 0.4591 - val_acc: 0.8912\n",
      "Epoch 731/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3941 - acc: 0.8872 - val_loss: 0.3932 - val_acc: 0.9082\n",
      "Epoch 732/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3945 - acc: 0.8872 - val_loss: 0.4400 - val_acc: 0.8955\n",
      "Epoch 733/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3939 - acc: 0.8882 - val_loss: 0.4841 - val_acc: 0.8816\n",
      "Epoch 734/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3937 - acc: 0.8867 - val_loss: 0.4258 - val_acc: 0.8988\n",
      "Epoch 735/1000\n",
      "30246/30246 [==============================] - 1s 25us/step - loss: 0.3932 - acc: 0.8869 - val_loss: 0.5392 - val_acc: 0.8639\n",
      "Epoch 736/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3930 - acc: 0.8882 - val_loss: 0.4141 - val_acc: 0.9010\n",
      "Epoch 737/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3932 - acc: 0.8868 - val_loss: 0.4064 - val_acc: 0.9048\n",
      "Epoch 738/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3932 - acc: 0.8865 - val_loss: 0.4718 - val_acc: 0.8868\n",
      "Epoch 739/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3929 - acc: 0.8874 - val_loss: 0.4743 - val_acc: 0.8856\n",
      "Epoch 740/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3928 - acc: 0.8874 - val_loss: 0.4730 - val_acc: 0.8840\n",
      "Epoch 741/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3928 - acc: 0.8868 - val_loss: 0.4378 - val_acc: 0.8961\n",
      "Epoch 742/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3924 - acc: 0.8864 - val_loss: 0.4557 - val_acc: 0.8906\n",
      "Epoch 743/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3923 - acc: 0.8880 - val_loss: 0.4442 - val_acc: 0.8931\n",
      "Epoch 744/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3921 - acc: 0.8870 - val_loss: 0.4459 - val_acc: 0.8928\n",
      "Epoch 745/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3918 - acc: 0.8874 - val_loss: 0.4405 - val_acc: 0.8954\n",
      "Epoch 746/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3917 - acc: 0.8875 - val_loss: 0.4740 - val_acc: 0.8855\n",
      "Epoch 747/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3916 - acc: 0.8874 - val_loss: 0.4381 - val_acc: 0.8953\n",
      "Epoch 748/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3915 - acc: 0.8874 - val_loss: 0.4429 - val_acc: 0.8929\n",
      "Epoch 749/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3915 - acc: 0.8883 - val_loss: 0.4463 - val_acc: 0.8926\n",
      "Epoch 750/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3910 - acc: 0.8873 - val_loss: 0.4647 - val_acc: 0.8876\n",
      "Epoch 751/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3907 - acc: 0.8871 - val_loss: 0.4543 - val_acc: 0.8900\n",
      "Epoch 752/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3909 - acc: 0.8882 - val_loss: 0.4614 - val_acc: 0.8891\n",
      "Epoch 753/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3903 - acc: 0.8875 - val_loss: 0.4659 - val_acc: 0.8867\n",
      "Epoch 754/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3900 - acc: 0.8880 - val_loss: 0.4279 - val_acc: 0.8990\n",
      "Epoch 755/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3904 - acc: 0.8887 - val_loss: 0.4238 - val_acc: 0.8972\n",
      "Epoch 756/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3903 - acc: 0.8885 - val_loss: 0.4647 - val_acc: 0.8871\n",
      "Epoch 757/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3898 - acc: 0.8880 - val_loss: 0.4817 - val_acc: 0.8835\n",
      "Epoch 758/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3902 - acc: 0.8883 - val_loss: 0.4744 - val_acc: 0.8850\n",
      "Epoch 759/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3891 - acc: 0.8883 - val_loss: 0.4508 - val_acc: 0.8918\n",
      "Epoch 760/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3894 - acc: 0.8884 - val_loss: 0.4116 - val_acc: 0.9024\n",
      "Epoch 761/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3893 - acc: 0.8886 - val_loss: 0.3948 - val_acc: 0.9088\n",
      "Epoch 762/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3892 - acc: 0.8879 - val_loss: 0.4310 - val_acc: 0.8975\n",
      "Epoch 763/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3889 - acc: 0.8891 - val_loss: 0.4774 - val_acc: 0.8840\n",
      "Epoch 764/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3887 - acc: 0.8888 - val_loss: 0.4550 - val_acc: 0.8909\n",
      "Epoch 765/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3887 - acc: 0.8888 - val_loss: 0.4102 - val_acc: 0.9047\n",
      "Epoch 766/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3886 - acc: 0.8887 - val_loss: 0.4394 - val_acc: 0.8939\n",
      "Epoch 767/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3881 - acc: 0.8886 - val_loss: 0.4384 - val_acc: 0.8937\n",
      "Epoch 768/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3885 - acc: 0.8885 - val_loss: 0.4953 - val_acc: 0.8770\n",
      "Epoch 769/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3880 - acc: 0.8889 - val_loss: 0.4193 - val_acc: 0.9016\n",
      "Epoch 770/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3878 - acc: 0.8887 - val_loss: 0.4384 - val_acc: 0.8959\n",
      "Epoch 771/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3875 - acc: 0.8880 - val_loss: 0.4511 - val_acc: 0.8909\n",
      "Epoch 772/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3876 - acc: 0.8893 - val_loss: 0.4892 - val_acc: 0.8826\n",
      "Epoch 773/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3872 - acc: 0.8882 - val_loss: 0.4841 - val_acc: 0.8823\n",
      "Epoch 774/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3872 - acc: 0.8884 - val_loss: 0.4905 - val_acc: 0.8810\n",
      "Epoch 775/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3872 - acc: 0.8882 - val_loss: 0.4644 - val_acc: 0.8868\n",
      "Epoch 776/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3867 - acc: 0.8882 - val_loss: 0.4327 - val_acc: 0.8969\n",
      "Epoch 777/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3866 - acc: 0.8890 - val_loss: 0.4493 - val_acc: 0.8906\n",
      "Epoch 778/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3867 - acc: 0.8893 - val_loss: 0.4379 - val_acc: 0.8957\n",
      "Epoch 779/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3863 - acc: 0.8886 - val_loss: 0.4563 - val_acc: 0.8898\n",
      "Epoch 780/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3860 - acc: 0.8885 - val_loss: 0.4410 - val_acc: 0.8954\n",
      "Epoch 781/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3861 - acc: 0.8893 - val_loss: 0.4622 - val_acc: 0.8881\n",
      "Epoch 782/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3864 - acc: 0.8893 - val_loss: 0.4398 - val_acc: 0.8943\n",
      "Epoch 783/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3858 - acc: 0.8886 - val_loss: 0.4224 - val_acc: 0.8999\n",
      "Epoch 784/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3858 - acc: 0.8899 - val_loss: 0.4454 - val_acc: 0.8922\n",
      "Epoch 785/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3853 - acc: 0.8892 - val_loss: 0.4830 - val_acc: 0.8818\n",
      "Epoch 786/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3853 - acc: 0.8899 - val_loss: 0.4873 - val_acc: 0.8806\n",
      "Epoch 787/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3849 - acc: 0.8894 - val_loss: 0.4577 - val_acc: 0.8887\n",
      "Epoch 788/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.3851 - acc: 0.8888 - val_loss: 0.4957 - val_acc: 0.8783\n",
      "Epoch 789/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3849 - acc: 0.8888 - val_loss: 0.4444 - val_acc: 0.8939\n",
      "Epoch 790/1000\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3845 - acc: 0.8894 - val_loss: 0.3868 - val_acc: 0.9118\n",
      "Epoch 791/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3843 - acc: 0.8887 - val_loss: 0.4641 - val_acc: 0.8880\n",
      "Epoch 792/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3844 - acc: 0.8893 - val_loss: 0.4171 - val_acc: 0.9008\n",
      "Epoch 793/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3840 - acc: 0.8893 - val_loss: 0.5086 - val_acc: 0.8734\n",
      "Epoch 794/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3841 - acc: 0.8897 - val_loss: 0.4917 - val_acc: 0.8774\n",
      "Epoch 795/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3839 - acc: 0.8889 - val_loss: 0.4318 - val_acc: 0.8971\n",
      "Epoch 796/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3841 - acc: 0.8901 - val_loss: 0.4224 - val_acc: 0.8995\n",
      "Epoch 797/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3841 - acc: 0.8895 - val_loss: 0.4770 - val_acc: 0.8834\n",
      "Epoch 798/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3838 - acc: 0.8894 - val_loss: 0.4414 - val_acc: 0.8918\n",
      "Epoch 799/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3834 - acc: 0.8896 - val_loss: 0.4390 - val_acc: 0.8966\n",
      "Epoch 800/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3834 - acc: 0.8898 - val_loss: 0.4539 - val_acc: 0.8900\n",
      "Epoch 801/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3829 - acc: 0.8901 - val_loss: 0.4299 - val_acc: 0.8959\n",
      "Epoch 802/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3830 - acc: 0.8898 - val_loss: 0.4779 - val_acc: 0.8823\n",
      "Epoch 803/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3824 - acc: 0.8892 - val_loss: 0.4340 - val_acc: 0.8970\n",
      "Epoch 804/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3825 - acc: 0.8896 - val_loss: 0.4397 - val_acc: 0.8945\n",
      "Epoch 805/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3825 - acc: 0.8896 - val_loss: 0.4345 - val_acc: 0.8969\n",
      "Epoch 806/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3823 - acc: 0.8894 - val_loss: 0.4382 - val_acc: 0.8943\n",
      "Epoch 807/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3823 - acc: 0.8897 - val_loss: 0.4284 - val_acc: 0.8967\n",
      "Epoch 808/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3824 - acc: 0.8903 - val_loss: 0.4663 - val_acc: 0.8863\n",
      "Epoch 809/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3823 - acc: 0.8900 - val_loss: 0.4528 - val_acc: 0.8912\n",
      "Epoch 810/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3817 - acc: 0.8909 - val_loss: 0.5305 - val_acc: 0.8660\n",
      "Epoch 811/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3817 - acc: 0.8895 - val_loss: 0.4236 - val_acc: 0.9000\n",
      "Epoch 812/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3817 - acc: 0.8898 - val_loss: 0.4710 - val_acc: 0.8844\n",
      "Epoch 813/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3813 - acc: 0.8907 - val_loss: 0.4229 - val_acc: 0.8988\n",
      "Epoch 814/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3811 - acc: 0.8909 - val_loss: 0.4811 - val_acc: 0.8824\n",
      "Epoch 815/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3808 - acc: 0.8898 - val_loss: 0.4964 - val_acc: 0.8774\n",
      "Epoch 816/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3807 - acc: 0.8901 - val_loss: 0.4791 - val_acc: 0.8827\n",
      "Epoch 817/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3808 - acc: 0.8899 - val_loss: 0.4787 - val_acc: 0.8840\n",
      "Epoch 818/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3807 - acc: 0.8901 - val_loss: 0.4296 - val_acc: 0.8988\n",
      "Epoch 819/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3804 - acc: 0.8897 - val_loss: 0.4253 - val_acc: 0.8990\n",
      "Epoch 820/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3802 - acc: 0.8908 - val_loss: 0.4049 - val_acc: 0.9037\n",
      "Epoch 821/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3796 - acc: 0.8908 - val_loss: 0.4480 - val_acc: 0.8933\n",
      "Epoch 822/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3799 - acc: 0.8910 - val_loss: 0.4601 - val_acc: 0.8896\n",
      "Epoch 823/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3802 - acc: 0.8910 - val_loss: 0.4352 - val_acc: 0.8965\n",
      "Epoch 824/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3797 - acc: 0.8906 - val_loss: 0.4765 - val_acc: 0.8830\n",
      "Epoch 825/1000\n",
      "30246/30246 [==============================] - 1s 45us/step - loss: 0.3800 - acc: 0.8909 - val_loss: 0.4544 - val_acc: 0.8892\n",
      "Epoch 826/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3795 - acc: 0.8901 - val_loss: 0.4573 - val_acc: 0.8900\n",
      "Epoch 827/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3792 - acc: 0.8904 - val_loss: 0.4596 - val_acc: 0.8887\n",
      "Epoch 828/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3794 - acc: 0.8898 - val_loss: 0.4431 - val_acc: 0.8937\n",
      "Epoch 829/1000\n",
      "30246/30246 [==============================] - 1s 40us/step - loss: 0.3789 - acc: 0.8915 - val_loss: 0.4207 - val_acc: 0.8996\n",
      "Epoch 830/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3792 - acc: 0.8904 - val_loss: 0.4602 - val_acc: 0.8884\n",
      "Epoch 831/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3789 - acc: 0.8913 - val_loss: 0.4583 - val_acc: 0.8881\n",
      "Epoch 832/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3788 - acc: 0.8904 - val_loss: 0.4426 - val_acc: 0.8938\n",
      "Epoch 833/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3785 - acc: 0.8902 - val_loss: 0.4506 - val_acc: 0.8910\n",
      "Epoch 834/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3787 - acc: 0.8908 - val_loss: 0.3904 - val_acc: 0.9110\n",
      "Epoch 835/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3785 - acc: 0.8898 - val_loss: 0.4417 - val_acc: 0.8929\n",
      "Epoch 836/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3781 - acc: 0.8908 - val_loss: 0.4681 - val_acc: 0.8846\n",
      "Epoch 837/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3780 - acc: 0.8907 - val_loss: 0.5029 - val_acc: 0.8742\n",
      "Epoch 838/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3783 - acc: 0.8917 - val_loss: 0.4133 - val_acc: 0.9021\n",
      "Epoch 839/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3773 - acc: 0.8907 - val_loss: 0.4620 - val_acc: 0.8892\n",
      "Epoch 840/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3771 - acc: 0.8909 - val_loss: 0.4315 - val_acc: 0.8965\n",
      "Epoch 841/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3778 - acc: 0.8908 - val_loss: 0.4653 - val_acc: 0.8857\n",
      "Epoch 842/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3775 - acc: 0.8911 - val_loss: 0.4508 - val_acc: 0.8904\n",
      "Epoch 843/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3770 - acc: 0.8910 - val_loss: 0.4345 - val_acc: 0.8951\n",
      "Epoch 844/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3771 - acc: 0.8903 - val_loss: 0.4910 - val_acc: 0.8773\n",
      "Epoch 845/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3767 - acc: 0.8910 - val_loss: 0.4915 - val_acc: 0.8779\n",
      "Epoch 846/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3769 - acc: 0.8900 - val_loss: 0.4308 - val_acc: 0.8979\n",
      "Epoch 847/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3766 - acc: 0.8909 - val_loss: 0.4472 - val_acc: 0.8910\n",
      "Epoch 848/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3766 - acc: 0.8919 - val_loss: 0.4827 - val_acc: 0.8819\n",
      "Epoch 849/1000\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3762 - acc: 0.8915 - val_loss: 0.4211 - val_acc: 0.8991\n",
      "Epoch 850/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3760 - acc: 0.8917 - val_loss: 0.4584 - val_acc: 0.8884\n",
      "Epoch 851/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3761 - acc: 0.8912 - val_loss: 0.4501 - val_acc: 0.8910\n",
      "Epoch 852/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3763 - acc: 0.8915 - val_loss: 0.4894 - val_acc: 0.8790\n",
      "Epoch 853/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3759 - acc: 0.8916 - val_loss: 0.4299 - val_acc: 0.8949\n",
      "Epoch 854/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3761 - acc: 0.8916 - val_loss: 0.4874 - val_acc: 0.8799\n",
      "Epoch 855/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3753 - acc: 0.8913 - val_loss: 0.4121 - val_acc: 0.9008\n",
      "Epoch 856/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3755 - acc: 0.8923 - val_loss: 0.4321 - val_acc: 0.8961\n",
      "Epoch 857/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3754 - acc: 0.8916 - val_loss: 0.4457 - val_acc: 0.8937\n",
      "Epoch 858/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3756 - acc: 0.8908 - val_loss: 0.4377 - val_acc: 0.8953\n",
      "Epoch 859/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3750 - acc: 0.8911 - val_loss: 0.4269 - val_acc: 0.8994\n",
      "Epoch 860/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3752 - acc: 0.8913 - val_loss: 0.4675 - val_acc: 0.8865\n",
      "Epoch 861/1000\n",
      "30246/30246 [==============================] - 1s 39us/step - loss: 0.3749 - acc: 0.8916 - val_loss: 0.4443 - val_acc: 0.8917\n",
      "Epoch 862/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3749 - acc: 0.8916 - val_loss: 0.4400 - val_acc: 0.8926\n",
      "Epoch 863/1000\n",
      "30246/30246 [==============================] - 1s 49us/step - loss: 0.3749 - acc: 0.8911 - val_loss: 0.4332 - val_acc: 0.8963\n",
      "Epoch 864/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3750 - acc: 0.8908 - val_loss: 0.4792 - val_acc: 0.8811\n",
      "Epoch 865/1000\n",
      "30246/30246 [==============================] - 1s 27us/step - loss: 0.3748 - acc: 0.8911 - val_loss: 0.4484 - val_acc: 0.8912\n",
      "Epoch 866/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3739 - acc: 0.8906 - val_loss: 0.4454 - val_acc: 0.8920\n",
      "Epoch 867/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3743 - acc: 0.8911 - val_loss: 0.4270 - val_acc: 0.8958\n",
      "Epoch 868/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8916 - val_loss: 0.4911 - val_acc: 0.8791\n",
      "Epoch 869/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3733 - acc: 0.8920 - val_loss: 0.3723 - val_acc: 0.9148\n",
      "Epoch 870/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3741 - acc: 0.8916 - val_loss: 0.4564 - val_acc: 0.8869\n",
      "Epoch 871/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3736 - acc: 0.8919 - val_loss: 0.4113 - val_acc: 0.9023\n",
      "Epoch 872/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3736 - acc: 0.8920 - val_loss: 0.4897 - val_acc: 0.8798\n",
      "Epoch 873/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3733 - acc: 0.8926 - val_loss: 0.4383 - val_acc: 0.8926\n",
      "Epoch 874/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3730 - acc: 0.8923 - val_loss: 0.4373 - val_acc: 0.8946\n",
      "Epoch 875/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3731 - acc: 0.8926 - val_loss: 0.4259 - val_acc: 0.8975\n",
      "Epoch 876/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3730 - acc: 0.8917 - val_loss: 0.4637 - val_acc: 0.8872\n",
      "Epoch 877/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3729 - acc: 0.8923 - val_loss: 0.4391 - val_acc: 0.8950\n",
      "Epoch 878/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3729 - acc: 0.8923 - val_loss: 0.4296 - val_acc: 0.8975\n",
      "Epoch 879/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3724 - acc: 0.8919 - val_loss: 0.4575 - val_acc: 0.8885\n",
      "Epoch 880/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3728 - acc: 0.8921 - val_loss: 0.4561 - val_acc: 0.8880\n",
      "Epoch 881/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3723 - acc: 0.8919 - val_loss: 0.4809 - val_acc: 0.8810\n",
      "Epoch 882/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3720 - acc: 0.8916 - val_loss: 0.4480 - val_acc: 0.8909\n",
      "Epoch 883/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3719 - acc: 0.8920 - val_loss: 0.4315 - val_acc: 0.8974\n",
      "Epoch 884/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3718 - acc: 0.8922 - val_loss: 0.4194 - val_acc: 0.9004\n",
      "Epoch 885/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3714 - acc: 0.8921 - val_loss: 0.4741 - val_acc: 0.8835\n",
      "Epoch 886/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3719 - acc: 0.8922 - val_loss: 0.4244 - val_acc: 0.8980\n",
      "Epoch 887/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3717 - acc: 0.8926 - val_loss: 0.4835 - val_acc: 0.8806\n",
      "Epoch 888/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3716 - acc: 0.8922 - val_loss: 0.4271 - val_acc: 0.8969\n",
      "Epoch 889/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3714 - acc: 0.8922 - val_loss: 0.4343 - val_acc: 0.8953\n",
      "Epoch 890/1000\n",
      "30246/30246 [==============================] - 0s 14us/step - loss: 0.3708 - acc: 0.8925 - val_loss: 0.4422 - val_acc: 0.8906\n",
      "Epoch 891/1000\n",
      "30246/30246 [==============================] - 448s 15ms/step - loss: 0.3712 - acc: 0.8921 - val_loss: 0.4464 - val_acc: 0.8918\n",
      "Epoch 892/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3711 - acc: 0.8919 - val_loss: 0.4712 - val_acc: 0.8853\n",
      "Epoch 893/1000\n",
      "30246/30246 [==============================] - 1s 43us/step - loss: 0.3710 - acc: 0.8926 - val_loss: 0.4364 - val_acc: 0.8943\n",
      "Epoch 894/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3706 - acc: 0.8923 - val_loss: 0.4684 - val_acc: 0.8848\n",
      "Epoch 895/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3707 - acc: 0.8928 - val_loss: 0.4806 - val_acc: 0.8816\n",
      "Epoch 896/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3705 - acc: 0.8927 - val_loss: 0.4029 - val_acc: 0.9054\n",
      "Epoch 897/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3701 - acc: 0.8927 - val_loss: 0.4208 - val_acc: 0.8996\n",
      "Epoch 898/1000\n",
      "30246/30246 [==============================] - 1s 26us/step - loss: 0.3701 - acc: 0.8928 - val_loss: 0.4438 - val_acc: 0.8912\n",
      "Epoch 899/1000\n",
      "30246/30246 [==============================] - 1s 34us/step - loss: 0.3700 - acc: 0.8923 - val_loss: 0.4590 - val_acc: 0.8872\n",
      "Epoch 900/1000\n",
      "30246/30246 [==============================] - 1s 36us/step - loss: 0.3699 - acc: 0.8927 - val_loss: 0.4410 - val_acc: 0.8924\n",
      "Epoch 901/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3701 - acc: 0.8930 - val_loss: 0.5125 - val_acc: 0.8707\n",
      "Epoch 902/1000\n",
      "30246/30246 [==============================] - 1s 22us/step - loss: 0.3701 - acc: 0.8925 - val_loss: 0.4671 - val_acc: 0.8846\n",
      "Epoch 903/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3695 - acc: 0.8927 - val_loss: 0.4799 - val_acc: 0.8815\n",
      "Epoch 904/1000\n",
      "30246/30246 [==============================] - 1s 20us/step - loss: 0.3694 - acc: 0.8930 - val_loss: 0.4308 - val_acc: 0.8962\n",
      "Epoch 905/1000\n",
      "30246/30246 [==============================] - 1s 21us/step - loss: 0.3696 - acc: 0.8929 - val_loss: 0.5046 - val_acc: 0.8749\n",
      "Epoch 906/1000\n",
      "30246/30246 [==============================] - 1s 23us/step - loss: 0.3695 - acc: 0.8927 - val_loss: 0.4530 - val_acc: 0.8884\n",
      "Epoch 907/1000\n",
      "30246/30246 [==============================] - 1s 24us/step - loss: 0.3691 - acc: 0.8928 - val_loss: 0.4128 - val_acc: 0.9019\n",
      "Epoch 908/1000\n",
      "30246/30246 [==============================] - 1s 35us/step - loss: 0.3692 - acc: 0.8927 - val_loss: 0.4472 - val_acc: 0.8925\n",
      "Epoch 909/1000\n",
      "30246/30246 [==============================] - 1s 19us/step - loss: 0.3685 - acc: 0.8940 - val_loss: 0.4244 - val_acc: 0.8983\n",
      "Epoch 910/1000\n",
      "30246/30246 [==============================] - 1s 29us/step - loss: 0.3687 - acc: 0.8933 - val_loss: 0.4292 - val_acc: 0.8962\n",
      "Epoch 911/1000\n",
      "30246/30246 [==============================] - 1s 18us/step - loss: 0.3686 - acc: 0.8938 - val_loss: 0.4631 - val_acc: 0.8860\n",
      "Epoch 912/1000\n",
      "30246/30246 [==============================] - 1s 48us/step - loss: 0.3683 - acc: 0.8922 - val_loss: 0.4399 - val_acc: 0.8931\n",
      "Epoch 913/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3688 - acc: 0.8927 - val_loss: 0.4441 - val_acc: 0.8916\n",
      "Epoch 914/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3686 - acc: 0.8930 - val_loss: 0.4005 - val_acc: 0.9060\n",
      "Epoch 915/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3679 - acc: 0.8929 - val_loss: 0.4046 - val_acc: 0.9045\n",
      "Epoch 916/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3678 - acc: 0.8928 - val_loss: 0.4821 - val_acc: 0.8816\n",
      "Epoch 917/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3677 - acc: 0.8926 - val_loss: 0.4274 - val_acc: 0.8972\n",
      "Epoch 918/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3675 - acc: 0.8937 - val_loss: 0.4603 - val_acc: 0.8872\n",
      "Epoch 919/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3672 - acc: 0.8932 - val_loss: 0.4733 - val_acc: 0.8830\n",
      "Epoch 920/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3674 - acc: 0.8929 - val_loss: 0.3970 - val_acc: 0.9076\n",
      "Epoch 921/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3675 - acc: 0.8931 - val_loss: 0.4066 - val_acc: 0.9043\n",
      "Epoch 922/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3675 - acc: 0.8927 - val_loss: 0.4464 - val_acc: 0.8928\n",
      "Epoch 923/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3676 - acc: 0.8933 - val_loss: 0.4562 - val_acc: 0.8891\n",
      "Epoch 924/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3671 - acc: 0.8938 - val_loss: 0.4605 - val_acc: 0.8871\n",
      "Epoch 925/1000\n",
      "30246/30246 [==============================] - 1s 17us/step - loss: 0.3666 - acc: 0.8928 - val_loss: 0.4410 - val_acc: 0.8939\n",
      "Epoch 926/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3669 - acc: 0.8935 - val_loss: 0.4515 - val_acc: 0.8906\n",
      "Epoch 927/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3668 - acc: 0.8933 - val_loss: 0.4469 - val_acc: 0.8908\n",
      "Epoch 928/1000\n",
      "30246/30246 [==============================] - 0s 17us/step - loss: 0.3667 - acc: 0.8928 - val_loss: 0.4600 - val_acc: 0.8865\n",
      "Epoch 929/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3665 - acc: 0.8931 - val_loss: 0.4882 - val_acc: 0.8795\n",
      "Epoch 930/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3662 - acc: 0.8935 - val_loss: 0.4822 - val_acc: 0.8802\n",
      "Epoch 931/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3663 - acc: 0.8935 - val_loss: 0.4574 - val_acc: 0.8875\n",
      "Epoch 932/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8934 - val_loss: 0.4989 - val_acc: 0.8748\n",
      "Epoch 933/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3658 - acc: 0.8935 - val_loss: 0.4257 - val_acc: 0.8980\n",
      "Epoch 934/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8935 - val_loss: 0.5247 - val_acc: 0.8690\n",
      "Epoch 935/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3661 - acc: 0.8935 - val_loss: 0.4469 - val_acc: 0.8926\n",
      "Epoch 936/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3657 - acc: 0.8935 - val_loss: 0.4752 - val_acc: 0.8844\n",
      "Epoch 937/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3656 - acc: 0.8944 - val_loss: 0.4422 - val_acc: 0.8926\n",
      "Epoch 938/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3651 - acc: 0.8938 - val_loss: 0.4512 - val_acc: 0.8902\n",
      "Epoch 939/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3654 - acc: 0.8936 - val_loss: 0.4630 - val_acc: 0.8875\n",
      "Epoch 940/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3651 - acc: 0.8922 - val_loss: 0.4338 - val_acc: 0.8951\n",
      "Epoch 941/1000\n",
      "30246/30246 [==============================] - 2s 51us/step - loss: 0.3652 - acc: 0.8938 - val_loss: 0.4340 - val_acc: 0.8957\n",
      "Epoch 942/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3650 - acc: 0.8937 - val_loss: 0.5133 - val_acc: 0.8724\n",
      "Epoch 943/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3652 - acc: 0.8942 - val_loss: 0.4262 - val_acc: 0.8966\n",
      "Epoch 944/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3652 - acc: 0.8932 - val_loss: 0.4492 - val_acc: 0.8900\n",
      "Epoch 945/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3647 - acc: 0.8935 - val_loss: 0.4790 - val_acc: 0.8812\n",
      "Epoch 946/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3647 - acc: 0.8943 - val_loss: 0.4580 - val_acc: 0.8884\n",
      "Epoch 947/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3643 - acc: 0.8936 - val_loss: 0.4588 - val_acc: 0.8893\n",
      "Epoch 948/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3642 - acc: 0.8943 - val_loss: 0.5300 - val_acc: 0.8656\n",
      "Epoch 949/1000\n",
      "30246/30246 [==============================] - 0s 16us/step - loss: 0.3642 - acc: 0.8943 - val_loss: 0.4466 - val_acc: 0.8898\n",
      "Epoch 950/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3643 - acc: 0.8933 - val_loss: 0.4353 - val_acc: 0.8946\n",
      "Epoch 951/1000\n",
      "30246/30246 [==============================] - 0s 15us/step - loss: 0.3642 - acc: 0.8934 - val_loss: 0.4442 - val_acc: 0.8922\n",
      "Epoch 952/1000\n",
      " 3840/30246 [==>...........................] - ETA: 0s - loss: 0.3629 - acc: 0.8961"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Dense(32, input_shape=(256,)),\n",
    "    Activation('relu'),\n",
    "    Dense(5),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='sgd',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=1000, validation_split=0.2, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-3ceaebd669bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"asdf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ecf1552c7a75>\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(history, title)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_standardplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0max1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0max2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"validation\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_loss'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history, \"asdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: One hidden layer, different optizimizers\n",
    "### Description\n",
    "\n",
    "Train a network with one hidden layer and compare different optimizers.\n",
    "\n",
    "1. Use one hidden layer with 64 units and the 'relu' activation. Use the [summary method](https://keras.io/models/about-keras-models/) to inspect your model.\n",
    "2. Fit the model for 50 epochs with different learning rates of stochastic gradient descent and answer the question below.\n",
    "3. Replace the stochastic gradient descent optimizer with the [Adam optimizer](https://keras.io/optimizers/#adam).\n",
    "4. Plot the learning curves of SGD with a reasonable learning rate together with the learning curves of Adam in the same figure. Take care of a reasonable labeling of the curves in the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What happens if the learning rate of SGD is A) very large B) very small? Please answer A) and B) with one full sentence (double click this markdown cell to edit).\n",
    "\n",
    "**Answer**:\n",
    "\n",
    "A) \n",
    "\n",
    "B) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Overfitting and early stopping with Adam\n",
    "\n",
    "### Description\n",
    "\n",
    "Run the above simulation with Adam for sufficiently many epochs (be patient!) until you see clear overfitting.\n",
    "\n",
    "1. Plot the learning curves of a fit with Adam and sufficiently many epochs and answer the questions below.\n",
    "\n",
    "A simple, but effective mean to avoid overfitting is early stopping, i.e. a fit is not run until convergence but stopped as soon as the validation error starts to increase. We will use early stopping in all subsequent exercises.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**: At which epoch (approximately) does the model start to overfit? Please answer with one full sentence.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "**Question 2**: Explain the qualitative difference between the loss curves and the accuracy curves with respect to signs of overfitting. Please answer with at most 3 full sentences.\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Model performance as a function of number of hidden neurons\n",
    "\n",
    "### Description\n",
    "\n",
    "Investigate how the best validation loss and accuracy depends on the number of hidden neurons in a single layer.\n",
    "\n",
    "1. Fit a reasonable number of models with different hidden layer size (between 10 and 1000 hidden neurons) for a fixed number of epochs well beyond the point of overfitting.\n",
    "2. Collect some statistics by fitting the same models as in 1. for multiple initial conditions. Hints: 1. If you don't reset the random seed, you get different initial conditions each time you create a new model. 2. Let your computer work while you are asleep.\n",
    "3. Plot summary statistics of the final validation loss and accuracy versus the number of hidden neurons. Hint: [boxplots](https://matplotlib.org/examples/pylab_examples/boxplot_demo.html) (also [here](https://matplotlib.org/api/_as_gen/matplotlib.axes.Axes.boxplot.html?highlight=boxplot#matplotlib.axes.Axes.boxplot)) are useful. You may also want to use the matplotlib method set_xticklabels.\n",
    "4. Plot summary statistics of the loss and accuracy for early stopping versus the number of hidden neurons.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Comparison to deep models\n",
    "\n",
    "### Description\n",
    "\n",
    "Instead of choosing one hidden layer (with many neurons) you experiment here with multiple hidden layers (each with not so many neurons).\n",
    "\n",
    "1. Fit models with 2, 3 and 4 hidden layers with approximately the same number of parameters as a network with one hidden layer of 100 neurons. Hint: Calculate the number of parameters in a network with input dimensionality N_in, K hidden layers with N_h units, one output layer with N_out dimensions and solve for N_h. Confirm you result with the keras method model.summary().\n",
    "2. Run each model multiple times with different initial conditions and plot summary statistics of the best validation loss and accuracy versus the number of hidden layers.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Tricks (regularization, batch normalization, dropout)\n",
    "\n",
    "### Description\n",
    "\n",
    "Overfitting can also be counteracted with regularization and dropout. Batch normalization is supposed to mainly decrease convergence time.\n",
    "\n",
    "1. Try to improve the best validation scores of the model with 1 layer and 100 hidden neurons and the model with 4 hidden layers. Experiment with batch_normalization layers, dropout layers and l1- and l2-regularization on weights (kernels) and biases.\n",
    "2. After you have found good settings, plot for both models the learning curves of the naive model you fitted in the previous exercises together with the learning curves of the current version.\n",
    "3. For proper comparison, plot also the learning curves of the two current models in a third figure.\n",
    "\n",
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Convolutional networks\n",
    "\n",
    "### Description\n",
    "\n",
    "Convolutional neural networks have an inductive bias that is well adapted to image classification.\n",
    "\n",
    "1. Design a convolutional neural network, play with the parameters and fit it. Hint: You may get valuable inspiration from the keras [examples](https://github.com/keras-team/keras/tree/master/examples), e.g. [mnist_cnn](https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py).\n",
    "2. Plot the learning curves of the convolutional neural network together with the so far best performing model.\n",
    "\n",
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
